"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[38613],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>m});var o=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),p=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},d=function(e){var n=p(e.components);return o.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(t),m=a,h=u["".concat(s,".").concat(m)]||u[m]||c[m]||r;return t?o.createElement(h,i(i({ref:n},d),{},{components:t})):o.createElement(h,i({ref:n},d))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,i=new Array(r);i[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var p=2;p<r;p++)i[p]=t[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},10634:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var o=t(87462),a=(t(67294),t(3905));const r={sidebar_position:6020,slug:"2022-05-17",title:"Hashicorp Nomad Deployment",authors:"mpolinowski",tags:["Nomad","LINUX"]},i=void 0,l={unversionedId:"DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment/index",id:"DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment/index",title:"Hashicorp Nomad Deployment",description:"Shen Zhen, China",source:"@site/docs/DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment/index.md",sourceDirName:"DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment",slug:"/DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment/2022-05-17",permalink:"/docs/DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment/2022-05-17",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/DevOps/Hashicorp/2022-05-17-hashicorp-dojo-nomad-deployment/index.md",tags:[{label:"Nomad",permalink:"/docs/tags/nomad"},{label:"LINUX",permalink:"/docs/tags/linux"}],version:"current",sidebarPosition:6020,frontMatter:{sidebar_position:6020,slug:"2022-05-17",title:"Hashicorp Nomad Deployment",authors:"mpolinowski",tags:["Nomad","LINUX"]},sidebar:"tutorialSidebar",previous:{title:"Hashicorp Nomad Adding Encryption to your Cluster",permalink:"/docs/DevOps/Hashicorp/2022-05-18-hashicorp-dojo-nomad-adding-encryption/2022-05-18"},next:{title:"Hashicorp Nomad Dojo",permalink:"/docs/DevOps/Hashicorp/2022-05-16-hashicorp-dojo-nomad-starter/2022-05-16"}},s={},p=[{value:"Deploying a Job to a Multi-Cluster Node",id:"deploying-a-job-to-a-multi-cluster-node",level:2},{value:"Planning",id:"planning",level:3},{value:"Running",id:"running",level:3},{value:"Allocating a Static Service Port to the Frontend Container",id:"allocating-a-static-service-port-to-the-frontend-container",level:3},{value:"Testing the Allocation",id:"testing-the-allocation",level:4},{value:"Scaling the Deployment",id:"scaling-the-deployment",level:2},{value:"Dynamic Port Allocation",id:"dynamic-port-allocation",level:3}],d={toc:p};function c(e){let{components:n,...r}=e;return(0,a.kt)("wrapper",(0,o.Z)({},d,r,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Shen Zhen, China",src:t(32628).Z,width:"2230",height:"839"})),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#deploying-a-job-to-a-multi-cluster-node"},"Deploying a Job to a Multi-Cluster Node"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#planning"},"Planning")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#running"},"Running")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#allocating-a-static-service-port-to-the-frontend-container"},"Allocating a Static Service Port to the Frontend Container"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#testing-the-allocation"},"Testing the Allocation")))))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#scaling-the-deployment"},"Scaling the Deployment"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#dynamic-port-allocation"},"Dynamic Port Allocation"))))),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Continuation of ",(0,a.kt)("a",{parentName:"p",href:"/docs/DevOps/Hashicorp/2022-05-16-hashicorp-dojo-nomad-starter/2022-05-16"},"Hashicorp Nomad Dojo"))),(0,a.kt)("h2",{id:"deploying-a-job-to-a-multi-cluster-node"},"Deploying a Job to a Multi-Cluster Node"),(0,a.kt)("p",null,"When we now execute the planning and run command for the template job ",(0,a.kt)("inlineCode",{parentName:"p"},"frontend.nomad")," it will now be deployed to our new minion node instead of the ",(0,a.kt)("strong",{parentName:"p"},"MASTER"),":"),(0,a.kt)("h3",{id:"planning"},"Planning"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'nomad plan frontend.nomad\n\n+ Job: "frontend"\n+ Task Group: "frontend" (1 create)\n  + Task: "frontend" (forces create)\n\nScheduler dry-run:\n- All tasks successfully allocated.\n\nJob Modify Index: 0\nTo submit the job with version verification run:\n\nnomad job run -check-index 0 frontend.nomad\n')),(0,a.kt)("h3",{id:"running"},"Running"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'nomad job run -check-index 0 frontend.nomad\n\n==> 2022-06-05T12:02:58+02:00: Monitoring evaluation "1d55c9e3"\n    2022-06-05T12:02:58+02:00: Evaluation triggered by job "frontend"\n==> 2022-06-05T12:02:59+02:00: Monitoring evaluation "1d55c9e3"\n    2022-06-05T12:02:59+02:00: Evaluation within deployment: "5b251061"\n    2022-06-05T12:02:59+02:00: Allocation "b6e8c543" created: node "005f708b", group "frontend"\n    2022-06-05T12:02:59+02:00: Evaluation status changed: "pending" -> "complete"\n==> 2022-06-05T12:02:59+02:00: Evaluation "1d55c9e3" finished with status "complete"\n==> 2022-06-05T12:02:59+02:00: Monitoring deployment "5b251061"\n  \u2713 Deployment "5b251061" successful\n')),(0,a.kt)("p",null,"Run a ",(0,a.kt)("inlineCode",{parentName:"p"},"docker ps")," on the ",(0,a.kt)("strong",{parentName:"p"},"MINION")," and you will see that the docker frontend container has been deployed:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"docker ps\n\nCONTAINER ID        IMAGE                           CREATED              STATUS\n33f1ddc027fa        thedojoseries/frontend:latest   About a minute ago   Up About a minute\n")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Hashicorp Nomad UI",src:t(84798).Z,width:"1240",height:"831"})),(0,a.kt)("h3",{id:"allocating-a-static-service-port-to-the-frontend-container"},"Allocating a Static Service Port to the Frontend Container"),(0,a.kt)("p",null,"We now deployed a frontend container based on the ",(0,a.kt)("inlineCode",{parentName:"p"},"thedojoseries/frontend")," docker image. This container exposes a web service on port ",(0,a.kt)("inlineCode",{parentName:"p"},"8080")," that we have to allocate to our deployment. This can be done by editing the job definition ",(0,a.kt)("inlineCode",{parentName:"p"},"frontend.nomad"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'job "frontend" {\n  datacenters = ["instaryun"] \n  type = "service"\n\n  group "frontend" {\n    count = 1\n\n    task "frontend" {\n        driver = "docker"\n\n        config {\n          image = "thedojoseries/frontend:latest"\n        }\n\n        resources {\n          network {\n            port "http" {\n              static = 8080\n            }\n          }\n        }\n      }\n   }\n}\n')),(0,a.kt)("p",null,"Re-run plan and deploy the new version:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"nomad plan frontend.nomad\nnomad job run -check-index 103 frontend.nomad\n")),(0,a.kt)("p",null,"This leads to a deprecation warning for the network resources (I will fix this a few steps further down):"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'Scheduler dry-run:\n- All tasks successfully allocated.\n\nJob Warnings:\n1 warning(s):\n\n* Group "frontend" has warnings: 1 error occurred:\n        * 1 error occurred:\n        * Task "frontend": task network resources have been deprecated as of Nomad 0.12.0. Please configure networking via group network block.\n')),(0,a.kt)("p",null,"But running the job does work ",(0,a.kt)("inlineCode",{parentName:"p"},"nomad job run -check-index 210 frontend.nomad"),":"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Hashicorp Nomad UI",src:t(55766).Z,width:"1232",height:"768"})),(0,a.kt)("h4",{id:"testing-the-allocation"},"Testing the Allocation"),(0,a.kt)("p",null,"If you run a ",(0,a.kt)("inlineCode",{parentName:"p"},"docker ps")," on your minion you will see that Nomad bounded the port 8080 to the external IP of your server. This is why I at first failed when trying to access the frontend server on localhost ",(0,a.kt)("inlineCode",{parentName:"p"},"curl localhost:8080"),". I had to open the port 8080 and then use the external IP instead:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"ufw allow 8080/tcp\nufw reload\ncurl my.minion.com:8080\n")),(0,a.kt)("p",null,"This returns some HTML/JS code:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-html"},'<!DOCTYPE html>\n<html>\n\n<head>\n  <meta charset="utf-8">\n  <title>frontend</title>\n</head>\n\n<body>\n  <div class="container">\n    <div id="app"></div>\n  </div>\n<script type="text/javascript" src="/app.js"><\/script></body>\n\n</html>\n')),(0,a.kt)("p",null,"That my browser displays as:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Hashicorp Nomad UI",src:t(90066).Z,width:"1054",height:"327"})),(0,a.kt)("h2",{id:"scaling-the-deployment"},"Scaling the Deployment"),(0,a.kt)("p",null,"In Nomad, it's possible to horizontally scale an application, meaning that whenever a condition is true, Nomad will start spinning up more copies of the same application across the cluster so that the load is spread more evenly. What you need to do next is to define a scaling stanza in frontend.nomad using the information below:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"enabled")," = ",(0,a.kt)("inlineCode",{parentName:"li"},"true")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"min")," = ",(0,a.kt)("inlineCode",{parentName:"li"},"1")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"max")," = ",(0,a.kt)("inlineCode",{parentName:"li"},"3")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"policy")," = {} (A policy meant to be used by the ",(0,a.kt)("a",{parentName:"li",href:"https://www.nomadproject.io/tools/autoscaling"},"Nomad Autoscaler"),")")),(0,a.kt)("p",null,"I do not have the ",(0,a.kt)("a",{parentName:"p",href:"https://www.nomadproject.io/tools/autoscaling"},"autoscaler")," running and setting an empty object as policy here returns an error when I try to plan the job:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'Error getting job struct: Failed to parse using HCL 2. Use the HCL 1 parser with `nomad run -hcl1`, or address the following issues:\nfrontend.nomad:12,7-13: Unsupported argument; An argument named "policy" is not expected here. Did you mean to define a block of type "policy"?\n')),(0,a.kt)("p",null,"So I removed the policy completely and was able to run the ",(0,a.kt)("inlineCode",{parentName:"p"},"nomad plan frontend.nomad")," command. Without a policy running this command only results in the minimum amount of instances to be started - there is no trigger to extend the deployment. ",(0,a.kt)("strong",{parentName:"p"},"TODO"),": I will have to look into ",(0,a.kt)("a",{parentName:"p",href:"https://www.nomadproject.io/tools/autoscaling"},"Autoscaling"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'job "frontend" {\n  datacenters = ["instaryun"] \n  type = "service"\n\n  group "frontend" {\n    count = 2\n\n    scaling {\n      enabled = true\n      min     = 2\n      max     = 3\n    }\n\n    task "frontend" {\n        driver = "docker"\n\n        config {\n          image = "thedojoseries/frontend:latest"\n        }\n\n        resources {\n          network {\n            port "http" {\n              static = 8080\n            }\n          }\n        }\n      }\n   }\n}\n')),(0,a.kt)("p",null,"Running the job does show an issue here, though. Since the port ",(0,a.kt)("inlineCode",{parentName:"p"},"8080")," is hard coded running the command would end up in a port conflict where the second instance would crash as the port is already assigned to the first. The planning step would notice this issue and stop us from running the job. But since my minion server is configured to use ",(0,a.kt)("strong",{parentName:"p"},"IPv6")," and ",(0,a.kt)("strong",{parentName:"p"},"IPv4")," Nomad just spreads out the instances to those resources. Also, if I had more than 1 minion Nomad would ",(0,a.kt)("strong",{parentName:"p"},"automatically de-conflict")," the situation by using different hosts for each instance - love it!"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"docker ps\nCONTAINER ID        IMAGE                           PORTS\n7033df07401e        thedojoseries/frontend:latest   minion_ipv6:8080->8080/tcp\nb136c5ddd80e        thedojoseries/frontend:latest   minion_ipv4:8080->8080/tcp\n")),(0,a.kt)("h3",{id:"dynamic-port-allocation"},"Dynamic Port Allocation"),(0,a.kt)("p",null,"To prevent this issue from showing up we can assign dynamic ports to our application:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'job "frontend" {\n  datacenters = ["instaryun"] \n  type = "service"\n\n  group "frontend" {\n    count = 2\n\n    scaling {\n      enabled = true\n      min     = 2\n      max     = 3\n    }\n\n    task "frontend" {\n        driver = "docker"\n\n        config {\n          image = "thedojoseries/frontend:latest"\n        }\n\n        resources {\n          network {\n            port "http" {}\n          }\n        }\n      }\n   }\n}\n')),(0,a.kt)("p",null,"This Nomad used random ports. Both on the IPv4 interface. But there is another problem. Our web application inside the docker container exposes it's service on port ",(0,a.kt)("inlineCode",{parentName:"p"},"8080"),". Which means those dynamic ports are actually going nowhere:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"docker ps\nCONTAINER ID        IMAGE                           PORTS\n6d15e57bd756        thedojoseries/frontend:latest   8080/tcp, minion_ipv4:25500->25500/tcp\n029aee103aa7        thedojoseries/frontend:latest   8080/tcp, minion_ipv4:22124->22124/tcp\n")),(0,a.kt)("p",null,"We can fix ",(0,a.kt)("a",{parentName:"p",href:"https://discuss.hashicorp.com/t/simple-dynamic-port-mapping/25202/2"},"this issue with"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'job "frontend" {\n    datacenters = ["instaryun"]\n\n    group "frontend" {\n    count = 2\n\n    scaling {\n      enabled = true\n      min     = 2\n      max     = 3\n    }\n        \n        network {\n            mode = "host"\n            port "http" {\n                to = "8080"\n            }\n        }\n\n        task "app" {\n            driver = "docker"\n\n            config {\n                image = "thedojoseries/frontend:latest"\n                ports = ["http"]\n            }\n        }\n    }\n}\n')),(0,a.kt)("p",null,"Now we have two random ports on the outside, bound to the main IPv4 interface. And both are then directed to the internal service port ",(0,a.kt)("inlineCode",{parentName:"p"},"8080")," - yeah!"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"docker ps\nCONTAINER ID        IMAGE                           PORTS\naaa9650a9360        thedojoseries/frontend:latest   8080/tcp, minion_ipv4:29457->8080/tcp\n541b6f5caa47        thedojoseries/frontend:latest   8080/tcp, minion_ipv4:23526->8080/tcp\n")))}c.isMDXComponent=!0},84798:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/Hashicorp_Nomad_UI_01-3a62aec892f04f92839ade9277f476a1.png"},55766:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/Hashicorp_Nomad_UI_02-b5833e3efa3e3e69fbb280366f595550.png"},90066:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/Hashicorp_Nomad_UI_03-4620059d3c0b01f91fdf488e5e926770.png"},32628:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-6c1edb088dfea3a7d39f8eebb8e9dc23.jpg"}}]);