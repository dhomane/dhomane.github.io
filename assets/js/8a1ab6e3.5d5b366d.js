"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[14617],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>u});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),u=i,f=m["".concat(l,".").concat(u)]||m[u]||h[u]||r;return n?a.createElement(f,o(o({ref:t},p),{},{components:n})):a.createElement(f,o({ref:t},p))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},58829:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=n(87462),i=(n(67294),n(3905));const r={sidebar_position:6040,slug:"2021-12-09",title:"OpenCV CAMshift Algorithm for Object Tracking",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},o=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/index",id:"IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/index",title:"OpenCV CAMshift Algorithm for Object Tracking",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking",slug:"/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"OpenCV",permalink:"/docs/tags/open-cv"}],version:"current",sidebarPosition:6040,frontMatter:{sidebar_position:6040,slug:"2021-12-09",title:"OpenCV CAMshift Algorithm for Object Tracking",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},sidebar:"tutorialSidebar",previous:{title:"OpenCV Optical Flow Algorithm for Object Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/2021-12-10"},next:{title:"OpenCV Meanshift Algorithm for Object Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08"}},l={},c=[{value:"CAMshift",id:"camshift",level:2},{value:"Get your Videostream",id:"get-your-videostream",level:2},{value:"Histogram Calculation in OpenCV",id:"histogram-calculation-in-opencv",level:2},{value:"Apply the CAMshift Algorithm",id:"apply-the-camshift-algorithm",level:2}],p={toc:c};function h(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Shenzhen, China",src:n(81664).Z,width:"2385",height:"919"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#camshift"},"CAMshift")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#get-your-videostream"},"Get your Videostream")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#histogram-calculation-in-opencv"},"Histogram Calculation in OpenCV")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#apply-the-camshift-algorithm"},"Apply the CAMshift Algorithm"))),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv2-tracking-algorithm"},"Github Repo")),(0,i.kt)("h2",{id:"camshift"},"CAMshift"),(0,i.kt)("p",null,"One of the problems with the ",(0,i.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08"},"Meanshift Algorithm")," was that the region of interest always stayed at the same size when the object came closer to the camera or moved farther away. The region needs to adapt it's size with size and rotation of the target. The solution is called ",(0,i.kt)("strong",{parentName:"p"},"CAMshift")," (",(0,i.kt)("em",{parentName:"p"},"Continuously Adaptive Meanshift"),") published by Gary Bradsky in his paper \u201cComputer Vision Face Tracking for Use in a Perceptual User Interface\u201d in 1988."),(0,i.kt)("p",null,"It applies meanshift first. Once meanshift converges, it updates the size of the window. It also calculates the orientation of best fitting ellipse to it. Again it applies the meanshift with new scaled search window and previous window location. The process is continued until required accuracy is met."),(0,i.kt)("h2",{id:"get-your-videostream"},"Get your Videostream"),(0,i.kt)("p",null,"Get your RTSP video stream input and define a region of interest for the Meanshift algorithm:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# get video stream from IP camera\nprint("[INFO] starting video stream")\nvs = VideoStream(args["url"]).start()\n\n# first frame from stream\nframe = vs.read()\n# select region of interest\nbbox = cv2.selectROI(frame)\nx, y, w, h = bbox\ntrack_window = (x, y, w, h)\n# define area of bounding box as area of interest\nroi = frame[y:y+h, x:x+w]\n')),(0,i.kt)("h2",{id:"histogram-calculation-in-opencv"},"Histogram Calculation in OpenCV"),(0,i.kt)("p",null,"The Meanshift algorithm is going to use the histogram of your region of interest to track the object you selected above. But we have to convert the frame to to the HSV colour space and normalize it first:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n# get histogram for [0] blue, [1] green, [2] red channel\nroi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])\n# convert hist values 0-180 to a range between 0-1\nroi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n")),(0,i.kt)("p",null,"So now we use ",(0,i.kt)("a",{parentName:"p",href:"https://docs.opencv.org/4.x/d1/db7/tutorial_py_histogram_begins.html"},"cv.calcHist()")," function to find the histogram. Let's familiarize with the function and its parameters :"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"cv.calcHist(images, channels, mask, histSize, ranges[, hist","[, accumulate]","])")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"images")," : it is the source image of type uint8 or float32. it should be given in square brackets, ie, ",(0,i.kt)("inlineCode",{parentName:"li"},"[img]"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"channels")," : it is also given in square brackets. It is the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is ",(0,i.kt)("inlineCode",{parentName:"li"},"[0]"),". For color image, you can pass ",(0,i.kt)("inlineCode",{parentName:"li"},"[0]"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"[1]")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"[2]")," to calculate"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"histogram")," of blue, green or red channel respectively."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"mask"),' : mask image. To find histogram of full image, it is given as "None". But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask. (I will show an example later.)'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"histSize")," : this represents our BIN count. Need to be given in square brackets. For full scale, we pass ",(0,i.kt)("inlineCode",{parentName:"li"},"[256]")," for ",(0,i.kt)("strong",{parentName:"li"},"RGB")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"[180]")," for ",(0,i.kt)("strong",{parentName:"li"},"HSV"),".\nranges : this is our RANGE. Normally, it is ",(0,i.kt)("inlineCode",{parentName:"li"},"[0,256]")," for ",(0,i.kt)("strong",{parentName:"li"},"RGB")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"[0, 180]")," for ",(0,i.kt)("strong",{parentName:"li"},"HSV"),".")),(0,i.kt)("h2",{id:"apply-the-camshift-algorithm"},"Apply the CAMshift Algorithm"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"while True:\n    # get next frame\n    frame = vs.read()\n    if True:\n        # convert to hsv\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n        # compare blue channel of current with roi histogram\n        # https://docs.opencv.org/3.4.15/da/d7f/tutorial_back_projection.html\n        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n        # call meanshift() to find match of histogram in current frame\n        # and get the new coordinates\n        ok, track_window = cv2.meanShift(dst, (x, y, w, h), parameter)\n        if not ok:\n            print('[WARNING] track lost')\n        # now update the roi coordinates to new values\n        x, y, w, h = track_window\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)\n        # display track\n        cv2.imshow(\"Meanshift Track\", frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    else:\n        break\n")),(0,i.kt)("p",null,"The generated coordinates can be used to draw polylines around the calculated new position of our selected object:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:n(30317).Z,width:"639",height:"215"})))}h.isMDXComponent=!0},30317:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/OpenCV_CAMshift_Tracking_01-6df19bf659a15719cbcbca3093728cf7.gif"},81664:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a0b68587d9242bbb46a1f1aaab44216.jpg"}}]);