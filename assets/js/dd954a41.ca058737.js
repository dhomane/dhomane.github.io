"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[89069],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=c(n),m=o,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||r;return n?a.createElement(h,l(l({ref:t},p),{},{components:n})):a.createElement(h,l({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,l=new Array(r);l[0]=u;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:o,l[1]=i;for(var c=2;c<r;c++)l[c]=n[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},13426:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var a=n(87462),o=(n(67294),n(3905));const r={sidebar_position:7090,slug:"2021-11-09",title:"Tensorflow2 Crash Course - Part II",authors:"mpolinowski",tags:["Machine Learning","Python","Tensorflow"]},l=void 0,i={unversionedId:"IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/index",id:"IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/index",title:"Tensorflow2 Crash Course - Part II",description:"Mong Kok, Hongkong",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii",slug:"/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/2021-11-09",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/2021-11-09",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:7090,frontMatter:{sidebar_position:7090,slug:"2021-11-09",title:"Tensorflow2 Crash Course - Part II",authors:"mpolinowski",tags:["Machine Learning","Python","Tensorflow"]},sidebar:"tutorialSidebar",previous:{title:"Tensorflow2 Crash Course - Part III",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-10--tensorflow-crash-course-part-iii/2021-11-10"},next:{title:"Tensorflow Crash Course - Part I",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-08--tensorflow-crash-course-part-i/2021-11-08"}},s={},c=[{value:"Training",id:"training",level:2},{value:"Download Pre-trained Model",id:"download-pre-trained-model",level:3},{value:"Install TFOD",id:"install-tfod",level:3},{value:"Prepare your Model",id:"prepare-your-model",level:3},{value:"Train the Model",id:"train-the-model",level:3},{value:"Evaluate the Model",id:"evaluate-the-model",level:3}],p={toc:c};function d(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mong Kok, Hongkong",src:n(44039).Z,width:"1500",height:"544"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#training"},"Training"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#download-pre-trained-model"},"Download Pre-trained Model")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#install-tfod"},"Install TFOD")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#prepare-your-model"},"Prepare your Model")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#train-the-model"},"Train the Model")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#evaluate-the-model"},"Evaluate the Model"))))),(0,o.kt)("p",null,"This set of Notebooks provides a complete set of code to be able to train and leverage your own custom object detection model using the Tensorflow Object Detection API."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"This article is based on a ",(0,o.kt)("a",{parentName:"p",href:"https://www.youtube.com/channel/UCHXa4OpASJEwrHrLeIzw7Yg"},"Tutorial")," by ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/nicknochnack"},"@nicknochnack"),".")),(0,o.kt)("h2",{id:"training"},"Training"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-08--tensorflow-crash-course-part-i/2021-11-08"},"Tensorflow2 Crash Course Part I")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/2021-11-09"},"Tensorflow2 Crash Course Part II")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-10--tensorflow-crash-course-part-iii/2021-11-10"},"Tensorflow2 Crash Course Part III")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-11--tensorflow-crash-course-part-iv/2021-11-11"},"Tensorflow2 Crash Course Part IV")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-12--tensorflow-crash-course-part-v/2021-11-12"},"Tensorflow2 Crash Course Part V"))),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tensorflow2-starter"},"Github Repository")),(0,o.kt)("h3",{id:"download-pre-trained-model"},"Download Pre-trained Model"),(0,o.kt)("p",null,"Begin training process by opening ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/TFODCourse/blob/main/training_the_model.py.ipynb"},"training_the_model.py.ipynb"),", this notebook will walk you through installing Tensorflow Object Detection, making detections, saving and exporting your model:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Precision"),": True Positiv / (True Positiv + False Positive)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Recall"),": True Positiv / (True Positiv + False Negative)")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"},"Detection Models"),": Tensorflow provides a collection of detection models pre-trained on the COCO 2017 dataset. These models can be useful for out-of-the-box inference if you are interested in categories already in those datasets."),(0,o.kt)("p",null,"The model table gives you both a ",(0,o.kt)("strong",{parentName:"p"},"Speed")," parameter as well as a ",(0,o.kt)("strong",{parentName:"p"},"COCO mAP")," that correlates with the accuracy of the model. The higher the speed the less accurate the detection will become."),(0,o.kt)("h3",{id:"install-tfod"},"Install TFOD"),(0,o.kt)("p",null,"During this process the Notebook will install Tensorflow Object Detection. You should ideally receive a notification indicating that the API has installed successfully with the last line stating:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Tensorflow Object Detection Walkthrough",src:n(53303).Z,width:"1124",height:"471"})),(0,o.kt)("p",null,"If not, resolve installation errors by referring to the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/TFODCourse/blob/main/README.md"},"Error Guide.md")," in this folder."),(0,o.kt)("h3",{id:"prepare-your-model"},"Prepare your Model"),(0,o.kt)("p",null,"The Step ",(0,o.kt)("strong",{parentName:"p"},"Get your pretrained Model")," downloads the model you defined under ",(0,o.kt)("inlineCode",{parentName:"p"},"PRETRAINED_MODEL_PATH")," and unzips it. The following steps provide a ",(0,o.kt)("strong",{parentName:"p"},"Label Map")," with the labels you assigned to your images as well as the location of all your training and testing images for the ",(0,o.kt)("strong",{parentName:"p"},"TF Records"),"."),(0,o.kt)("p",null,"The model that was downloaded now has to be configured with the correct paths according to our project structure. This can be done in the following file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n")),(0,o.kt)("p",null,"The file is going to be copied to the trainings folder and all the missing pieces will be automatically filled out. Here we have the 5 instances that need to be defined:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED"\n  num_steps: 50000\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: "classification"\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: "PATH_TO_BE_CONFIGURED"\n  tf_record_input_reader {\n    input_path: "PATH_TO_BE_CONFIGURED"\n  }\n}\neval_config {\n  metrics_set: "coco_detection_metrics"\n  use_moving_averages: false\n}\neval_input_reader {\n  label_map_path: "PATH_TO_BE_CONFIGURED"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: "PATH_TO_BE_CONFIGURED"\n  }\n')),(0,o.kt)("p",null,"As well as the number of classes the model has to handle. By default this is set to 90. In our case we are only looking for 4 different hand gestures:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"num_classes: 4\n")),(0,o.kt)("h3",{id:"train-the-model"},"Train the Model"),(0,o.kt)("p",null,"You may choose to train the model from within the notebook. I have noticed however that training inside of a separate terminal you're able to display live loss metrics. To do so run the following command from the root dir of your app (and within the virtual environment):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=2000\n")),(0,o.kt)("p",null,"This will run a ",(0,o.kt)("inlineCode",{parentName:"p"},"2000")," steps trainings run for the initial training:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Tensorflow Object Detection Walkthrough",src:n(84985).Z,width:"1154",height:"499"})),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"How to check the GPU load on Linux ? Mem load went up during the run. CPU load stayed low - I guess this means the calculations were run on my GPU ? There is a strang warning in the logs ",(0,o.kt)("inlineCode",{parentName:"p"},"returning NUMA node zero")," - though apparently ",(0,o.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/69241922/tensor-flow-installation-success-but-returning-numa-node-zero-warning-on-ubuntu"},"you can ignore it"),":")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"2022-01-01 19:28:12.478293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-01 19:28:12.508530: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-01-01 19:28:13.763340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4691 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"UPDATE")," run the ",(0,o.kt)("inlineCode",{parentName:"p"},"nvidia-smi")," command and you will see the python process occupying the majority of your GPU resources:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Tensorflow Object Detection Walkthrough",src:n(1566).Z,width:"1310",height:"552"})),(0,o.kt)("p",null,"The training will print out an intermediate result every ",(0,o.kt)("inlineCode",{parentName:"p"},"100")," steps during which you should see a constant decrease of the ",(0,o.kt)("strong",{parentName:"p"},"Loss Parameter"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"INFO:tensorflow:Step 2000 per-step time 0.119s\nI0101 19:33:06.292452 140223989212992 model_lib_v2.py:705] Step 2000 per-step time 0.119s\nINFO:tensorflow:{'Loss/classification_loss': 0.09995355,\n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"All trainings data is stored in ",(0,o.kt)("inlineCode",{parentName:"p"},"/opt/Python/TFODCourse/Tensorflow/workspace/models/my_ssd_mobnet"),"!")),(0,o.kt)("h3",{id:"evaluate-the-model"},"Evaluate the Model"),(0,o.kt)("p",null,"Again, you can use the Jupyter notebook a=or just copy the command into your terminal:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --checkpoint_dir=Tensorflow/workspace/models/my_ssd_mobnet\n")),(0,o.kt)("p",null,"This will give you the average ",(0,o.kt)("strong",{parentName:"p"},"Precision")," and ",(0,o.kt)("strong",{parentName:"p"},"Recall")," for your model - both should ideally be ",(0,o.kt)("inlineCode",{parentName:"p"},"1")," and are already ok-ish with ",(0,o.kt)("inlineCode",{parentName:"p"},"0.706")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"0.713")," in my case:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Accumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.706\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.881\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.706\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.713\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.725\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.744\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.744\n")),(0,o.kt)("p",null,"You can optionally evaluate your model inside of ",(0,o.kt)("strong",{parentName:"p"},"Tensorboard"),". Once the model has been trained and you have run the evaluation command. Open ",(0,o.kt)("strong",{parentName:"p"},"Tensorboard")," for the ",(0,o.kt)("strong",{parentName:"p"},"Evaluation Metrics")," with the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"tensorboard --logdir=Tensorflow/workspace/models/my_ssd_mobnet/eval\n\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Tensorflow Object Detection Walkthrough",src:n(51652).Z,width:"1245",height:"877"})),(0,o.kt)("p",null,"Tensorboard will be accessible through your browser and you will be able to see metrics including mAP - mean Average Precision, and Recall. Currently we only have one data point. But the metrics will be tracked over every trainings run."),(0,o.kt)("p",null,"The board also gives us access to result images:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Tensorflow Object Detection Walkthrough",src:n(82437).Z,width:"800",height:"225"})),(0,o.kt)("p",null,"Your ",(0,o.kt)("strong",{parentName:"p"},"Loss Metrics")," are available from:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"tensorboard --logdir=Tensorflow/workspace/models/my_ssd_mobnet/train\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Tensorflow Object Detection Walkthrough",src:n(24872).Z,width:"1228",height:"908"})))}d.isMDXComponent=!0},53303:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Training_01-307a905f53a9069cbbc06fe4913b1517.png"},84985:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Training_02-5f9d608fa57599c8ce6228aeec3ce305.png"},51652:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Training_03-03b3d48256b9d8bfdbe6a06387cc0039.png"},24872:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Training_04-041a7249fe6dde5d3a3037a3016e1d3a.png"},82437:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Training_05-0e3bdb9e7e87ae7a16c737f4267586a9.png"},1566:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/nvidia-smi-1185fa54a20d998a9916232570a4f2fe.png"},44039:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-8629210580d5b66cf728356357923b9f.jpg"}}]);