"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[7377],{3905:(e,t,n)=>{n.d(t,{Zo:()=>h,kt:()=>g});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},h=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),p=c(n),g=i,d=p["".concat(l,".").concat(g)]||p[g]||m[g]||r;return n?a.createElement(d,o(o({ref:t},h),{},{components:n})):a.createElement(d,o({ref:t},h))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},39528:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=n(87462),i=(n(67294),n(3905));const r={sidebar_position:6050,slug:"2021-12-08",title:"OpenCV Meanshift Algorithm for Object Tracking",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},o=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/index",id:"IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/index",title:"OpenCV Meanshift Algorithm for Object Tracking",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking",slug:"/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"OpenCV",permalink:"/docs/tags/open-cv"}],version:"current",sidebarPosition:6050,frontMatter:{sidebar_position:6050,slug:"2021-12-08",title:"OpenCV Meanshift Algorithm for Object Tracking",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},sidebar:"tutorialSidebar",previous:{title:"OpenCV CAMshift Algorithm for Object Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09"},next:{title:"OpenCV Object Detection and Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-07--opencv-detection-and-tracking/2021-12-07"}},l={},c=[{value:"Meanshift",id:"meanshift",level:2},{value:"Get your Videostream",id:"get-your-videostream",level:2},{value:"Histogram Calculation in OpenCV",id:"histogram-calculation-in-opencv",level:2},{value:"Histogram Plot",id:"histogram-plot",level:4},{value:"Apply the Meanshift Algorithm",id:"apply-the-meanshift-algorithm",level:2}],h={toc:c};function m(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},h,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Shenzhen, China",src:n(78278).Z,width:"2385",height:"919"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#meanshift"},"Meanshift")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#get-your-videostream"},"Get your Videostream")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#histogram-calculation-in-opencv"},"Histogram Calculation in OpenCV"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#histogram-plot"},"Histogram Plot")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#apply-the-meanshift-algorithm"},"Apply the Meanshift Algorithm"))),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv2-tracking-algorithm"},"Github Repo")),(0,i.kt)("h2",{id:"meanshift"},"Meanshift"),(0,i.kt)("p",null,"The idea behind the ",(0,i.kt)("a",{parentName:"p",href:"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_video/py_meanshift/py_meanshift.html"},"Meanshift Algorithm")," is that every instance of the video is checked for the pixel distribution in that frame. We define an initial window, a region of interest (",(0,i.kt)("strong",{parentName:"p"},"ROI"),") which identifies the area of maximum pixel distribution. By doing so we are defining a colour histogram. The algorithm tries to keep track of that area in the video so that the ROI moves towards the region of maximum pixel distribution - it tries to maximize the overlap of the resulting histogram with the original histogram of the area we selected. The direction of movement depends upon the difference between the center of our tracking window and the centroid of all the k-pixels inside that window."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Disadvantages of using the Meanshift Algorithm"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The size of the ROI remains the same irrespective of the distance of the object from the camera."),(0,i.kt)("li",{parentName:"ul"},"The ROI will track the object only when it is inside the initial bounding box we define.")),(0,i.kt)("h2",{id:"get-your-videostream"},"Get your Videostream"),(0,i.kt)("p",null,"Get your RTSP video stream input and define a region of interest for the Meanshift algorithm:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# get video stream from IP camera\nprint("[INFO] starting video stream")\nvs = VideoStream(args["url"]).start()\n\n# first frame from stream\nframe = vs.read()\n# select region of interest\nbbox = cv2.selectROI(frame)\nx, y, w, h = bbox\ntrack_window = (x, y, w, h)\n# define area of bounding box as area of interest\nroi = frame[y:y+h, x:x+w]\n')),(0,i.kt)("h2",{id:"histogram-calculation-in-opencv"},"Histogram Calculation in OpenCV"),(0,i.kt)("p",null,"The Meanshift algorithm is going to use the histogram of your region of interest to track the object you selected above. But we have to convert the frame to to the HSV colour space and normalize it first:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n# get histogram for [0] blue, [1] green, [2] red channel\nroi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])\n# convert hist values 0-180 to a range between 0-1\nroi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n")),(0,i.kt)("p",null,"So now we use ",(0,i.kt)("a",{parentName:"p",href:"https://docs.opencv.org/4.x/d1/db7/tutorial_py_histogram_begins.html"},"cv.calcHist()")," function to find the histogram. Let's familiarize with the function and its parameters :"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"cv.calcHist(images, channels, mask, histSize, ranges[, hist","[, accumulate]","])")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"images")," : it is the source image of type uint8 or float32. it should be given in square brackets, ie, ",(0,i.kt)("inlineCode",{parentName:"li"},"[img]"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"channels")," : it is also given in square brackets. It is the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is ",(0,i.kt)("inlineCode",{parentName:"li"},"[0]"),". For color image, you can pass ",(0,i.kt)("inlineCode",{parentName:"li"},"[0]"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"[1]")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"[2]")," to calculate"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"histogram")," of blue, green or red channel respectively."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"mask"),' : mask image. To find histogram of full image, it is given as "None". But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask. (I will show an example later.)'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"histSize")," : this represents our BIN count. Need to be given in square brackets. For full scale, we pass ",(0,i.kt)("inlineCode",{parentName:"li"},"[256]")," for ",(0,i.kt)("strong",{parentName:"li"},"RGB")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"[180]")," for ",(0,i.kt)("strong",{parentName:"li"},"HSV"),".\nranges : this is our RANGE. Normally, it is ",(0,i.kt)("inlineCode",{parentName:"li"},"[0,256]")," for ",(0,i.kt)("strong",{parentName:"li"},"RGB")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"[0, 180]")," for ",(0,i.kt)("strong",{parentName:"li"},"HSV"),".")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"img = cv.imread('image_hsv.jpg',0)\nhist = cv.calcHist([img],[0],None,[180],[0, 180])\n")),(0,i.kt)("h4",{id:"histogram-plot"},"Histogram Plot"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"with OpenCV")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"img = cv.imread('image_rgb.jpg',0)\n# create a mask\nmask = np.zeros(img.shape[:2], np.uint8)\nmask[100:300, 100:400] = 255\nmasked_img = cv.bitwise_and(img,img,mask = mask)\n# Calculate histogram with mask and without mask\n# Check third argument for mask\nhist_full = cv.calcHist([img],[0],None,[256],[0,256])\nhist_mask = cv.calcHist([img],[0],mask,[256],[0,256])\nplt.subplot(221), plt.imshow(img, 'gray')\nplt.subplot(222), plt.imshow(mask,'gray')\nplt.subplot(223), plt.imshow(masked_img, 'gray')\nplt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)\nplt.xlim([0,256])\nplt.show()\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"with Matplotlib")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('home.jpg')\ncolor = ('b','g','r')\nfor i,col in enumerate(color):\n    histr = cv.calcHist([img],[i],None,[255],[0,255])\n    plt.plot(histr,color = col)\n    plt.xlim([0,255])\nplt.show()\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:n(91072).Z,width:"1247",height:"513"})),(0,i.kt)("h2",{id:"apply-the-meanshift-algorithm"},"Apply the Meanshift Algorithm"),(0,i.kt)("p",null,"Now that we have the ROI coordinates and the corresponding histogram we can add a while loop that keeps fetching new frames from the video stream. We are using the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4.15/da/d7f/tutorial_back_projection.html"},"OpenCV Back Projection")," to compare each incoming new frame with the histogram of our ROI. The Meanshift tracking algorithm is then using the generated density function to find a best match for the new coordinates of our region of interest:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# set up the termination criteria, either 10 iteration or move by at least 1 pt\nparameter = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n\n# now loop through the rest of avail frames\n# and use meanshift to track defined roi\nwhile True:\n    # get next frame\n    frame = vs.read()\n    if True:\n        # convert to hsv\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n        # compare blue channel of current with roi histogram\n        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n        # call meanshift() to find match of histogram in current frame\n        # and get the new coordinates\n        ok, track_window = cv2.meanShift(dst, (x, y, w, h), parameter)\n        if not ok:\n            print('[WARNING] track lost')\n        # now update the roi coordinates to new values\n        x, y, w, h = track_window\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 5)\n        # display track\n        cv2.imshow(\"Meanshift Track\", frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    else:\n        break\n")),(0,i.kt)("p",null,"A representation of the density function ",(0,i.kt)("inlineCode",{parentName:"p"},"dst")," used by the Meanshift algorithm to track down the region of interest:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:n(90769).Z,width:"639",height:"218"})),(0,i.kt)("p",null,"The generated coordinates can be used to draw a rectangle around the calculated new position of our selected object:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:n(74966).Z,width:"639",height:"218"})))}m.isMDXComponent=!0},91072:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/OpenCV_Meanshift_Tracking_01-b5a765ae9b4984b27cee6883ad6167a5.png"},90769:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/OpenCV_Meanshift_Tracking_02-a3ab7e5b412f7adc4f028b2bbdbcfac7.gif"},74966:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/OpenCV_Meanshift_Tracking_03-ed481e1872821ee1689ddd282ace2477.gif"},78278:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a0b68587d9242bbb46a1f1aaab44216.jpg"}}]);