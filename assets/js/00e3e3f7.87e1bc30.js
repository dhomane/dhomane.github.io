"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[96278],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>g});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var o=a.createContext({}),p=function(e){var t=a.useContext(o),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(o.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,o=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),c=p(n),g=i,h=c["".concat(o,".").concat(g)]||c[g]||d[g]||r;return n?a.createElement(h,s(s({ref:t},m),{},{components:n})):a.createElement(h,s({ref:t},m))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,s=new Array(r);s[0]=c;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=e,l.mdxType="string"==typeof e?e:i,s[1]=l;for(var p=2;p<r;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},63743:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=n(87462),i=(n(67294),n(3905));const r={sidebar_position:4870,slug:"2022-12-21",title:"Tensorflow Deep Dream",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"DeepDream is an experiment that visualizes the patterns learned by a neural network."},s=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/index",id:"IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/index",title:"Tensorflow Deep Dream",description:"DeepDream is an experiment that visualizes the patterns learned by a neural network.",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream",slug:"/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/2022-12-21",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/2022-12-21",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4870,frontMatter:{sidebar_position:4870,slug:"2022-12-21",title:"Tensorflow Deep Dream",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"DeepDream is an experiment that visualizes the patterns learned by a neural network."},sidebar:"tutorialSidebar",previous:{title:"Tensorflow Downsampling",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/2022-12-21"},next:{title:"Tensorflow Representation Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-20-tf-representation/2022-12-19"}},o={},p=[{value:"Prepare the Feature Extraction Model",id:"prepare-the-feature-extraction-model",level:2},{value:"Prepare your Image",id:"prepare-your-image",level:2},{value:"Calculate Loss",id:"calculate-loss",level:2},{value:"Gradient Ascent",id:"gradient-ascent",level:2},{value:"Main Loop",id:"main-loop",level:2},{value:"Finetuning",id:"finetuning",level:2},{value:"Octaves",id:"octaves",level:3},{value:"Scaling up with Tiles",id:"scaling-up-with-tiles",level:3}],m={toc:p};function d(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},m,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Guangzhou, China",src:n(39270).Z,width:"1500",height:"383"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#prepare-the-feature-extraction-model"},"Prepare the Feature Extraction Model")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#prepare-your-image"},"Prepare your Image")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#calculate-loss"},"Calculate Loss")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#gradient-ascent"},"Gradient Ascent")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#main-loop"},"Main Loop")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#finetuning"},"Finetuning"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#octaves"},"Octaves")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#scaling-up-with-tiles"},"Scaling up with Tiles"))))),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-dreaming"},"Github")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"},"DeepDream")," is an experiment that visualizes the patterns learned by a neural network. Similar to when a child watches clouds and tries to interpret random shapes, DeepDream over-interprets and enhances the patterns it sees in an image."),(0,i.kt)("p",null,"It does so by forwarding an image through the network, then calculating the gradient of the image with respect to the activations of a particular layer. The image is then modified to increase these activations, enhancing the patterns seen by the network, and resulting in a dream-like image. "),(0,i.kt)("p",null,"The following code is taken from one of the ",(0,i.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/tutorials/generative/deepdream"},"official Tensorflow Tutorials"),":"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Deep Dream Steps"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feed an image into a trained neural network"),(0,i.kt)("li",{parentName:"ul"},"Using different layers will result in different dream-like images. Deeper layers respond to higher-level features (such as eyes and faces), while earlier layers respond to simpler features (such as edges, shapes, and textures)."),(0,i.kt)("li",{parentName:"ul"},"Calculate the activations coming out from the layer of interest"),(0,i.kt)("li",{parentName:"ul"},"Calculate the gradient of the activations"),(0,i.kt)("li",{parentName:"ul"},"Add features to your image that will increase those activations"),(0,i.kt)("li",{parentName:"ul"},"Iterate and repeat")),(0,i.kt)("h2",{id:"prepare-the-feature-extraction-model"},"Prepare the Feature Extraction Model"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/fchollet/deep-learning-models/releases"},"model with trained weights can be downloaded from Github")," and is available pre-trained based on different architectures. The base model can be directly downloaded using Keras:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"# import base model\nbase_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n")),(0,i.kt)("p",null,'The idea in DeepDream is to choose a layer (or layers) and maximize the "loss" in a way that the image increasingly "excites" the layers. The complexity of the features incorporated depends on layers chosen by you, i.e, lower layers produce strokes or simple patterns, while deeper layers give sophisticated features in images, or even whole objects.'),(0,i.kt)("p",null,"The layers of interest are those where the convolutions are concatenated. There are 11 of these layers named ",(0,i.kt)("inlineCode",{parentName:"p"},"mixed0")," though ",(0,i.kt)("inlineCode",{parentName:"p"},"mixed10"),". Deeper layers (those with a higher index) ",(0,i.kt)("strong",{parentName:"p"},"will take longer to train on")," since the gradient computation is deeper."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"# Maximize the activations of these layers\nnames = ['mixed3', 'mixed5']\nlayers = [base_model.get_layer(name).output for name in names]\n\n# Create the feature extraction model\ndream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n")),(0,i.kt)("h2",{id:"prepare-your-image"},"Prepare your Image"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"url = 'https://img.freepik.com/premium-photo/cartoon-ninja-girl-beautiful-japanese-ninja-girl-concept-art-digital-painting-fantasy-illustration_743201-2848.jpg'\n\n# Download an image and read it into a NumPy array.\ndef download(url, max_dim=None):\n  name = url.split('/')[-1]\n  image_path = tf.keras.utils.get_file(name, origin=url)\n  img = Image.open(image_path)\n  if max_dim:\n    img.thumbnail((max_dim, max_dim))\n  return np.array(img)\n\n# Normalize an image\ndef deprocess(img):\n  img = 255*(img + 1.0)/2.0\n  return tf.cast(img, tf.uint8)\n\n\n# Downsizing the image makes it easier to work with.\noriginal_img = download(url, max_dim=500)\n")),(0,i.kt)("h2",{id:"calculate-loss"},"Calculate Loss"),(0,i.kt)("p",null,"The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. Normally, loss is a quantity you wish to minimize via gradient descent. In DeepDream, you will maximize this loss via gradient ascent."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"def calc_loss(img, model):\n  # Pass forward the image through the model to retrieve the activations.\n  # Converts the image into a batch of size 1.\n  img_batch = tf.expand_dims(img, axis=0) # Convert into batch format\n  layer_activations = model(img_batch) # Run the model\n  if len(layer_activations) == 1:\n    layer_activations = [layer_activations]\n\n  losses = [] # accumulator to hold all the losses\n  # loop over activations and append losses to array\n  for act in layer_activations:\n    # calculate mean of each activation\n    loss = tf.math.reduce_mean(act)\n    # append loss for each activation \n    losses.append(loss)\n\n  # calculate sum\n  return  tf.reduce_sum(losses)\n")),(0,i.kt)("h2",{id:"gradient-ascent"},"Gradient Ascent"),(0,i.kt)("p",null,"Once you have calculated the loss for the chosen layers, all that is left is to calculate the gradients with respect to the image, and add them to the original image. Adding the gradients to the image enhances the patterns seen by the network."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},'class DeepDream(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(\n      input_signature=(\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n        tf.TensorSpec(shape=[], dtype=tf.int32),\n        tf.TensorSpec(shape=[], dtype=tf.float32),)\n  )\n  def __call__(self, img, steps, step_size):\n      print("Tracing")\n      loss = tf.constant(0.0)\n      for n in tf.range(steps):\n        with tf.GradientTape() as tape:\n          # This needs gradients relative to `img`\n          # `GradientTape` only watches `tf.Variable`s by default\n          tape.watch(img)\n          loss = calc_loss(img, self.model)\n\n        # Calculate the gradient of the loss with respect to the pixels of the input image.\n        gradients = tape.gradient(loss, img)\n\n        # Normalize the gradients.\n        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n\n        # In gradient ascent, the "loss" is maximized so that the input image increasingly "excites" the layers.\n        # You can update the image by directly adding the gradients (because they\'re the same shape!)\n        img = img + gradients*step_size\n        img = tf.clip_by_value(img, -1, 1)\n\n      return loss, img\n\ndeepdream = DeepDream(dream_model)\n')),(0,i.kt)("h2",{id:"main-loop"},"Main Loop"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},'def run_deep_dream_simple(img, steps=100, step_size=0.01):\n  # Convert from uint8 to the range expected by the model.\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\n  img = tf.convert_to_tensor(img)\n  step_size = tf.convert_to_tensor(step_size)\n  steps_remaining = steps\n  step = 0\n  while steps_remaining:\n    if steps_remaining>100:\n      run_steps = tf.constant(100)\n    else:\n      run_steps = tf.constant(steps_remaining)\n    steps_remaining -= run_steps\n    step += run_steps\n\n    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n    print ("Step {}, loss {}".format(step, loss))\n\n\n  result = deprocess(img)\n  plt.figure(figsize=(12,12))\n  plt.imshow(result)\n  plt.show()\n\n  return result\n\n\ndream_img = run_deep_dream_simple(img=original_img, \n                                  steps=100, step_size=0.01)\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Tensorflow Transfer Learning",src:n(74214).Z,width:"1190",height:"839"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("inlineCode",{parentName:"p"},"Step 100, loss 2.1992263793945312"))),(0,i.kt)("h2",{id:"finetuning"},"Finetuning"),(0,i.kt)("h3",{id:"octaves"},"Octaves"),(0,i.kt)("p",null,"Pretty good, but the output is noisy. One approach that addresses all these problems is applying gradient ascent at different scales. This will allow patterns generated at smaller scales to be incorporated into patterns at higher scales and filled in with additional detail. To do this you can perform the previous gradient ascent approach, then increase the size of the image (which is referred to as an octave), and repeat this process for multiple octaves."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"start = time.time()\nOCTAVE_SCALE = 1.30\n\nimg = tf.constant(np.array(original_img))\nbase_shape = tf.shape(img)[:-1]\nfloat_base_shape = tf.cast(base_shape, tf.float32)\n\nfor n in range(-2, 3):\n  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n\n  img = tf.image.resize(img, new_shape).numpy()\n\n  img = run_deep_dream_simple(img=img, steps=50, step_size=0.01)\n\ndisplay.clear_output(wait=True)\nimg = tf.image.resize(img, base_shape)\nimg = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\nshow(img)\n\nend = time.time()\nend-start\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Tensorflow Transfer Learning",src:n(87471).Z,width:"2541",height:"626"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("inlineCode",{parentName:"p"},"Step 50, loss 1.9624563455581665"),"\n",(0,i.kt)("inlineCode",{parentName:"p"},"Step 50, loss 2.1029279232025146"),"\n",(0,i.kt)("inlineCode",{parentName:"p"},"Step 50, loss 2.0884666442871094"),"\n",(0,i.kt)("inlineCode",{parentName:"p"},"Step 50, loss 2.07767915725708"),"\n",(0,i.kt)("inlineCode",{parentName:"p"},"Step 50, loss 2.0698883533477783"))),(0,i.kt)("h3",{id:"scaling-up-with-tiles"},"Scaling up with Tiles"),(0,i.kt)("p",null,"One thing to consider is that as the image increases in size, so will the time and memory necessary to perform the gradient calculation. The above octave implementation will not work on very large images, or many octaves. To avoid this issue you can split the image into tiles and compute the gradient for each tile."),(0,i.kt)("p",null,"Applying random shifts to the image before each tiled computation prevents tile seams from appearing:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"def random_roll(img, maxroll):\n  # Randomly shift the image to avoid tiled boundaries.\n  shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n  img_rolled = tf.roll(img, shift=shift, axis=[0,1])\n  return shift, img_rolled\n\nshift, img_rolled = random_roll(np.array(original_img), 512)\nshow(img_rolled)\n")),(0,i.kt)("p",null,"And implement the random shifts into the deepdream function defined earlier:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"class TiledGradients(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(\n      input_signature=(\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n        tf.TensorSpec(shape=[2], dtype=tf.int32),\n        tf.TensorSpec(shape=[], dtype=tf.int32),)\n  )\n  def __call__(self, img, img_size, tile_size=512):\n    shift, img_rolled = random_roll(img, tile_size)\n\n    # Initialize the image gradients to zero.\n    gradients = tf.zeros_like(img_rolled)\n\n    # Skip the last tile, unless there's only one tile.\n    xs = tf.range(0, img_size[1], tile_size)[:-1]\n    if not tf.cast(len(xs), bool):\n      xs = tf.constant([0])\n    ys = tf.range(0, img_size[0], tile_size)[:-1]\n    if not tf.cast(len(ys), bool):\n      ys = tf.constant([0])\n\n    for x in xs:\n      for y in ys:\n        # Calculate the gradients for this tile.\n        with tf.GradientTape() as tape:\n          # This needs gradients relative to `img_rolled`.\n          # `GradientTape` only watches `tf.Variable`s by default.\n          tape.watch(img_rolled)\n\n          # Extract a tile out of the image.\n          img_tile = img_rolled[y:y+tile_size, x:x+tile_size]\n          loss = calc_loss(img_tile, self.model)\n\n        # Update the image gradients for this tile.\n        gradients = gradients + tape.gradient(loss, img_rolled)\n\n    # Undo the random shift applied to the image and its gradients.\n    gradients = tf.roll(gradients, shift=-shift, axis=[0,1])\n\n    # Normalize the gradients.\n    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n\n    return gradients\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Tensorflow Transfer Learning",src:n(13097).Z,width:"400",height:"327"})))}d.isMDXComponent=!0},74214:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Transfer_Learning_01-0e329df3d3930cc7eed887f64dd0d4e2.png"},87471:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Transfer_Learning_02-66e3f66e9454053741e4706e1367fc0d.png"},13097:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Tensorflow_Transfer_Learning_03-d7afb0cff0778e03963bebeba0834f62.gif"},39270:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);