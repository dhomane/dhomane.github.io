"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[89579],{3905:(e,a,n)=>{n.d(a,{Zo:()=>d,kt:()=>g});var t=n(67294);function o(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function i(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function l(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?i(Object(n),!0).forEach((function(a){o(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function r(e,a){if(null==e)return{};var n,t,o=function(e,a){if(null==e)return{};var n,t,o={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||(o[n]=e[n]);return o}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=t.createContext({}),s=function(e){var a=t.useContext(p),n=a;return e&&(n="function"==typeof e?e(a):l(l({},a),e)),n},d=function(e){var a=s(e.components);return t.createElement(p.Provider,{value:a},e.children)},m={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},c=t.forwardRef((function(e,a){var n=e.components,o=e.mdxType,i=e.originalType,p=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),c=s(n),g=o,u=c["".concat(p,".").concat(g)]||c[g]||m[g]||i;return n?t.createElement(u,l(l({ref:a},d),{},{components:n})):t.createElement(u,l({ref:a},d))}));function g(e,a){var n=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var i=n.length,l=new Array(i);l[0]=c;var r={};for(var p in a)hasOwnProperty.call(a,p)&&(r[p]=a[p]);r.originalType=e,r.mdxType="string"==typeof e?e:o,l[1]=r;for(var s=2;s<i;s++)l[s]=n[s];return t.createElement.apply(null,l)}return t.createElement.apply(null,n)}c.displayName="MDXCreateElement"},45962:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>p,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>s});var t=n(87462),o=(n(67294),n(3905));const i={sidebar_position:6020,slug:"2022-02-15",title:"Yolo App - Data Collection",authors:"mpolinowski",tags:["Machine Learning","Python","YOLO"]},l=void 0,r={unversionedId:"IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/index",id:"IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/index",title:"Yolo App - Data Collection",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data",slug:"/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"YOLO",permalink:"/docs/tags/yolo"}],version:"current",sidebarPosition:6020,frontMatter:{sidebar_position:6020,slug:"2022-02-15",title:"Yolo App - Data Collection",authors:"mpolinowski",tags:["Machine Learning","Python","YOLO"]},sidebar:"tutorialSidebar",previous:{title:"Yolo App - Train a Model with Tensorflow",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-16--yolo-app-tensorflow-model/2022-02-16"},next:{title:"OpenCV Optical Flow Algorithm for Object Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/2021-12-10"}},p={},s=[{value:"Project Setup",id:"project-setup",level:2},{value:"Data Collection",id:"data-collection",level:2},{value:"Image Labeling",id:"image-labeling",level:3},{value:"Get Bounding Boxes Coordinates",id:"get-bounding-boxes-coordinates",level:3},{value:"Get Image Files for Each Bounding Box",id:"get-image-files-for-each-bounding-box",level:3},{value:"Draw Bounding Box on Images",id:"draw-bounding-box-on-images",level:3},{value:"Normalize Data",id:"normalize-data",level:3},{value:"Divide into Training and Testing Data Set",id:"divide-into-training-and-testing-data-set",level:3}],d={toc:s};function m(e){let{components:a,...i}=e;return(0,o.kt)("wrapper",(0,t.Z)({},d,i,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Shenzhen, China",src:n(11738).Z,width:"1500",height:"688"})),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15"},"Prepare your Images and get Data")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-16--yolo-app-tensorflow-model/2022-02-16"},"Train your Tensorflow Model")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-17--yolo-app-prediction-pipeline/2022-02-17"},"Use your Model to do Predictions")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/2022-02-18"},"Use Tesseract to Read Number Plates")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-19--yolo-app-flask/2022-02-19"},"Flask Web Application")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-20--yolo-app-yolov5-data-prep/2022-02-20"},"Yolo v5 - Data Prep"))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#project-setup"},"Project Setup")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#data-collection"},"Data Collection"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#image-labeling"},"Image Labeling")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#get-bounding-boxes-coordinates"},"Get Bounding Boxes Coordinates")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#get-image-files-for-each-bounding-box"},"Get Image Files for Each Bounding Box")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#draw-bounding-box-on-images"},"Draw Bounding Box on Images")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#normalize-data"},"Normalize Data")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#divide-into-training-and-testing-data-set"},"Divide into Training and Testing Data Set"))))),(0,o.kt)("h2",{id:"project-setup"},"Project Setup"),(0,o.kt)("p",null," Create a ",(0,o.kt)("inlineCode",{parentName:"p"},"dependencies.txt")," file and install all dependencies:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"opencv-python==4.5.5.62\ntensorflow-gpu==2.8.0\nnotebook\npandas\nnumpy\nmatplotlib\nsklearn\npytesseract\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note"),": that I am using GPU accelerated version of Tensorflow for Nvidia GPUs. Replace ",(0,o.kt)("inlineCode",{parentName:"p"},"tensorflow-gpu")," with ",(0,o.kt)("inlineCode",{parentName:"p"},"tensorflow")," if you don't have a compatible graphic card in your PC."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install -r dependencies.txt\n")),(0,o.kt)("p",null,"Verify that OpenCV and Tensorflow was installed by creating and executing ",(0,o.kt)("inlineCode",{parentName:"p"},"test.py"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import cv2\nimport tensorflow as tf\n\nprint('Tensorflow Version: ' + tf.__version__)\nprint('OpenCV Version: ' + cv2.__version__)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python test.py \nTensorflow Version: 2.8.0\nOpenCV Version: 4.5.4\n")),(0,o.kt)("h2",{id:"data-collection"},"Data Collection"),(0,o.kt)("h3",{id:"image-labeling"},"Image Labeling"),(0,o.kt)("p",null,"I can use Google to collect photos of cars with visible license plates. I have to label those images so that they can be used for the Tensorflow training. To lable the images we are going to use ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/tzutalin/labelImg"},"labelImg"),", which is a graphical image annotation tool."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip3 install labelImg\n\nSuccessfully installed PyQt5-Qt5-5.15.2 PyQt5-sip-12.9.0 labelImg-1.8.3 pyqt5-5.15.6\n")),(0,o.kt)("p",null,"Start the Software from your console with ",(0,o.kt)("inlineCode",{parentName:"p"},"labelImg")," and label all your trainings images:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Yolo App Data Collection",src:n(99163).Z,width:"1016",height:"608"})),(0,o.kt)("p",null,"An example XML label generated by this process looks like:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<annotation>\n    <folder>d</folder>\n    <filename>N3.jpeg</filename>\n    <path>./resources/car.jpg</path>\n    <source>\n        <database>Unknown</database>\n    </source>\n    <size>\n        <width>932</width>\n        <height>699</height>\n        <depth>3</depth>\n    </size>\n    <segmented>0</segmented>\n    <object>\n        <name>num_plate</name>\n        <pose>Unspecified</pose>\n        <truncated>0</truncated>\n        <difficult>0</difficult>\n        <bndbox>\n            <xmin>73</xmin>\n            <ymin>381</ymin>\n            <xmax>260</xmax>\n            <ymax>462</ymax>\n        </bndbox>\n    </object>\n</annotation>\n")),(0,o.kt)("h3",{id:"get-bounding-boxes-coordinates"},"Get Bounding Boxes Coordinates"),(0,o.kt)("p",null,"I now need to extract the ",(0,o.kt)("strong",{parentName:"p"},"Bounding Box")," coordinates ",(0,o.kt)("inlineCode",{parentName:"p"},"xmin"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"ymin"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"xmax")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"ymax")," from the ",(0,o.kt)("strong",{parentName:"p"},"XML")," files and write them into ",(0,o.kt)("strong",{parentName:"p"},"CSV"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"jupyter notebook\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import pandas as pd\nimport xml.etree.ElementTree as xet\nfrom glob import glob\n\n# Get all generated image XML labels\npath = glob('../labels/*.xml')\n\n# Create empty label dictionary\nlabels = dict(filepath=[], xmin=[], ymin=[], xmax=[], ymax=[])\n# Extract bounding box coordinates for all labels\nfor filename in path:\n    info = xet.parse(filename)\n    root = info.getroot()\n    member_object = root.find('object')\n    labels_info = member_object.find('bndbox')\n    xmin = int(labels_info.find('xmin').text)\n    ymin = int(labels_info.find('ymin').text)\n    xmax = int(labels_info.find('xmax').text)\n    ymax = int(labels_info.find('ymax').text)\n    # Append values to dictionary\n    labels['filepath'].append(filename)\n    labels['xmin'].append(xmin)\n    labels['ymin'].append(ymin)\n    labels['xmax'].append(xmax)\n    labels['ymax'].append(ymax)\n\n# Create data frame from dictionary\ndf = pd.DataFrame(labels)\n\n# Write data frame to CSV\ndf.to_csv('../labels/labels.csv')\n")),(0,o.kt)("h3",{id:"get-image-files-for-each-bounding-box"},"Get Image Files for Each Bounding Box"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as xet\nimport os\nimport cv2\n\ndf = pd.read_csv('../labels/labels.csv')\n\n# Find image file path for a given label\ndef getImagePath(filename):\n    image = xet.parse(filename).getroot().find('filename').text\n    image_filepath = os.path.join('../resources', image)\n    return image_filepath\n\n# Select labels and find corresponding images\nimage_paths = list(df['filepath'].apply(getImagePath))\n")),(0,o.kt)("h3",{id:"draw-bounding-box-on-images"},"Draw Bounding Box on Images"),(0,o.kt)("p",null,"To verify that everything is working we can use the bounding box coordinates to draw a rectangle onto the corresponding image:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"# Get image path by index\npath = image_paths[0]\nimg = cv2.imread(path)\n\n# Draw bounding box onto image\n# Coordinates copied from generated label\ncv2.rectangle(img,(1093,645),(1396,727),(0,255,128),3)\n\n# Make window with name Test resizeable\ncv2.namedWindow('Test', cv2.WINDOW_NORMAL)\n\n# Display selected image in Test window\ncv2.imshow('Test', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Yolo App Data Collection",src:n(32286).Z,width:"985",height:"264"})),(0,o.kt)("h3",{id:"normalize-data"},"Normalize Data"),(0,o.kt)("p",null,"The models I am going to use later have been trained on a specific image size. I have to normalize all images and the generated bounding boxes to fit this requirement - e.g. an file size of ",(0,o.kt)("strong",{parentName:"p"},"224x224 pixels"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"from tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# Get array coordinates from labels \nlabels = df.iloc[:,2:].values\n\ndata = []\noutput = []\n\n# Loop over all images and normalize\nfor i in range(len(image_paths)):\n\n    # Get image path by index\n    image = image_paths[i]\n\n    # Get image dimensions from it' s shape\n    image_array = cv2.imread(image)\n    h,w,d = image_array.shape\n\n    # Normalize image size to fit tf model (input)\n    load_image = load_img(image,target_size=(224,224))\n    load_image_array = img_to_array(load_image)\n    norm_load_image_array = load_image_array/255\n\n    # Normalize coordinates from labels (output)\n    xmin, xmax, ymin, ymax = labels[i]\n    nxmin, nxmax = xmin/w, xmax/w\n    nymin, nymax = ymin/h, ymax/h\n    label_norm = (nxmin,nxmax,nymin,nymax)\n    \n    # Append results to output arrays\n    data.append(norm_load_image_array)\n    output.append(label_norm)\n\nX = np.array(data, dtype=np.float32)\nY = np.array(output, dtype=np.float32)\n")),(0,o.kt)("h3",{id:"divide-into-training-and-testing-data-set"},"Divide into Training and Testing Data Set"),(0,o.kt)("p",null,"Divide the training images and labels by a 80:20 split:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.model_selection import train_test_split\n\nX = np.array(data, dtype=np.float32)\nY = np.array(output, dtype=np.float32)\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, random_state=0)\n")),(0,o.kt)("p",null,"Now I am ready to continue training my Tensorflow model to be able to detect license plates!"))}m.isMDXComponent=!0},99163:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/Yolo_App_Data_Collection_01-e3097a2b90610e2a985d708c794db6d8.png"},32286:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/Yolo_App_Data_Collection_02-b32c7b79866fdb49928eecae6a61d98e.png"},11738:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ffe80356d19fb4b090a3bef79b45aab3.jpg"}}]);