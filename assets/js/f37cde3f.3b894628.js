"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[41208],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>u});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=l(a),u=i,f=m["".concat(c,".").concat(u)]||m[u]||p[u]||r;return a?n.createElement(f,o(o({ref:t},d),{},{components:a})):n.createElement(f,o({ref:t},d))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var l=2;l<r;l++)o[l]=a[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},51:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var n=a(87462),i=(a(67294),a(3905));const r={sidebar_position:9060,slug:"2022-09-17",title:"Python - Video Processing with OpenCV",authors:"mpolinowski",tags:["Python"]},o=void 0,s={unversionedId:"Development/Python/2022-09-17-python-video-processing/index",id:"Development/Python/2022-09-17-python-video-processing/index",title:"Python - Video Processing with OpenCV",description:"Sham Sui Po, Hong Kong",source:"@site/docs/Development/Python/2022-09-17-python-video-processing/index.md",sourceDirName:"Development/Python/2022-09-17-python-video-processing",slug:"/Development/Python/2022-09-17-python-video-processing/2022-09-17",permalink:"/docs/Development/Python/2022-09-17-python-video-processing/2022-09-17",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Development/Python/2022-09-17-python-video-processing/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"}],version:"current",sidebarPosition:9060,frontMatter:{sidebar_position:9060,slug:"2022-09-17",title:"Python - Video Processing with OpenCV",authors:"mpolinowski",tags:["Python"]},sidebar:"tutorialSidebar",previous:{title:"Python - Text Processing with",permalink:"/docs/Development/Python/2022-10-05-python-text-processing/2022-10-05"},next:{title:"Web Scraping Essentials with Python",permalink:"/docs/Development/Python/2022-06-27-python-web-scraping/2022-06-27"}},c={},l=[{value:"Basic Video Operations",id:"basic-video-operations",level:2},{value:"Getting Video Information",id:"getting-video-information",level:3},{value:"Extracting Frames",id:"extracting-frames",level:2},{value:"Extract Frame at Timestamp",id:"extract-frame-at-timestamp",level:3},{value:"Detect Faces using OpenCV Cascades &amp; Alarm Recordings",id:"detect-faces-using-opencv-cascades--alarm-recordings",level:2},{value:"Still Images",id:"still-images",level:3},{value:"Videos",id:"videos",level:3},{value:"IP Camera Streams",id:"ip-camera-streams",level:3},{value:"Continuous Video Display with Face Detection",id:"continuous-video-display-with-face-detection",level:2}],d={toc:l};function p(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,n.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Sham Sui Po, Hong Kong",src:a(11278).Z,width:"1500",height:"548"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#basic-video-operations"},"Basic Video Operations"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#getting-video-information"},"Getting Video Information")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#extracting-frames"},"Extracting Frames"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#extract-frame-at-timestamp"},"Extract Frame at Timestamp")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#detect-faces-using-opencv-cascades--alarm-recordings"},"Detect Faces using OpenCV Cascades \\& Alarm Recordings"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#still-images"},"Still Images")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#videos"},"Videos")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#ip-camera-streams"},"IP Camera Streams")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#continuous-video-display-with-face-detection"},"Continuous Video Display with Face Detection"))),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"see ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv-video-processing"},"Git Repository")," for source code.")),(0,i.kt)("h2",{id:"basic-video-operations"},"Basic Video Operations"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install opencv-python\nRequirement already satisfied: opencv-python in /usr/lib/python3.10/site-packages (4.5.5.62)\n")),(0,i.kt)("h3",{id:"getting-video-information"},"Getting Video Information"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"import cv2\n\nvideo = cv2.VideoCapture('security_cam.mp4')\n\nwidth = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = int(video.get(cv2.CAP_PROP_FPS))\n\nprint(width, height, frames, fps)\n")),(0,i.kt)("p",null,"The security camera footage has a ",(0,i.kt)("inlineCode",{parentName:"p"},"1440p")," resolution and consists of ",(0,i.kt)("inlineCode",{parentName:"p"},"451")," frames that are displayed at a frame rate of ",(0,i.kt)("inlineCode",{parentName:"p"},"30 fps"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python main.py\n2560 1440 451 30\n")),(0,i.kt)("h2",{id:"extracting-frames"},"Extracting Frames"),(0,i.kt)("p",null,"The OpenCV ",(0,i.kt)("inlineCode",{parentName:"p"},"read()")," function allows you to grab a single frame from the video file in from of a Numpy array:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"print(video.read())\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python main.py\n(True, array([[[ 45,  45,  45],\n        [ 47,  47,  47],\n        [ 39,  39,  39],\n        ...,\n        [122, 122, 122],\n        [126, 126, 126],\n        [140, 140, 140]]], dtype=uint8))\n")),(0,i.kt)("p",null,"Repeated calls will return the consecutive frames until the last, after which the function will return a ",(0,i.kt)("strong",{parentName:"p"},"False, None"),". To extract all frames into a folder ",(0,i.kt)("inlineCode",{parentName:"p"},"images")," we can run a while loop:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"# get the first frame and see if successful\nsuccess, frame = video.read()\n# initiate count\ncount = 1\n# keep extracting frames as long as success is True\nwhile success:\n    cv2.imwrite(f'images/{count}.jpg', frame)\n    success, frame = video.read()\n    count += 1\n")),(0,i.kt)("h3",{id:"extract-frame-at-timestamp"},"Extract Frame at Timestamp"),(0,i.kt)("p",null,"This way I now ended up with ",(0,i.kt)("strong",{parentName:"p"},"450"),", mostly useless, images from my cameras alarm video. The next step has to be to specify a timestamp from where a frame should be extracted - e.g. given by an ",(0,i.kt)("strong",{parentName:"p"},"Object Detection")," algorithm:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"# get frame at a specific timestamp\ntimestamp = '00:00:03.00'\nhours, minutes, seconds = [float(i) for i in timestamp.split(':')]\n# print(hours, minutes, seconds)\n\n# get number of frames up to timestamp\ntrigger_frame = hours * 3600 * fps + minutes * 60 * fps + seconds * fps\nprint(frames, trigger_frame)\n")),(0,i.kt)("p",null,"The total amount of frames in the video file is represented by ",(0,i.kt)("inlineCode",{parentName:"p"},"frames")," and the number of fames up to the specified timestamp are represented by ",(0,i.kt)("inlineCode",{parentName:"p"},"trigger_frame")," - in this example it is frame number ",(0,i.kt)("inlineCode",{parentName:"p"},"90")," we want to take a look at:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python main.py\n451 90.0\n")),(0,i.kt)("p",null,"Now we can select this frame and store it inside the ",(0,i.kt)("inlineCode",{parentName:"p"},"images")," directory:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"# Go to frame selected by timestamp\nvideo.set(1, trigger_frame)\nsuccess, frame = video.read()\n\n# save the frame into an image file\nif success:\n    cv2.imwrite(f'images/{filename}_{hours}-{minutes}-{seconds}.jpg', frame)\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"security","_","cam","_","0.0-0.0-4.25.jpg")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python - Video Editing with OpenCV",src:a(17602).Z,width:"2560",height:"1440"})),(0,i.kt)("h2",{id:"detect-faces-using-opencv-cascades--alarm-recordings"},"Detect Faces using OpenCV Cascades & Alarm Recordings"),(0,i.kt)("h3",{id:"still-images"},"Still Images"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"import cv2\n\n# load image file & frontal face cascade\nimage = cv2.imread('images/faces.jpg', 1)\nface_cascade = cv2.CascadeClassifier('cascades/adaboost_frontal_face_detector.xml')\n\n# use cascade to detect frontal faces\nfaces = face_cascade.detectMultiScale(image, 1.1, 4)\n# print(faces)\n\n# use returned coordinates to draw a frame\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image, (x, y), (x+w, y+h), (30,211,198), 3)\n\n# save edited image to file\ncv2.imwrite('images/faces_detected.jpg', image)\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"faces","_","detected.jpg")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python - Video Editing with OpenCV",src:a(38136).Z,width:"422",height:"585"})),(0,i.kt)("h3",{id:"videos"},"Videos"),(0,i.kt)("p",null,"Using a Harr Cascade to detect faces in a video file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"import cv2\n\n# load video file & frontal face cascade\nvideo = cv2.VideoCapture('videos/faces_01.mp4')\nface_cascade = cv2.CascadeClassifier('cascades/adaboost_frontal_face_detector.xml')\n\n# read first frame of the video\nsuccess, frame = video.read()\n\n# get video fps of input video\nfps = int(video.get(cv2.CAP_PROP_FPS))\n\n# prepare empty video file\noutput = cv2.VideoWriter('videos/output.avi', cv2.VideoWriter_fourcc(*'DIVX'), fps, (1280, 720))\n\nwhile success:\n    resize = cv2.resize(frame, (1280, 720))\n    # use cascade to detect frontal faces\n    faces = face_cascade.detectMultiScale(resize, 1.4, 4)\n\n    # use returned coordinates to draw a frame\n    for (x, y, w, h) in faces:\n        cv2.rectangle(resize, (x, y), (x+w, y+h), (30,211,198), 3)\n\n    # write frame to empty output\n    output.write(resize)\n\n    # read next frame to start the loop\n    success, frame = video.read()\n\n# generate video when you reached end of file\noutput.release()\n")),(0,i.kt)("p",null,"This code will detect faces, and use the detected coordinates to draw a bounding box around them. These edited frames are then stashed in a new video file:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python - Video Editing with OpenCV",src:a(32121).Z,width:"935",height:"572"})),(0,i.kt)("p",null,"To issues with this code - the ",(0,i.kt)("inlineCode",{parentName:"p"},"detectMultiScale")," function is struggling with my source video. Setting the scale factor to ",(0,i.kt)("inlineCode",{parentName:"p"},"1.4")," and minimum neighbors to ",(0,i.kt)("inlineCode",{parentName:"p"},"4")," still gives me false positives on background objects. I hope to improve this by turning the image grayscale."),(0,i.kt)("p",null,"And secondly, I am returning the entire video. By moving the ",(0,i.kt)("inlineCode",{parentName:"p"},"write()")," function into the for-loop I would get an output video that only contains frames with detected faces. But I can also simply save every frame into a jpg file instead:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"import cv2\n\n# load video file & frontal face cascade\nvideo = cv2.VideoCapture('videos/faces_01.mp4')\nface_cascade = cv2.CascadeClassifier('cascades/adaboost_frontal_face_detector.xml')\n\n# read first frame of the video\nsuccess, frame = video.read()\ncount = 1\n\nwhile success:\n    # convert image to grayscale and resize\n    resize = cv2.resize(frame, (1280, 720))\n    gray_image = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY)\n    # use cascade to detect frontal faces on grayscale frame\n    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.4, minNeighbors=4)\n\n    # use returned coordinates to draw bounding box on colour frame\n    for (x, y, w, h) in faces:\n        cv2.rectangle(resize, (x, y), (x+w, y+h), (198,211,30), thickness=3)\n        cv2.imwrite(f'images/face_detection_{count}.jpg', resize)\n        count += 1\n\n    # read next frame to start the loop\n    success, frame = video.read()\n")),(0,i.kt)("p",null,"This time I am still seeing a few false positives but the detection rate in general has much improved. And I end up with a collection of images that I can process further:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python - Video Editing with OpenCV",src:a(76404).Z,width:"2216",height:"915"})),(0,i.kt)("h3",{id:"ip-camera-streams"},"IP Camera Streams"),(0,i.kt)("p",null,"Now that I am able to handle local video files I now want to use an RTSP stream from my INSTAR 2k+ WQHD camera as input source. I ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv-with-videos"},"already looked at how to process RTSP streams with OpenCV"),". So now I just have to merge those codes to get an ",(0,i.kt)("strong",{parentName:"p"},"Alarm Snapshot")," function that will record a single JPG image whenever a face is detected:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"import cv2\nimport os\n\nface_cascade = cv2.CascadeClassifier('cascades/adaboost_frontal_face_detector.xml')\nRTSP_URL = 'rtsp://admin:instar@192.168.2.120/livestream/12'\n\n# use tcp instead of udp if stream is unstable\nos.environ['OPENCV_FFMPEG_CAPTURE_OPTIONS'] = 'rtsp_transport;udp'\n# start the stream and verify\ncap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)\n\nif not cap.isOpened():\n    print(\"ERROR :: Cannot open RTSP stream\")\n    exit(-1)\n\n\n# start reading frames\nsuccess, frame = cap.read()\ncount = 0\n\nwhile success:\n    # resize frame and convert to grayscale\n    resize = cv2.resize(frame, (1280, 720))\n    gray_image = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY)\n\n    # use cascade to detect frontal faces on grayscale frame\n    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.4, minNeighbors=4)\n\n    # use returned coordinates to draw bounding box on colour frame\n    for (x, y, w, h) in faces:\n        cv2.rectangle(resize, (x, y), (x+w, y+h), (198,211,30), thickness=3)\n        cv2.imwrite(f'images/face_detection_{count}.jpg', resize)\n    \n    count += 1\n    success, frame = cap.read()\n")),(0,i.kt)("p",null,"So now I have a piece of code that systematically ignores James May. But rigorously triggers whenever Richard Hammond or Jeremy Clarkson walk in - fair enough:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python - Video Editing with OpenCV",src:a(32662).Z,width:"2081",height:"876"})),(0,i.kt)("p",null,"And with a minor change I am also able to record every frame - with a successful face detection - into a video file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"# prepare empty video file\noutput = cv2.VideoWriter('videos/face_detection.avi', cv2.VideoWriter_fourcc(*'DIVX'), 15, (1280, 720))\n\n\n# start reading frames\nsuccess, frame = cap.read()\ncount = 0\n\nwhile success:\n    # resize frame and convert to grayscale\n    resize = cv2.resize(frame, (1280, 720))\n    gray_image = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY)\n\n    # use cascade to detect frontal faces on grayscale frame\n    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.4, minNeighbors=4)\n\n    # use returned coordinates to draw bounding box on colour frame\n    for (x, y, w, h) in faces:\n        cv2.rectangle(resize, (x, y), (x+w, y+h), (198,211,30), thickness=3)\n        output.write(resize)\n    \n    count += 1\n    success, frame = cap.read()\n\noutput.release()\n")),(0,i.kt)("h2",{id:"continuous-video-display-with-face-detection"},"Continuous Video Display with Face Detection"),(0,i.kt)("p",null,"To be able to monitor the live video while recording with face detection use the following code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"import cv2\nimport os\n\nface_cascade = cv2.CascadeClassifier('cascades/adaboost_frontal_face_detector.xml')\nRTSP_URL = 'rtsp://admin:instar@192.168.2.120/livestream/12'\n\nos.environ['OPENCV_FFMPEG_CAPTURE_OPTIONS'] = 'rtsp_transport;udp' # Use tcp instead of udp if stream is unstable\n\ncap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)\n\nif not cap.isOpened():\n    print('Cannot open RTSP stream')\n    exit(-1)\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = 15\n\nvideo_codec = cv2.VideoWriter_fourcc(*'DIVX')\n# video_output = cv2.VideoWriter('videos/captured_video.avi', video_codec, fps, (frame_width, frame_height))\nvideo_output = cv2.VideoWriter('videos/captured_video.avi', video_codec, 15, (1280, 720))\n\nwhile True:\n    success, frame = cap.read()\n\n    if success:\n        # resize frame and convert to grayscale\n        resize = cv2.resize(frame, (1280, 720))\n        gray_image = cv2.cvtColor(resize, cv2.COLOR_BGR2GRAY)\n\n        # use cascade to detect frontal faces on grayscale frame\n        faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.4, minNeighbors=4)\n\n        # use returned coordinates to draw bounding box on colour frame\n        for (x, y, w, h) in faces:\n            cv2.rectangle(resize, (x, y), (x+w, y+h), (198,211,30), thickness=3)\n\n        cv2.imshow(\"Video Recording\", resize)\n        video_output.write(resize)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            cap.release()\n            video_output.release()\n            cv2.destroyAllWindows()\n            print('INFO :: Video was saved.')\n            break\n\n    else:\n        cap.release()\n        video_output.release()\n        cv2.destroyAllWindows()\n        print('ERROR :: Video recording aborted!')\n        break\n")))}p.isMDXComponent=!0},32121:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/cascade_face_detection_01-49e1a3020b72b826b9ba4eebb5bb5e7a.png"},76404:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/cascade_face_detection_02-c95275cd044d93168ce2aed17f03b5b2.png"},32662:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/cascade_face_detection_03-c48baa256ca97e9cc571c8586bad501d.png"},38136:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/faces_detected-4a414a36b4b683fbf27bfab66c615848.jpg"},11278:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5f44d483789c3ce79f05418f930f5cd2.jpg"},17602:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/security_cam_0.0-0.0-4.25-abc766dcce0ee0615f473f1fcf0de4cc.jpg"}}]);