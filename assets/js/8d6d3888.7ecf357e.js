"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[20946],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>u});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=i,g=m["".concat(s,".").concat(u)]||m[u]||c[u]||r;return n?a.createElement(g,o(o({ref:t},d),{},{components:n})):a.createElement(g,o({ref:t},d))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},24055:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=n(87462),i=(n(67294),n(3905));const r={sidebar_position:9060,slug:"2021-11-02",title:"spaCy NER Predictions",authors:"mpolinowski",tags:["Machine Learning","Python"]},o=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/index",id:"IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/index",title:"spaCy NER Predictions",description:"Victoria Harbour, Hongkong",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions",slug:"/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"}],version:"current",sidebarPosition:9060,frontMatter:{sidebar_position:9060,slug:"2021-11-02",title:"spaCy NER Predictions",authors:"mpolinowski",tags:["Machine Learning","Python"]},sidebar:"tutorialSidebar",previous:{title:"Streamlit user interface for openCV/Mediapipe face mesh app",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-03--streamlit-opencv-mediapipe/2021-11-03"},next:{title:"spaCy NER on Arch Linux",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01"}},s={},p=[{value:"Setup",id:"setup",level:2},{value:"Import Libraries",id:"import-libraries",level:3},{value:"Text Cleanup",id:"text-cleanup",level:3},{value:"Load NER Model",id:"load-ner-model",level:3},{value:"Process your Data",id:"process-your-data",level:2},{value:"Load Image",id:"load-image",level:3},{value:"Extract Data",id:"extract-data",level:3},{value:"Data Preprocessing",id:"data-preprocessing",level:3},{value:"Get Predictions from NER Model",id:"get-predictions-from-ner-model",level:2},{value:"Bringing Results into a Dataframe",id:"bringing-results-into-a-dataframe",level:3},{value:"Drawing Bounding Boxes",id:"drawing-bounding-boxes",level:2},{value:"End and Start Position",id:"end-and-start-position",level:3},{value:"Table Join",id:"table-join",level:3}],d={toc:p};function c(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Victoria Harbour, Hongkong",src:n(18258).Z,width:"1500",height:"663"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-10-31--tesseract_ocr_arch_linux/2021-10-31"},"Part I - Tesseract OCR on Arch Linux")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01"},"Part II - spaCy NER on Arch Linux")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02"},"Part III - spaCy NER Predictions"))),(0,i.kt)("p",null,"I now want to use my model to extract the name, email address, web address and designation from an unknown name card:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(88720).Z,width:"905",height:"511"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#setup"},"Setup"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#import-libraries"},"Import Libraries")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#text-cleanup"},"Text Cleanup")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#load-ner-model"},"Load NER Model")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#process-your-data"},"Process your Data"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#load-image"},"Load Image")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#extract-data"},"Extract Data")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#data-preprocessing"},"Data Preprocessing")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#get-predictions-from-ner-model"},"Get Predictions from NER Model"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#bringing-results-into-a-dataframe"},"Bringing Results into a Dataframe")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#drawing-bounding-boxes"},"Drawing Bounding Boxes"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#end-and-start-position"},"End and Start Position")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#table-join"},"Table Join"))))),(0,i.kt)("h2",{id:"setup"},"Setup"),(0,i.kt)("p",null,"Re-enter the virtual environment and create a new Python notebook in Jupyter:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"source .env/bin/activate\njupyter notebook\n")),(0,i.kt)("h3",{id:"import-libraries"},"Import Libraries"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nimport pandas as pd\nimport cv2 as cv\nimport pytesseract\nfrom glob import glob\nimport spacy\nimport re\nimport string\n")),(0,i.kt)("h3",{id:"text-cleanup"},"Text Cleanup"),(0,i.kt)("p",null,"I can re-use the clean text function created in the ",(0,i.kt)("strong",{parentName:"p"},"Data Preprocessing")," step:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def cleanText(txt):\n    \n    whitespace = string.whitespace\n    punctuation = '!#$%&\\'()*+:;<=>?[\\\\]^`{|}~'\n    tableWhitespace = str.maketrans('','',whitespace)\n    tablePunctuation = str.maketrans('','',punctuation)\n\n    text = str(txt)\n    text = text.lower()\n    removeWhitespace = text.translate(tableWhitespace)\n    removePunctuation = removeWhitespace.translate(tablePunctuation)\n    \n    return str(removePunctuation)\n")),(0,i.kt)("h3",{id:"load-ner-model"},"Load NER Model"),(0,i.kt)("p",null,"All I need to do here is to import the ",(0,i.kt)("inlineCode",{parentName:"p"},"best-model")," with spaCy from the output folder:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"model_ner = spacy.load('./output/model-best/')\n")),(0,i.kt)("h2",{id:"process-your-data"},"Process your Data"),(0,i.kt)("h3",{id:"load-image"},"Load Image"),(0,i.kt)("p",null,"Use OpenCV2 to load the image (and verify that the images was loaded):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"image = cv.imread('../images/card_00.jpg')\n\n# cv.imshow('businesscard', image)\n# cv.waitKey(0)\n# cv.destroyAllWindows()\n")),(0,i.kt)("h3",{id:"extract-data"},"Extract Data"),(0,i.kt)("p",null,"Now grab the text from the image using Pytesseract:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"tessData = pytesseract.image_to_data(image)\n")),(0,i.kt)("p",null,"Run ",(0,i.kt)("inlineCode",{parentName:"p"},"tessData")," to verify that the data was sucessfully extracted:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"tessData\n\n...\nMike\\n5\\t1\\t1\\t1\\t1\\t2\\t645\\t29\\t225\\t34\\t69\\tPolinowski\\n2\\t1\\t2\\t0\\t0\\t0\\t628\\t78\\t247\\t37\\t-1\\t\\n3\\t1\\t2\\t1\\t0\\t0\\t628\\t78\\t247\\t37\\t-1\\t\\n4\\t1\\t2\\t1\\t1\\t0\\t628\\t78\\t244\\t19\\t-1\\t\\n5\\t1\\t2\\t1\\t1\\t1\\t628\\t78\\t62\\t19\\t91\\tChief\\n5\\t1\\t2\\t1\\t1\\t2\\t701\\t78\\t171\\t19\\t90\\tProcrastinator\\n\n...\n")),(0,i.kt)("p",null,"Convert this data into a Pandas Dataframe:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"tessList = list(map(lambda x:x.split('\\t'),tessData.split('\\n')))\ndf = pd.DataFrame(tessList[1:],columns=tessList[0])\n")),(0,i.kt)("p",null,"Run ",(0,i.kt)("inlineCode",{parentName:"p"},"df")," to verify that the data was wrapped inside the data frame:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(24164).Z,width:"874",height:"368"})),(0,i.kt)("p",null,"Clean the data frame:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"df.dropna(inplace=True) #Drop missing values\ndf['text'] = df['text'].apply(cleanText) #Apply cleanText function on text column\n")),(0,i.kt)("h3",{id:"data-preprocessing"},"Data Preprocessing"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"df_clean = df.query('text !=\"\" ') #Ignore whitespace\ncontent = \" \".join([w for w in df_clean['text']]) #Join every word in text column concatenated by spaces\n")),(0,i.kt)("p",null,"Verify by printing content:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"print(content)\n\nmike polinowski chief procrastinator i 39c, street 318, boeung keng kong phnom penh, cambodia tel 855 0 23 21 59 60 email me@example.com www.some-place.com\n")),(0,i.kt)("h2",{id:"get-predictions-from-ner-model"},"Get Predictions from NER Model"),(0,i.kt)("p",null,"Now I can use the NER model to get my named entities out of that string:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"doc = model_ner(content)\n")),(0,i.kt)("p",null,"spaCy offers a tool that allows us to display the recognized named entities:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from spacy import displacy\ndisplacy.serve(doc,style='ent')\n")),(0,i.kt)("p",null,"The display is served on localhost port 5000."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Use ",(0,i.kt)("inlineCode",{parentName:"p"},"displacy.render(doc,style='ent')")," instead if you are using a Jupyter notebook.")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(5941).Z,width:"1063",height:"136"})),(0,i.kt)("p",null,"All entities were sucessfully recognized - except the job designation. Procrastination does not seem to have much of a future ..."),(0,i.kt)("h3",{id:"bringing-results-into-a-dataframe"},"Bringing Results into a Dataframe"),(0,i.kt)("p",null,"To work with the data I will now write it into an JSON object:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dockJSON = doc.to_json()\n")),(0,i.kt)("p",null,"This way I can now get hold of the data by keys - ",(0,i.kt)("inlineCode",{parentName:"p"},"text")," (all recognized strings), ",(0,i.kt)("inlineCode",{parentName:"p"},"ents")," (the tags used to those words) and ",(0,i.kt)("inlineCode",{parentName:"p"},"tokens")," (character positions within the string):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dockJSON.keys()\n")),(0,i.kt)("p",null,"This will retrieve the available keys - ",(0,i.kt)("inlineCode",{parentName:"p"},"dict_keys(['text', 'ents', 'tokens'])")," which can be queried by:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dockJSON['text']\ndockJSON['ents']\ndockJSON['tokens']\n")),(0,i.kt)("p",null,"Let's wrap everything into a Pandas Data Frame:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dockJSON = doc.to_json()\n\ndoc_text = dockJSON['text'] \n# doc_text #Testing\ndf_tokens = pd.DataFrame(dockJSON['tokens']) #Create data frame from tokens\n# df_tokens.head() #Testing\ndf_tokens['token'] = df_tokens[['start','end']].apply(lambda x:doc_text[x[0]:x[1]], axis=1) #Add text table\n# df_tokens.head(10) #Testing\nright_table = pd.DataFrame(dockJSON['ents'])[['start','label']] #Take the entities table\ndf_tokens = pd.merge(df_tokens,right_table,how='left',on='start') #And left-join it with the tokens+text table\n# df_tokens.head(10) #Testing\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(16584).Z,width:"1077",height:"370"})),(0,i.kt)("p",null,"Replace all ",(0,i.kt)("inlineCode",{parentName:"p"},"NaN")," fields in the lable column with ",(0,i.kt)("inlineCode",{parentName:"p"},"O"),"'s:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-Python"},"df_tokens.fillna('O',inplace=True)\n")),(0,i.kt)("h2",{id:"drawing-bounding-boxes"},"Drawing Bounding Boxes"),(0,i.kt)("h3",{id:"end-and-start-position"},"End and Start Position"),(0,i.kt)("p",null,"To highlight detected entities we need the position of each entity as reported by Pytesseract:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(64254).Z,width:"791",height:"134"})),(0,i.kt)("p",null,"For this we can join ",(0,i.kt)("inlineCode",{parentName:"p"},"df_tokens")," table with the ",(0,i.kt)("inlineCode",{parentName:"p"},"df_clean")," table using a common key."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Get the ",(0,i.kt)("strong",{parentName:"li"},"End Position")," of every word inside the ",(0,i.kt)("inlineCode",{parentName:"li"},"text")," column in ",(0,i.kt)("inlineCode",{parentName:"li"},"df_clean['text']"),":")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"lambda x: len(x)+1.cumsum()-1 #End position is length of each string +1 space. The cummulative sum adds the length of each prior string to get the absolut endposition inside `text`\n")),(0,i.kt)("p",null,"Cumulative sum example:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Input: 10, 15, 20, 25, 30"),(0,i.kt)("li",{parentName:"ul"},"Output: 10, 25, 45, 70, 100")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"df_clean['end'] = df_clean['text'].apply(lambda x: len(x) + 1).cumsum() -1\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"The ",(0,i.kt)("strong",{parentName:"li"},"Start Position"),"  of each word is the end position minus the length of the word:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"df_clean['start'] = df_clean[['text','end']].apply(lambda x: x[1] - len(x[0]),axis=1)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(20993).Z,width:"889",height:"187"})),(0,i.kt)("h3",{id:"table-join"},"Table Join"),(0,i.kt)("p",null,"Now that we have a common key in both tables ",(0,i.kt)("inlineCode",{parentName:"p"},"df_clean")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"df_tokens")," we can use that one - the ",(0,i.kt)("strong",{parentName:"p"},"Start Position")," - to perform an inner join:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"df_card = pd.merge(df_clean,df_tokens[['start','token','label']],how='inner',on='start')\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(26069).Z,width:"1007",height:"203"})),(0,i.kt)("p",null,"The table contains all the information that we need to draw our bounding box. I can extract them for every entity that is not labled with ",(0,i.kt)("inlineCode",{parentName:"p"},"O"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"bb_df = df_card.query(\"label ! = 'O'\")\n")),(0,i.kt)("p",null,"From this I can now:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"img = image.copy() #first take a copy of the original image\nfor x,y,w,h,label in bb_df[['left','top','width','height','label']].values: #write data into an array\n  x = int(x)\n  y = int(y)\n  w = int(w)\n  h = int(h)\n\n  cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) #Draw rectangle around entities\n  cv.putText(img,str(label),(x,y),cv.FONT_HERSHEY_PLAIN,1,(255,0,0),2) #Add a tag from value of label\n\ncv.imshow('Prediction', img)\ncv.waitKey(0)\ncv.destroyAllWindows()\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(25198).Z,width:"905",height:"584"})),(0,i.kt)("p",null,"The bounding boxes are still split into several parts that need to be combined. I labeled them with ",(0,i.kt)("inlineCode",{parentName:"p"},"B-")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"I-")," prefixes - e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"B-NAME")," for the beginning of the name entity and ",(0,i.kt)("inlineCode",{parentName:"p"},"I-NAME")," for all following entities that are part of it. We can remove those prefixes with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"bb_df['label'] = bb_df['label'].apply(lambda x: x[2:])\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(54841).Z,width:"895",height:"165"})),(0,i.kt)("p",null,"And now I can group all entities that are assigned the same label:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class groupgen():\n    def __init__(self):\n        self.id = 0\n        self.text = ''\n        \n    def getgroup(self,text):\n        if self.text == text: #If entity has the same label - group them under the same id\n            return self.id\n        else:\n            self.id +=1 #Else increment\n            self.text = text\n            return self.id\n        \ngrp_gen = groupgen()\n")),(0,i.kt)("p",null,"Add the group ID to the bounding box data frame:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"bb_df['group'] = bb_df['label'].apply(grp_gen.getgroup)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(28362).Z,width:"993",height:"346"})),(0,i.kt)("p",null,"Calculate the missing parameter of the right and bottom edge:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"bb_df[['top','left','width','height']] = bb_df[['top','left','width','height']].astype(int)\nbb_df['right'] = bb_df['left'] +  bb_df['width'] #Calculate right edge\nbb_df['bottom'] = bb_df['top'] +  bb_df['height'] #Calculate bottom edge\n")),(0,i.kt)("p",null,"Aggregate all entities of a group and find the position of the enclosing bounding box:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Left")," - Minimum value of ",(0,i.kt)("inlineCode",{parentName:"li"},"left")," column"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Right")," - Maximum value of ",(0,i.kt)("inlineCode",{parentName:"li"},"right")," column"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Top")," - Minimum value of ",(0,i.kt)("inlineCode",{parentName:"li"},"top")," column"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Bottom")," - Maximum value of ",(0,i.kt)("inlineCode",{parentName:"li"},"bottom")," column")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"col_group = ['top','bottom','left','right','label','token','group']\ngroup_tag_img = bb_df[col_group].groupby(by='group')\n\nimg_tagging = group_tag_img.agg({\n    'top':min,\n    'bottom':max,\n    'left':min,\n    'right':max,\n    'label':np.unique,\n    'token':lambda x: \" \".join(x) #Join all words together seperated by a space\n})\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"NER Prediction",src:n(25659).Z,width:"1068",height:"232"})))}c.isMDXComponent=!0},18258:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-4f747fa38245d3c618169ab90d8c3f77.jpg"},88720:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_00-3de0e71d4629a8358c03a71154005d77.jpg"},24164:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_01-3c793e106a2c510a281251f2a7dab3b2.png"},5941:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_02-b1f63d6c31e8b06fce57145569e746cd.png"},16584:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_03-e419b05ec810c184a2b85de2a3cdca7e.png"},64254:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_04-b59ef2988100c14796e5c98aef0a38a0.png"},20993:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_05-246be03c7282d64541f331e6cae948e7.png"},26069:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_06-ac190ed1d9a80fe9763636fddcc722f5.png"},25198:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_07-d101ff0d2b0a25f170d7539b897ae266.png"},54841:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_08-580bb3c25872ce30bc98077e38220c02.png"},28362:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_09-550ef8978b5ccc4f3dd53fefa1f8bbb0.png"},25659:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/predictions_10-f041dc2ac9b403086a882019563634b4.png"}}]);