"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[95558],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>u});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=n.createContext({}),s=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=s(e.components);return n.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=s(a),u=r,h=m["".concat(c,".").concat(u)]||m[u]||d[u]||o;return a?n.createElement(h,i(i({ref:t},p),{},{components:a})):n.createElement(h,i({ref:t},p))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=m;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var s=2;s<o;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},4823:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var n=a(87462),r=(a(67294),a(3905));const o={sidebar_position:6030,slug:"2021-12-10",title:"OpenCV Optical Flow Algorithm for Object Tracking",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},i=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/index",id:"IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/index",title:"OpenCV Optical Flow Algorithm for Object Tracking",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking",slug:"/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/2021-12-10",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/2021-12-10",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"OpenCV",permalink:"/docs/tags/open-cv"}],version:"current",sidebarPosition:6030,frontMatter:{sidebar_position:6030,slug:"2021-12-10",title:"OpenCV Optical Flow Algorithm for Object Tracking",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},sidebar:"tutorialSidebar",previous:{title:"Yolo App - Data Collection",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15"},next:{title:"OpenCV CAMshift Algorithm for Object Tracking",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09"}},c={},s=[{value:"Optical Flow (Sparse)",id:"optical-flow-sparse",level:2},{value:"Get your Video",id:"get-your-video",level:3},{value:"Auto Select Object to Track",id:"auto-select-object-to-track",level:3},{value:"Manually Select Object to Track",id:"manually-select-object-to-track",level:3},{value:"Optical Flow (Dense)",id:"optical-flow-dense",level:2}],p={toc:s};function d(e){let{components:t,...o}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Shenzhen, China",src:a(9582).Z,width:"2385",height:"919"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#optical-flow-sparse"},"Optical Flow (Sparse)"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#get-your-video"},"Get your Video")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#auto-select-object-to-track"},"Auto Select Object to Track")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#manually-select-object-to-track"},"Manually Select Object to Track")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#optical-flow-dense"},"Optical Flow (Dense)"))),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/opencv2-tracking-algorithm"},"Github Repo")),(0,r.kt)("p",null,"One of the problems with the ",(0,r.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08"},"Meanshift Algorithm")," was that the region of interest always stayed at the same size when the object came closer to the camera or moved farther away. The region needs to adapt it's size with size and rotation of the target. This was remedied by the ",(0,r.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09"},"CAMShift Algorithm"),"."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html"},"Optical flow")," is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second."),(0,r.kt)("h2",{id:"optical-flow-sparse"},"Optical Flow (Sparse)"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html"},"Optical flow")," is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. "),(0,r.kt)("h3",{id:"get-your-video"},"Get your Video"),(0,r.kt)("p",null,"Get your video file input:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"cap = cv2.VideoCapture('resources/group_of_people_02.mp4')\n# get first video frame\nok, frame = cap.read()\n")),(0,r.kt)("h3",{id:"auto-select-object-to-track"},"Auto Select Object to Track"),(0,r.kt)("p",null,"I want to use the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html"},"Shi-Tomasi Corner Detector")," which is used by the OpenCV ",(0,r.kt)("inlineCode",{parentName:"p"},"Good Features to Track")," function to detect the corner points of an object. As usual, image should be a grayscale image. Then you specify number of corners you want to find. Then you specify the quality level, which is a value between 0-1, which denotes the minimum quality of corner below which everyone is rejected. Then we provide the minimum euclidean distance between corners detected:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# generate initial corners of detected object\n# set limit, minimum distance in pixels and quality of object corner to be tracked\nparameters_shitomasi = dict(maxCorners=100, qualityLevel=0.3, minDistance=7)\n")),(0,r.kt)("p",null,"With all this information, the function finds corners in the image. All corners below quality level are rejected. Then it sorts the remaining corners based on quality in the descending order. Then function takes first strongest corner, throws away all the nearby corners in the range of minimum distance and returns N strongest corners:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# convert to grayscale\nframe_gray_init = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n# Use Shi-Tomasi to detect object corners / edges from initial frame\nedges = cv2.goodFeaturesToTrack(frame_gray_init, mask = None, **parameters_shitomasi)\n")),(0,r.kt)("p",null,"Now that we have the ",(0,r.kt)("inlineCode",{parentName:"p"},"edges")," - corner points - of all detected objects in the initial frame wen can start comparing consecutive frames of the video to this initial frame and edges. But one more thing - we need to create a clean sheet to draw our detection lines on and a random colour generator for those lines:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# create a black canvas the size of the initial frame\ncanvas = np.zeros_like(frame)\n# create random colours for visualization for all 100 max corners for RGB channels\ncolours = np.random.randint(0, 255, (100, 3))\n")),(0,r.kt)("p",null,"And now we can loop through the following video frames, comparing each one to the initial detection. The while loop uses the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4.15/d7/d08/classcv_1_1SparsePyrLKOpticalFlow.html"},"cv2.calcOpticalFlowPyrLK")," function that ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4.15/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323"},"needs to be configured with the following parameter"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# set min size of tracked object, e.g. 15x15px\nparameter_lucas_kanade = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameters"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"winSize"),(0,r.kt)("td",{parentName:"tr",align:null},"size of the search window at each pyramid level.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"maxLevel"),(0,r.kt)("td",{parentName:"tr",align:null},"0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than maxLevel.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"criteria"),(0,r.kt)("td",{parentName:"tr",align:null},"parameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon.")))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"while True:\n    # get next frame\n    ok, frame = cap.read()\n    if not ok:\n        print(\"[INFO] end of file reached\")\n        break\n    # prepare grayscale image\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    # update object corners by comparing with found edges in initial frame\n    update_edges, status, errors = cv2.calcOpticalFlowPyrLK(frame_gray_init, frame_gray, edges, None,\n                                                         **parameter_lucas_kanade)\n    # only update edges if algorithm successfully tracked\n    new_edges = update_edges[status == 1]\n    # to calculate directional flow we need to compare with previous position\n    old_edges = edges[status == 1]\n\n    for i, (new, old) in enumerate(zip(new_edges, old_edges)):\n        a, b = new.ravel()\n        c, d = old.ravel()\n\n        # draw line between old and new corner point with random colour\n        mask = cv2.line(canvas, (int(a), int(b)), (int(c), int(d)), colours[i].tolist(), 2)\n        # draw circle around new position\n        frame = cv2.circle(frame, (int(a), int(b)), 5, colours[i].tolist(), -1)\n\n    result = cv2.add(frame, mask)\n    cv2.imshow('Optical Flow (sparse)', result)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n    # overwrite initial frame with current before restarting the loop\n    frame_gray_init = frame_gray.copy()\n    # update to new edges before restarting the loop\n    edges = new_edges.reshape(-1, 1, 2)\n")),(0,r.kt)("p",null,"The result looks like:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:a(64915).Z,width:"670",height:"191"})),(0,r.kt)("h3",{id:"manually-select-object-to-track"},"Manually Select Object to Track"),(0,r.kt)("p",null,"The auto-selection works OK in the example above. But it can quickly become messy in crowded spaces. Here you might want to select the starting point of your track yourself. So let's create a function that is automatically called when a window with the name ",(0,r.kt)("inlineCode",{parentName:"p"},"Optical Flow")," is created and that listens to your left mouse-button. Whenever you click the frame displayed inside the window the coordinates of your cursor are recorded and the algorithm tries to track the underlying edge point of your selected object:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# define function to manually select object to track\ndef select_point(event, x, y, flags, params):\n    global point, selected_point, old_points\n    # record coordinates of mouse click\n    if event == cv2.EVENT_LBUTTONDOWN:\n        point = (x, y)\n        selected_point = True\n        old_points = np.array([[x, y]], dtype=np.float32)\n\n\n# associate select function with window Selector\ncv2.namedWindow('Optical Flow')\ncv2.setMouseCallback('Optical Flow', select_point)\n\n# initialize variables updated by function\nselected_point = False\npoint = ()\nold_points = ([[]])\n")),(0,r.kt)("p",null,"And again we need to loop through the remaining frames applying the Sparse Optical Flow algorithm to track the object:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# loop through the remaining frames of the video\n# and apply algorithm to track selected objects\nwhile True:\n    # get next frame\n    frame = vs.read()\n    # covert to grayscale\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    if selected_point is True:\n        cv2.circle(frame, point, 5, (0, 0, 255), 2)\n        # update object corners by comparing with found edges in initial frame\n        new_points, status, errors = cv2.calcOpticalFlowPyrLK(frame_gray_init, frame_gray, old_points, None,\n                                                         **parameter_lucas_kanade)\n\n        # overwrite initial frame with current before restarting the loop\n        frame_gray_init = frame_gray.copy()\n        # update to new edges before restarting the loop\n        old_points = new_points\n\n        x, y = new_points.ravel()\n        j, k = old_points.ravel()\n\n        # draw line between old and new corner point with random colour\n        canvas = cv2.line(canvas, (int(x), int(y)), (int(j), int(k)), (0, 255, 0), 3)\n        # draw circle around new position\n        frame = cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n\n    result = cv2.add(frame, canvas)\n    cv2.imshow('Optical Flow', result)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:a(65838).Z,width:"741",height:"295"})),(0,r.kt)("h2",{id:"optical-flow-dense"},"Optical Flow (Dense)"),(0,r.kt)("p",null,"Lucas-Kanade method computes optical flow for a sparse feature set (in our example, corners detected using Shi-Tomasi algorithm). OpenCV provides another algorithm to find the dense optical flow. It computes the optical flow for all the points in the frame."),(0,r.kt)("p",null,"There is only one change outside of the while-loop compared to sparse flow - we need to define our canvas in the HSV colour space and initialize it with maximum saturation:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# create canvas to paint on\nhsv_canvas = np.zeros_like(first_frame)\n# set saturation value (position 2 in HSV space) to 255\nhsv_canvas[..., 1] = 255\n")),(0,r.kt)("p",null,"Again, we need to grab the following frames from our video and compare the current frame with the initial frame. The function in OpenCV is called ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/4.x/dc/d6b/group__video__track.html#ga5d10ebbd59fe09c5f650289ec0ece5af"},"cv2.calcOpticalFlowFarneback()")," and uses the following configuration parameters:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameters"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"prev"),(0,r.kt)("td",{parentName:"tr",align:null},"first 8-bit single-channel input image.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"next"),(0,r.kt)("td",{parentName:"tr",align:null},"second input image of the same size and the same type as prev.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"flow"),(0,r.kt)("td",{parentName:"tr",align:null},"computed flow image that has the same size as prev and type CV_32FC2.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"pyr_scale"),(0,r.kt)("td",{parentName:"tr",align:null},"parameter, specifying the image scale (<1) to build pyramids for each image; pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"levels"),(0,r.kt)("td",{parentName:"tr",align:null},"number of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"winsize"),(0,r.kt)("td",{parentName:"tr",align:null},"averaging window size; larger values increase the algorithm robustness to image noise and give more chances for fast motion detection, but yield more blurred motion field.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"iterations"),(0,r.kt)("td",{parentName:"tr",align:null},"number of iterations the algorithm does at each pyramid level.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"poly_n"),(0,r.kt)("td",{parentName:"tr",align:null},"size of the pixel neighborhood used to find polynomial expansion in each pixel; larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"poly_sigma"),(0,r.kt)("td",{parentName:"tr",align:null},"standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5.")))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"while True:\n    # get next frame\n    ok, frame = cap.read()\n    if not ok:\n        print(\"[ERROR] reached end of file\")\n        break\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    # compare initial frame with current frame\n    flow = cv2.calcOpticalFlowFarneback(frame_gray_init, frame_gray, None, 0.5, 3, 15, 3, 5, 1.1, 0)\n    # get x and y coordinates\n    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n    # set hue of HSV canvas (position 1)\n    hsv_canvas[..., 0] = angle*(180/(np.pi/2))\n    # set pixel intensity value (position 3\n    hsv_canvas[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n\n    frame_rgb = cv2.cvtColor(hsv_canvas, cv2.COLOR_HSV2BGR)\n\n    # optional recording result/mask\n    video_output.write(frame_rgb)\n\n    cv2.imshow('Optical Flow (dense)', frame_rgb)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n    # set initial frame to current frame\n    frame_gray_init = frame_gray\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Meanshift Algorithm for Object Tracking",src:a(29151).Z,width:"657",height:"184"})))}d.isMDXComponent=!0},64915:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/OpenCV_Optical_Flow_Tracking_01-847bf512368f7ffccaeff5691c3e0c05.gif"},65838:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/OpenCV_Optical_Flow_Tracking_02-16a87e22e9c00167718cbf544c54e5c9.gif"},29151:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/OpenCV_Optical_Flow_Tracking_03-4579858a7dfd480cce75818e6ecc04ea.gif"},9582:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5a0b68587d9242bbb46a1f1aaab44216.jpg"}}]);