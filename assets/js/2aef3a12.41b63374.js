"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[70041],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>c});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=p(a),c=o,g=u["".concat(l,".").concat(c)]||u[c]||d[c]||i;return a?n.createElement(g,r(r({ref:t},m),{},{components:a})):n.createElement(g,r({ref:t},m))}));function c(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},27374:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var n=a(87462),o=(a(67294),a(3905));const i={sidebar_position:4800,slug:"2023-01-10",title:"YOLOv7 Training with Custom Data",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO"],description:"Use your Custom Dataset to train YOLOv7"},r=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/index",id:"IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/index",title:"YOLOv7 Training with Custom Data",description:"Use your Custom Dataset to train YOLOv7",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data",slug:"/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"YOLO",permalink:"/docs/tags/yolo"}],version:"current",sidebarPosition:4800,frontMatter:{sidebar_position:4800,slug:"2023-01-10",title:"YOLOv7 Training with Custom Data",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO"],description:"Use your Custom Dataset to train YOLOv7"},sidebar:"tutorialSidebar",previous:{title:"YOLOv7 Label Conversion",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/2023-01-13"},next:{title:"MiDaS Depth Vision",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08"}},l={},p=[{value:"Preparation",id:"preparation",level:2},{value:"Downloading the Dataset",id:"downloading-the-dataset",level:3},{value:"Annotations",id:"annotations",level:3},{value:"Training",id:"training",level:2},{value:"Testing",id:"testing",level:2},{value:"Predictions",id:"predictions",level:2},{value:"Images",id:"images",level:3},{value:"Videos",id:"videos",level:3},{value:"YOLOv7 Multi-Resolution Training",id:"yolov7-multi-resolution-training",level:2}],m={toc:p};function d(e){let{components:t,...i}=e;return(0,o.kt)("wrapper",(0,n.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Guangzhou, China",src:a(99192).Z,width:"1500",height:"652"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#preparation"},"Preparation"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#downloading-the-dataset"},"Downloading the Dataset")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#annotations"},"Annotations")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#training"},"Training")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#testing"},"Testing")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#predictions"},"Predictions"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#images"},"Images")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#videos"},"Videos")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#yolov7-multi-resolution-training"},"YOLOv7 Multi-Resolution Training"))),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"see also ",(0,o.kt)("a",{parentName:"em",href:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05"},"Getting started with object detection in YOLOv7"))),(0,o.kt)("h2",{id:"preparation"},"Preparation"),(0,o.kt)("p",null,"In the ",(0,o.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05"},"previous step")," we cloned the YOLOv7 repository and run predictions using testing weights. To train the model we need the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/yolov7#transfer-learning"},"training weights")," that are also available through the repository:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\ntar -xf dataset.tar.gz\nrm dataset.tar.gz\n")),(0,o.kt)("h3",{id:"downloading-the-dataset"},"Downloading the Dataset"),(0,o.kt)("p",null,"To be able to train YOLOv7 we first need a dataset to train it on. A wide collection of image datasets are available from ",(0,o.kt)("a",{parentName:"p",href:"https://storage.googleapis.com/openimages/web/index.html"},"Open Images Dataset"),". YOLOv7 takes label data in the text(.txt) file and has the following format:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"<object-class-id> <x> <y> <width> <height>\n")),(0,o.kt)("h3",{id:"annotations"},"Annotations"),(0,o.kt)("p",null,"I was searching for a dataset that I can use as a template - ready to go with YOLOv7 Annotations and found:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://www.kaggle.com/datasets/deepakat002/indian-vehicle-number-plate-yolo-annotation"},"Indian vehicle number plate yolo annotation"))),(0,o.kt)("p",null,"The set only has one class defined ",(0,o.kt)("inlineCode",{parentName:"p"},"number_plate")," and every image is annotated the YOLO way - defining the class of the object together with a bounding box location, e.g.:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"0 0.338281 0.676389 0.032813 0.028704\n")),(0,o.kt)("p",null,"If you are using your own dataset you can use the tool ",(0,o.kt)("strong",{parentName:"p"},"LabelImg")," to generate those annotations:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install labelImg\n")),(0,o.kt)("p",null,"You can start the application with ",(0,o.kt)("inlineCode",{parentName:"p"},"labelImg")," and open your image folder. Make sure to toggle the app to generate YOLO annotations, create the class you want to annotate for and draw the bounding box around the object you want YOLO to search for (don't forget to save afterwards):"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(18444).Z,width:"1224",height:"523"})),(0,o.kt)("p",null,"Move the image files with their annotations to ",(0,o.kt)("inlineCode",{parentName:"p"},"yolov7/custom_data/indian_number_plate/train")," and split them up into sub-dirs ",(0,o.kt)("inlineCode",{parentName:"p"},"images")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"labels")," and move around 20% of them into ",(0,o.kt)("inlineCode",{parentName:"p"},"yolov7/custom_data/indian_number_plate/validation"),". Exclude a random selection for interference ",(0,o.kt)("inlineCode",{parentName:"p"},"yolov7/custom_data/indian_number_plate/test"),"."),(0,o.kt)("p",null,"Now we need to tell YOLO where to find the data. Create the following file ",(0,o.kt)("inlineCode",{parentName:"p"},"coco.yaml")," inside the ",(0,o.kt)("inlineCode",{parentName:"p"},"custom_data/indian_number_plates")," folder:"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"custom.yaml")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yml"},"# Custom Dataset\n# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\ntrain: path/custom_data/train/\nval: path/custom_data/validation/\ntest: path/custom_data/test/\n# number of classes\nnc: 1\n# class names\nnames: [ 'number_plate' ]\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"./yolov7/custom_data/indian_number_plate/\n\u251c\u2500\u2500 coco.yaml\n\u251c\u2500\u2500 test\n\u2502\xa0\xa0 \u251c\u2500\u2500 classes.txt\n\u2502\xa0\xa0 \u251c\u2500\u2500 images\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 frame-1130.jpg\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 frame-2.jpg\n\u2502\xa0\xa0 \u2502\xa0\xa0 ...\n\u2502\xa0\xa0 \u2514\u2500\u2500 labels\n\u2502\xa0\xa0     \u251c\u2500\u2500 frame-1130.txt\n\u2502\xa0\xa0     \u251c\u2500\u2500 frame-2.txt\n\u2502\xa0\xa0     ...\n\u251c\u2500\u2500 train\n\u2502\xa0\xa0 \u251c\u2500\u2500 classes.txt\n\u2502\xa0\xa0 \u251c\u2500\u2500 images\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 frame-680.jpg\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 frame-945.jpg\n\u2502\xa0\xa0 \u2502\xa0\xa0 ...\n\u2502\xa0\xa0 \u2514\u2500\u2500 labels\n\u2502\xa0\xa0     \u251c\u2500\u2500 frame-680.txt\n\u2502\xa0\xa0     \u251c\u2500\u2500 frame-945.txt\n\u2502\xa0\xa0     ...\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 classes.txt\n    \u251c\u2500\u2500 images\n    \u2502\xa0\xa0 \u251c\u2500\u2500 frame-305.jpg\n    \u2502\xa0\xa0 \u251c\u2500\u2500 frame-630.jpg\n    \u2502\xa0\xa0 ...\n    \u2514\u2500\u2500 labels\n        \u251c\u2500\u2500 frame-305.txt\n        \u251c\u2500\u2500 frame-630.txt\n        ...\n")),(0,o.kt)("p",null,"Now create a training configuration file by creating a copy of ",(0,o.kt)("inlineCode",{parentName:"p"},"yolov7/cfg/training/yolov7.yaml"),". All we need to change there is the number of classes YOLO should expect - which is only 1:"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"yolov7","_","custom.yaml")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yml"},"# parameters\nnc: 1  # number of classes\n")),(0,o.kt)("h2",{id:"training"},"Training"),(0,o.kt)("p",null,"We now have to use the ",(0,o.kt)("inlineCode",{parentName:"p"},"train.py")," script and point it to our data with ",(0,o.kt)("inlineCode",{parentName:"p"},"custom_data/custom.yaml")," and downloaded training weights ",(0,o.kt)("inlineCode",{parentName:"p"},"yolov7_training.pt"),". The YOLOv7 repository already provides a file with hyperparameters for custom datasets that we can use ",(0,o.kt)("inlineCode",{parentName:"p"},"data/hyp.scratch.custom.yaml"),". Other parameters are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"--device"),": Only for GPU support"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"--workers"),": How many parallel processes can your system handle"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"--batch-size"),": How many images should be used per batch (dependent on VRAM)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"--img"),": Image size to train on")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python train.py --workers 1 --device 0 --batch-size 1 --epochs 100 --img 640 640 --hyp data/hyp.scratch.custom.yaml --data custom_data/indian_number_plate/coco.yaml --name yolov7-custom --weights yolov7_training.pt\n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("strong",{parentName:"p"},"ERROR"),": ",(0,o.kt)("inlineCode",{parentName:"p"},"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 5.93 GiB total capacity; 4.25 GiB already allocated; 60.31 MiB free; 4.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"))),(0,o.kt)("p",null,"I guess I need a new GPU after all... I had to reduce the batch size to ",(0,o.kt)("inlineCode",{parentName:"p"},"--batch-size 1")," to be able to run the training - now it is working. But the GPU memory only fills up to 2.5Gig. I will try a batch size of 2 next:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n99/99     2.54G   0.01324  0.001894         0   0.01513         2       640\n\nClass      Images      Labels           P           R\n  all          31          54        0.95       0.703\n100 epochs completed in 1.213 hours.\n\nOptimizer stripped from runs/train/yolov7-custom6/weights/last.pt, 74.8MB\nOptimizer stripped from runs/train/yolov7-custom6/weights/best.pt, 74.8MB\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(35563).Z,width:"2400",height:"1200"})),(0,o.kt)("p",null,"The first results look promising - but there is still a lot of room for improvements - time for a larger dataset and more epochs:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(81718).Z,width:"3000",height:"2250"})),(0,o.kt)("h2",{id:"testing"},"Testing"),(0,o.kt)("p",null,"Now we can take an image from the testing dataset and run a prediction based on the best weights that were just generated:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python test.py --weights runs/train/yolov7-custom6/weights/best.pt \\\n    --task test \\\n    --data custom_data/indian_number_plate/coco.yaml\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Class      Images      Labels           P           R\n  all           3           7       0.999       0.429\nSpeed: 36.0/1.0/37.0 ms inference/NMS/total per 640x640 image at batch-size 32\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(78505).Z,width:"1997",height:"675"})),(0,o.kt)("h2",{id:"predictions"},"Predictions"),(0,o.kt)("h3",{id:"images"},"Images"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python detect.py --weights runs/train/yolov7-custom6/weights/best.pt \\\n    --conf 0.5 \\\n    --img-size 640 \\\n    --source custom_data/indian_number_plate/test/images/frame-2.jpg\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Done. (45.7ms) Inference, (0.7ms) NMS\nThe image with the result is saved in: runs/detect/exp3/frame-2.jpg\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(57866).Z,width:"2557",height:"670"})),(0,o.kt)("h3",{id:"videos"},"Videos"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Video source Youtube ",(0,o.kt)("a",{parentName:"p",href:"https://youtu.be/rLCL-dyyDn0"},"Making no sense at all #2 | Indian Roads"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python detect.py --weights runs/train/yolov7-custom6/weights/best.pt \\\n    --conf 0.5 \\\n    --img-size 640 \\\n    --source indian_roads.mp4 --no-trace\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(8001).Z,width:"850",height:"475"})),(0,o.kt)("h2",{id:"yolov7-multi-resolution-training"},"YOLOv7 Multi-Resolution Training"),(0,o.kt)("p",null,"To get a more robust prediction model we can train YOLOv7 model with multi-resolution images. For this case, we just need to add the ",(0,o.kt)("inlineCode",{parentName:"p"},"\u2013multi-scale")," flag and change the project name."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python train.py --epochs 100 --workers 4 --device 0 --batch-size 1 \\\n--img 640 640 --data custom_data/indian_number_plate/coco.yaml --weights yolov7_training.pt \\\n--name yolov7_indian_number_plates --hyp data/hyp.scratch.custom.yaml \\\n--multi-scale\n")),(0,o.kt)("p",null,"This time I tried a batch size of 2 but immediately ran into VRAM issues again. But increasing the amount of workers from 1 to 4 seems to work fine - let's see if this speeds things up:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("strong",{parentName:"p"},"ERROR"),": ",(0,o.kt)("inlineCode",{parentName:"p"},"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.97 GiB (GPU 0; 5.93 GiB total capacity; 1.00 GiB already allocated; 2.97 GiB free; 1.33 GiB reserved in total by PyTorch)"))),(0,o.kt)("p",null,"It uses less VRAM compared to before. But adding more workers did not speed it up (or using multi-res input is much slower - needs testing). Also the accuracy looks a lot worse:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n99/99     2.28G    0.0192  0.003187         0   0.02239         5       768\nClass      Images      Labels           P           R\n  all          31          54       0.411       0.259\n100 epochs completed in 1.344 hours.\n\nOptimizer stripped from runs/train/yolov7_indian_number_plates3/weights/last.pt, 74.8MB\nOptimizer stripped from runs/train/yolov7_indian_number_plates3/weights/best.pt, 74.8MB\n")),(0,o.kt)("p",null,"We can, again, test the model with:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python test.py --weights runs/train/yolov7_indian_number_plates3/weights/best.pt \\\n    --task test \\\n    --data custom_data/indian_number_plate/coco.yaml\n")),(0,o.kt)("p",null,"As expected the results are unusable - hmmm:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Training with Custom Data",src:a(2695).Z,width:"2076",height:"665"})),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Class      Images      Labels           P           R\n                 all           3           7       0.996\nSpeed: 35.1/1.1/36.2 ms inference/NMS/total per 640x640 image at batch-size 32\n")))}d.isMDXComponent=!0},18444:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_01-2d05382814ec06add68c1f6e3ad4676f.png"},57866:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_02-f2fcead845d8d83f3898da5975733dc1.png"},35563:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_03-f63e68835fe09a333b8b516f7b9c16a1.png"},81718:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_04-3f237a921721875ee8b8a2495128a48e.png"},8001:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_05-b971ecff568a9d68417142ca582a4f54.gif"},78505:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_06-320d35c90d8b46766993071844fb3c66.png"},2695:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/YOLOv7_Custom_DataSets_07-04bd22cf56815f5c74a8d6ce21f69822.png"},99192:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ba3b23aa3d5392c02b451d1b2b911721.jpg"}}]);