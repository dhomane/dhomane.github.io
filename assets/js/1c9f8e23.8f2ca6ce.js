"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[22058],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>d});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var o=n.createContext({}),p=function(e){var t=n.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(o.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,o=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),u=p(a),d=i,c=u["".concat(o,".").concat(d)]||u[d]||h[d]||r;return a?n.createElement(c,s(s({ref:t},m),{},{components:a})):n.createElement(c,s({ref:t},m))}));function d(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,s=new Array(r);s[0]=u;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=e,l.mdxType="string"==typeof e?e:i,s[1]=l;for(var p=2;p<r;p++)s[p]=a[p];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},93322:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var n=a(87462),i=(a(67294),a(3905));const r={sidebar_position:1e4,slug:"2018-01-02",title:"Machine Learning with SciKit Learn",authors:"mpolinowski",tags:["Machine Learning","Python"]},s=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/index",id:"IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/index",title:"Machine Learning with SciKit Learn",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/index.mdx",sourceDirName:"IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python",slug:"/IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/2018-01-02",permalink:"/docs/IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/2018-01-02",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/index.mdx",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"}],version:"current",sidebarPosition:1e4,frontMatter:{sidebar_position:1e4,slug:"2018-01-02",title:"Machine Learning with SciKit Learn",authors:"mpolinowski",tags:["Machine Learning","Python"]},sidebar:"tutorialSidebar",previous:{title:"Introduction to TensorFlow 2 Beta",permalink:"/docs/IoT-and-Machine-Learning/ML/2019-03-31--introduction-to-tensorflow-2-beta/2019-03-31"},next:{title:"AIOps",permalink:"/docs/category/aiops"}},o={},p=[{value:"Data Representation",id:"data-representation",level:2},{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"SciKit-Learn API",id:"scikit-learn-api",level:2},{value:"Estimation",id:"estimation",level:2},{value:"Predictor",id:"predictor",level:2},{value:"Transformer",id:"transformer",level:2},{value:"Clustering Tasks",id:"clustering-tasks",level:2},{value:"Clustering",id:"clustering",level:2},{value:"Visualization",id:"visualization",level:2},{value:"k-Means Algorithm",id:"k-means-algorithm",level:2},{value:"Initialization Methods",id:"initialization-methods",level:3},{value:"Mean-Shift Algorithm",id:"mean-shift-algorithm",level:2},{value:"DBSCAN Algorithm",id:"dbscan-algorithm",level:2},{value:"Evaluating Performance",id:"evaluating-performance",level:2},{value:"Classification Tasks",id:"classification-tasks",level:2},{value:"Regression Tasks",id:"regression-tasks",level:2},{value:"Data Split",id:"data-split",level:2},{value:"Cross Validation",id:"cross-validation",level:2},{value:"Metric Evaluation",id:"metric-evaluation",level:2},{value:"Table of Confusion",id:"table-of-confusion",level:2},{value:"Evaluation of Tasks with binary output labels",id:"evaluation-of-tasks-with-binary-output-labels",level:3},{value:"Evaluation of Regression Tasks",id:"evaluation-of-regression-tasks",level:3}],m={toc:p};function h(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,n.Z)({},m,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Shenzhen, China",src:a(84990).Z,width:"1500",height:"480"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#prerequisite"},"Prerequisite")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#scikit-introduction"},"SciKit Introduction"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#data-representation"},"Data Representation")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#data-preprocessing"},"Data Preprocessing")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#scikit-learn-api"},"SciKit-Learn API")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#estimation"},"Estimation")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#predictor"},"Predictor")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#transformer"},"Transformer")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#unsupervised-learning-introduction"},"Unsupervised Learning Introduction"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#clustering-tasks"},"Clustering Tasks")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#unsupervised-learning-irl"},"Unsupervised Learning IRL"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#clustering"},"Clustering")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#visualization"},"Visualization")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#k-means-algorithm"},"k-Means Algorithm"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#initialization-methods"},"Initialization Methods")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#mean-shift-algorithm"},"Mean-Shift Algorithm")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#dbscan-algorithm"},"DBSCAN Algorithm")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#evaluating-performance"},"Evaluating Performance")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#supervised-learning-introduction"},"Supervised Learning Introduction"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#classification-tasks"},"Classification Tasks")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#regression-tasks"},"Regression Tasks")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#supervised-learning-irl"},"Supervised Learning IRL"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#data-split"},"Data Split")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#cross-validation"},"Cross Validation")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#metric-evaluation"},"Metric Evaluation")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#table-of-confusion"},"Table of Confusion"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#evaluation-of-tasks-with-binary-output-labels"},"Evaluation of Tasks with binary output labels")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#evaluation-of-regression-tasks"},"Evaluation of Regression Tasks"))))))),(0,i.kt)("p",null,"An Introduction in building machine learning applications with the SciKit Python library. Learn data preprocessing and implement supervised and unsupervised algorithms as well as performing error analysis to evaluate their performance."),(0,i.kt)("h1",{id:"prerequisite"},"Prerequisite"),(0,i.kt)("p",null,"First we need to install the ",(0,i.kt)("a",{parentName:"p",href:"https://www.anaconda.com/download/"},"Anaconda Environment")," for Windows, macOS or LINUX. This package combines everything we need to get started with Python. From libraries like ",(0,i.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/"},"SciKit-Learn"),", Pandas and Matplotlib to Jupyter Notebook, that will help us to execute our Python scripts."),(0,i.kt)("p",null,"We will begin working with the ",(0,i.kt)("a",{parentName:"p",href:"https://anaconda.org/anaconda/seaborn"},"Seaborn Package")," dataset that is included in the Anaconda package to become familiar with Python based data analysis."),(0,i.kt)("p",null,"In the later steps, we are going to use publicly available data from the ",(0,i.kt)("a",{parentName:"p",href:"https://archive.ics.uci.edu"},"UCI Archive"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"http://archive.ics.uci.edu/ml/datasets/Wholesale+customers"},"Wholesale Customer Dataset"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://archive.ics.uci.edu/ml/datasets/Fertility"},"Adult Fertility Study")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://archive.ics.uci.edu/ml/machine-learning-databases/00244/"},"https://archive.ics.uci.edu/ml/machine-learning-databases/00244/")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/"},"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"},"Bank+Marketing Study")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/"},"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/"))))),(0,i.kt)("h1",{id:"scikit-introduction"},"SciKit Introduction"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://scikit-learn.org/"},"SciKit-Learn")," is a OpenSource library for building models based on built-in machine learning and statistical algorithms. The library offers both supervised and unsupervised models, that we will use to analyze our data with."),(0,i.kt)("p",null,"The library is used to:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Interpret data and train models"),(0,i.kt)("li",{parentName:"ul"},"Perform predictions from data sets"),(0,i.kt)("li",{parentName:"ul"},"Cross validation and performance metric analysis"),(0,i.kt)("li",{parentName:"ul"},"To create sample data sets and test algorithms")),(0,i.kt)("h2",{id:"data-representation"},"Data Representation"),(0,i.kt)("p",null,"To feed data into SciKit it needs to be represented as a table or matrix. Most data used in machine learning is 2-dimensional - that means it can be represented by a classical Excel sheet with rows and columns:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Rows represent observations (instances)"),(0,i.kt)("li",{parentName:"ul"},"Columns represent characteristics (features)")),(0,i.kt)("p",null,"Datasets often have many features that will be represented in the ",(0,i.kt)("strong",{parentName:"p"},"Feature Matrix"),". In most cases it will only be one or two features that will separated for the later analysis of the data set - this skimmed down dataset is called the ",(0,i.kt)("strong",{parentName:"p"},"Target Matrix"),":"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Feature Matrix")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Contains data from each instance for all features"),(0,i.kt)("li",{parentName:"ul"},"The dimensions of the matrix are ",(0,i.kt)("inlineCode",{parentName:"li"},"[n_i, n_f]")," denoting the number of instances and features.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Target Matrix")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Is usually 1-dimensional as it only contains 1 feature for all instances. If more than 1 feature is necessary to describe the model the dimension increases accordingly.")),(0,i.kt)("p",null,"The Feature Matrix is usually stored in the variable ",(0,i.kt)("strong",{parentName:"p"},"X"),", while the variable ",(0,i.kt)("strong",{parentName:"p"},"Y")," is used to store the ",(0,i.kt)("strong",{parentName:"p"},"Target Matrix"),". Both matrices can be created by using a ",(0,i.kt)("strong",{parentName:"p"},"NumPy Array")," or a ",(0,i.kt)("strong",{parentName:"p"},"Panda DataFrame"),". "),(0,i.kt)("p",null,"In the following example we are going to look at plant statistic from the ",(0,i.kt)("a",{parentName:"p",href:"https://anaconda.org/anaconda/seaborn"},"Seaborn Package")," included in the ",(0,i.kt)("a",{parentName:"p",href:"https://www.anaconda.com/download/"},"Anaconda Environment"),". Each row in the set will represent a species of the Setosa family and columns represent the plant characteristics of the sepal as well as petal length and width:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(46780).Z,width:"800",height:"622"})),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"We import the ",(0,i.kt)("strong",{parentName:"li"},"Seaborn")," package into the variable ",(0,i.kt)("strong",{parentName:"li"},"sns"),"."),(0,i.kt)("li",{parentName:"ol"},"We can now extract the ",(0,i.kt)("strong",{parentName:"li"},"Iris Dataset")," from it and store the data inside the variable ",(0,i.kt)("strong",{parentName:"li"},"iris"),"."),(0,i.kt)("li",{parentName:"ol"},"We then drop the ",(0,i.kt)("strong",{parentName:"li"},"Species Feature")," from the dataset and store it inside the variable ",(0,i.kt)("strong",{parentName:"li"},"X"),". Thus the ",(0,i.kt)("strong",{parentName:"li"},"Feature Matrix")," consists of all the features ",(0,i.kt)("strong",{parentName:"li"},"BUT")," the target for all instances. Making it a 2 dimensional dataset."),(0,i.kt)("li",{parentName:"ol"},"Now we can have a look at the top 10 rows of our data to get an idea what it looks like."),(0,i.kt)("li",{parentName:"ol"},"The ",(0,i.kt)("strong",{parentName:"li"},"Shape")," command shows us that we have a ",(0,i.kt)("strong",{parentName:"li"},"Feature Matrix")," that consists of ",(0,i.kt)("em",{parentName:"li"},"150 rows")," (instances) and ",(0,i.kt)("em",{parentName:"li"},"4 columns")," (features)."),(0,i.kt)("li",{parentName:"ol"},"We will now build the ",(0,i.kt)("strong",{parentName:"li"},"Target Matrix")," based on the ",(0,i.kt)("strong",{parentName:"li"},"Species Feature")," and store it in the variable ",(0,i.kt)("strong",{parentName:"li"},"Y"),"."),(0,i.kt)("li",{parentName:"ol"},"And we see that the first 10 species all belong to the Setosa family. The ",(0,i.kt)("strong",{parentName:"li"},"Target Matrix")," is now reduced from the earlier 2 to 1 dimension - only consisting of the target feature for all instances.")),(0,i.kt)("h2",{id:"data-preprocessing"},"Data Preprocessing"),(0,i.kt)("p",null,"IRL datasets are usually not analysis-friendly (",(0,i.kt)("strong",{parentName:"p"},"messy data"),"), as they are containing ",(0,i.kt)("em",{parentName:"p"},"noisy data"),", ",(0,i.kt)("em",{parentName:"p"},"missing entries")," and ",(0,i.kt)("em",{parentName:"p"},"outliers")," that need to be dealt with before feeding them to our algorithm."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Dealing with Missing Values"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Eliminate Data"),(0,i.kt)("li",{parentName:"ul"},"Or Replace it",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Mean Imputation")," - filling out missing fields using the mean or median value (",(0,i.kt)("em",{parentName:"li"},"may introduce bias to our model"),")"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Regression Imputation")," - Use prediction to fill out the values (",(0,i.kt)("em",{parentName:"li"},"may end up overfitting the model"),")"))),(0,i.kt)("li",{parentName:"ul"},"String-based values should be replaced with a class (",(0,i.kt)("em",{parentName:"li"},"like  'uncategorized'"),") ")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Dealing with Outliers"),":"),(0,i.kt)("p",null,"Outliers represent values that are far from the mean (often set to ",(0,i.kt)("strong",{parentName:"p"},"3-6 standard deviations")," when the data set follows a Gaussian distribution). If the values follow a Gaussian distribution, ",(0,i.kt)("em",{parentName:"p"},"global outliers")," are located at the tails of the bell curve. While ",(0,i.kt)("em",{parentName:"p"},"local outliers")," are inside the distribution but far off the group of data points they are associated with. E.g. a vehicle that can drive 500 MPH is a global outlier in a car statistic. While a truck that only has 40 hp is a local outlier in the group labeled as trucks, but is still well inside the curve of the vehicle dataset."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Delete Outliers."),(0,i.kt)("li",{parentName:"ul"},"If all instances above a certain value of a feature behave the same way, you can ",(0,i.kt)("strong",{parentName:"li"},"define a top")," for that feature and apply it to outliers."),(0,i.kt)("li",{parentName:"ul"},"Replace the value (",(0,i.kt)("strong",{parentName:"li"},"Mean")," or ",(0,i.kt)("strong",{parentName:"li"},"Regression"),")."),(0,i.kt)("li",{parentName:"ul"},"String values (e.g. misspelled features) can be eliminated or corrected (when possible).")),(0,i.kt)("p",null,"In the following example we will again use the ",(0,i.kt)("a",{parentName:"p",href:"https://anaconda.org/anaconda/seaborn"},"Seaborn Package")," included in the ",(0,i.kt)("a",{parentName:"p",href:"https://www.anaconda.com/download/"},"Anaconda Environment")," and take a look at the age of the passengers of the ",(0,i.kt)("strong",{parentName:"p"},"Titantic"),":"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(76934).Z,width:"800",height:"1284"})),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"We import the ",(0,i.kt)("strong",{parentName:"li"},"Seaborn")," package and store the ",(0,i.kt)("strong",{parentName:"li"},"Titanic Dataset")," inside the variable ",(0,i.kt)("em",{parentName:"li"},"titanic"),"."),(0,i.kt)("li",{parentName:"ol"},"We then load the ",(0,i.kt)("strong",{parentName:"li"},"Age Feature")," from the dataset and store it in the variable ",(0,i.kt)("em",{parentName:"li"},"age"),"."),(0,i.kt)("li",{parentName:"ol"},"Displaying the first 10 rows shows us that we already have a missing entry (",(0,i.kt)("strong",{parentName:"li"},"NaN"),", ",(0,i.kt)("em",{parentName:"li"},"not a number"),"). The ",(0,i.kt)("em",{parentName:"li"},"shape")," command shows us that there are 891 rows in total. "),(0,i.kt)("li",{parentName:"ol"},"We can check how many of those 891 have a value of NaN with the ",(0,i.kt)("em",{parentName:"li"},"isnull")," function. Summing them up shows us that we have 177 passengers of the Titanic where we do not know there age."),(0,i.kt)("li",{parentName:"ol"},"We can now use the ",(0,i.kt)("strong",{parentName:"li"},"Mean Method")," to replace all of those with the mean age. For this we call the mean method on the values in ",(0,i.kt)("em",{parentName:"li"},"age"),", round them up and store them inside the variable ",(0,i.kt)("em",{parentName:"li"},"mean"),". Printing out the value, we can see that the mean age was ",(0,i.kt)("em",{parentName:"li"},"30"),"."),(0,i.kt)("li",{parentName:"ol"},"We can now use the ",(0,i.kt)("strong",{parentName:"li"},"fillna Method")," to fill out every value that is NaN with the mean value. Taking a look at the first 10 rows again shows that the missing value has now been filled with the mean value 30."),(0,i.kt)("li",{parentName:"ol"},"To display our distribution - to be able to spot ",(0,i.kt)("strong",{parentName:"li"},"Outliers")," - we import ",(0,i.kt)("em",{parentName:"li"},"PyPlot")," from the ",(0,i.kt)("em",{parentName:"li"},"MatPlotLib")," library as ",(0,i.kt)("em",{parentName:"li"},"plt"),". We use the plt method to build a histogram of the values stored inside the ",(0,i.kt)("em",{parentName:"li"},"age")," variable and display the output with the show function."),(0,i.kt)("li",{parentName:"ol"},"To spot outliers we will set the ",(0,i.kt)("strong",{parentName:"li"},"Minimum Value")," that we will accept for our model as the mean value for age MINUS ",(0,i.kt)("em",{parentName:"li"},"3-times the standard deviation")," of the age dataset. This turns out to be a negative value - given that this does not make any sense in our particular dataset, we can ignore outliers on minimum side of the distribution."),(0,i.kt)("li",{parentName:"ol"},"To spot outliers we will set the ",(0,i.kt)("strong",{parentName:"li"},"Maximum Value")," that we will accept for our model as the mean value for age PLUS ",(0,i.kt)("em",{parentName:"li"},"3-times the standard deviation")," of the age dataset. Everyone who is listed as ~ 69 or above can be treated as an outlier."),(0,i.kt)("li",{parentName:"ol"},"We can thus define our ",(0,i.kt)("em",{parentName:"li"},"outlier")," variable as every value inside ",(0,i.kt)("em",{parentName:"li"},"age")," that is greater than ",(0,i.kt)("em",{parentName:"li"},"max","_","val"),". Counting the outliers shows us that we have ",(0,i.kt)("em",{parentName:"li"},"7")," inside our dataset."),(0,i.kt)("li",{parentName:"ol"},"We decide to remove all outliers from our dataset by only accepting values into our ",(0,i.kt)("em",{parentName:"li"},"age")," variable that are smaller or equal to ",(0,i.kt)("em",{parentName:"li"},"max","_","val"),". The shape command shows us that out of the initial 891 passengers we now eliminated 7 from our analysis - ",(0,i.kt)("em",{parentName:"li"},"884"),".")),(0,i.kt)("h2",{id:"scikit-learn-api"},"SciKit-Learn API"),(0,i.kt)("p",null,"SciKit-Learn offers us a unified syntax to make machine learning more accessible. The SciKit-Learn API is divided into 3 interfaces:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Estimator Interface"),": Used to create models and integrate your data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Predictor Interface"),": Used to make predictions based on the models created."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Transformer Interface"),": Used to transform data files.")),(0,i.kt)("h2",{id:"estimation"},"Estimation"),(0,i.kt)("p",null,"This is the interface that you use to initialize a model and apply a fit() method to your data. Your data is received as two variables - ",(0,i.kt)("strong",{parentName:"p"},"X_train")," is the feature matrix and ",(0,i.kt)("strong",{parentName:"p"},"Y_train")," the target matrix for your model. ",(0,i.kt)("em",{parentName:"p"},"Unsupervised Models")," only use the first of those two arguments. A ",(0,i.kt)("em",{parentName:"p"},"Supervised Model")," takes both."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\n")),(0,i.kt)("p",null,"In the example of a supervised model, we imported the model we want to use, store it in the variable ",(0,i.kt)("inlineCode",{parentName:"p"},"model")," and then apply it to our two arguments using the fit() method."),(0,i.kt)("p",null,"The Estimator can perform 3 more tasks for us:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Feature Extraction"),": A transformation of the input data into numerical features."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Feature Selection"),": Selecting a feature from your data that most contributes to the prediction output."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Dimensionality"),": Converting your data into a lower dimension.")),(0,i.kt)("h2",{id:"predictor"},"Predictor"),(0,i.kt)("p",null,"The Predictor interface performs prediction based on the model you trained. In supervised models it creates a new dataset called ",(0,i.kt)("strong",{parentName:"p"},"X_test")," and re-feeds it to your model. The implementation looks as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"Y_pred = model.predict(X_test)\n")),(0,i.kt)("p",null,"This allows us to quantify the ",(0,i.kt)("strong",{parentName:"p"},"Confidence")," or ",(0,i.kt)("strong",{parentName:"p"},"Performance")," of a model by comparing how far ",(0,i.kt)("em",{parentName:"p"},"X","_","test")," differs from ",(0,i.kt)("em",{parentName:"p"},"Y","_","test"),"."),(0,i.kt)("h2",{id:"transformer"},"Transformer"),(0,i.kt)("p",null,"The Transform interface gives us a transform() method to preprocess our input data. Using the same transformation for the data that we use to train our model as well as the for the data we later use the model on to perform predictions ensures that both datasets are comparable in their distribution. An example is the ",(0,i.kt)("strong",{parentName:"p"},"Normalization")," of a dataset:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n")),(0,i.kt)("p",null,"Here we imported the transformer and store it inside the variable ",(0,i.kt)("inlineCode",{parentName:"p"},"scaler"),". Our dataset is then fit to the imported method and the transformation performed."),(0,i.kt)("h1",{id:"unsupervised-learning-introduction"},"Unsupervised Learning Introduction"),(0,i.kt)("p",null,"In unsupervised learning the model is modelled to the data, without any relationship to an output label. It can be used to show up clusters of similarities inside unlabeled data."),(0,i.kt)("h2",{id:"clustering-tasks"},"Clustering Tasks"),(0,i.kt)("p",null,"Finding clusters in unlabeled data involves grouping instances that are similar to each other, while differing visibly from instances in other groups. The most popular Clustering Algorithms are:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"k-means"),": Separating instances in ",(0,i.kt)("em",{parentName:"li"},"n")," clusters of equal variance by minimizing the sum of the squared distances between 2 points - ",(0,i.kt)("em",{parentName:"li"},"Centroid-based Model"),"."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Mean-shift clustering"),": Using centroids to create a cluster, where every instances is a candidate to become a centroid (mean of the points in that cluster) - ",(0,i.kt)("em",{parentName:"li"},"Centroid-based Model"),"."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"DBSCAN"),": ",(0,i.kt)("em",{parentName:"li"},"Density-based spatial clustering of applications with noise")," separates areas with high densities of points as clusters from areas with a low density - ",(0,i.kt)("em",{parentName:"li"},"Density-based Model"),"."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Gaussian"),": The belonging to a cluster is shown as a deviation from a distribution as used in an expectation maximization model - ",(0,i.kt)("em",{parentName:"li"},"Distribution-based Models"),"."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Hierarchial"),": Similarity as proximity inside the data space - ",(0,i.kt)("em",{parentName:"li"},"Connectivity-based Model"))),(0,i.kt)("h1",{id:"unsupervised-learning-irl"},"Unsupervised Learning IRL"),(0,i.kt)("p",null,"We want to use unsupervised models  to analyze data sets from real-world applications. The Objectives are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Understanding different clustering techniques"),(0,i.kt)("li",{parentName:"ul"},"Using Panda Dataframes"),(0,i.kt)("li",{parentName:"ul"},"Data visualizations with MatPlotLib"),(0,i.kt)("li",{parentName:"ul"},"Working with algorithms like k-means, mean-shift and DBSCAN"),(0,i.kt)("li",{parentName:"ul"},"Using performance metrics to decide which one to use")),(0,i.kt)("h2",{id:"clustering"},"Clustering"),(0,i.kt)("p",null,"Clustering is a type of unsupervised machine-learning technique to find pattern in unlabeled input data and divide data points into ",(0,i.kt)("em",{parentName:"p"},"n")," clusters based on similarity (and difference to data points in other cluster). The assignment to a cluster can either be ",(0,i.kt)("strong",{parentName:"p"},"hard")," (absolute designation) or ",(0,i.kt)("strong",{parentName:"p"},"soft")," (probability of belonging to a cluster). Real-world applications are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Search Engine Results"),(0,i.kt)("li",{parentName:"ul"},"Recommendation Programs"),(0,i.kt)("li",{parentName:"ul"},"Image Recognition"),(0,i.kt)("li",{parentName:"ul"},"Market Segmentation for targeted Marketing")),(0,i.kt)("p",null,"We are going to use the ",(0,i.kt)("a",{parentName:"p",href:"#prerequisite"},"Wholesale Customer Dataset")," from the UC Irvine Machine Learning Repository to explore those techniques."),(0,i.kt)("h2",{id:"visualization"},"Visualization"),(0,i.kt)("p",null,"Visual representations of datasets help us to understand relationships in datasets, results and performance of a model. To work with Visualizations we can load our data into a ",(0,i.kt)("strong",{parentName:"p"},"Dataframe")," using ",(0,i.kt)("strong",{parentName:"p"},"Pandas"),". Panda dataframes manage stored data in a 2-dimensional, size-mutable matrix with labelled axes and are often stored in ",(0,i.kt)("em",{parentName:"p"},"*.csv")," files - like the example of the ",(0,i.kt)("a",{parentName:"p",href:"#prerequisite"},"Wholesale Customer Dataset"),". Data presented this way can be easily loaded into a frame using the Panda function ",(0,i.kt)("strong",{parentName:"p"},"read.csv()")," (there are alternatives for data based in Excel sheets or SQL databases - ",(0,i.kt)("em",{parentName:"p"},"read.xlx()")," and ",(0,i.kt)("em",{parentName:"p"},"read.sql()"),"):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import pandas as pd\n\nfile_path = './wholesale-customers-data.csv'\ndata = pd.read_csv(file_path)\n")),(0,i.kt)("p",null,"To create a visual representation of such a dataframe we can use the Python library ",(0,i.kt)("strong",{parentName:"p"},"Matplotlib")," and create:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Histograms (",(0,i.kt)("em",{parentName:"li"},"plt.hist()"),") "),(0,i.kt)("li",{parentName:"ul"},"Scatter Plots (",(0,i.kt)("em",{parentName:"li"},"plt.scatter()"),")"),(0,i.kt)("li",{parentName:"ul"},"Bar Charts (",(0,i.kt)("em",{parentName:"li"},"plt.bar()"),")"),(0,i.kt)("li",{parentName:"ul"},"Pie Charts (",(0,i.kt)("em",{parentName:"li"},"plt.pie()"),")")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(80906).Z,width:"800",height:"694"})),(0,i.kt)("p",null,"In this example we used the ",(0,i.kt)("strong",{parentName:"p"},"Numpy")," random number generator and ",(0,i.kt)("em",{parentName:"p"},"make","_","circles()")," method to generate a dataset and created a scatter plot and a histogram from it using Matplotlib."),(0,i.kt)("h2",{id:"k-means-algorithm"},"k-Means Algorithm"),(0,i.kt)("p",null,"The k-means algorithm is used on unlabelled data to divide it into ",(0,i.kt)("em",{parentName:"p"},"K")," number of clustered subgroups based on similarity. The ",(0,i.kt)("strong",{parentName:"p"},"Centroid")," of each cluster represents a collection of features that can be used to define the members of the cluster."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Initialization"),": Based on the number of clusters set by you, centroids are generated by initial estimates or at random."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Assignment"),": All data points are assigned to the nearest cluster."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Update"),": Centroids are recalculated by computing the mean of all data points inside the cluster.")),(0,i.kt)("p",null,"The algorithm runs until:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"It reaches the number of preset iterations."),(0,i.kt)("li",{parentName:"ul"},"Data points no longer change from one cluster to another."),(0,i.kt)("li",{parentName:"ul"},"The Euclidean distance between all data points and their assigned centroid is minimized.")),(0,i.kt)("h3",{id:"initialization-methods"},"Initialization Methods"),(0,i.kt)("p",null,"The ",(0,i.kt)("strong",{parentName:"p"},"k-means++")," (default) method chooses the initial centroids randomly with a maximized distance from other centroids."),(0,i.kt)("p",null,"The number of centroids has to be chosen by you to minimize the average cluster distance in relation to it's centroids. At a small number of centroids, the distance between forming clusters is high. The distance reduces the more initial centroids are added to the calculation - until a point where it stagnates. Adding more centroids after that point will falsify the results by over-representation of features by data points inside the cluster."),(0,i.kt)("p",null,"By plotting the distances between clusters against the number of clusters, the ideal number of centroids is given by the breaking point, where the rate decreases suddenly. In the example below we can see that the distances between points inside a cluster is very high, when we start with 1 centroid. It is very low with 15 centroids. k can be set to 5 as it is the point where the similarity between data points inside the cluster no longer increase significantly when we keep sub-dividing clusters:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(49148).Z,width:"800",height:"647"})),(0,i.kt)("p",null,"Here we imported the ",(0,i.kt)("em",{parentName:"p"},"sklearn.cluster")," packages as KMeans and initialize ",(0,i.kt)("em",{parentName:"p"},"ideal","_","k"),", which is then filled with a for-loop calculating the inertia (the average distance between data points within a cluster) as a function of the number of clusters inside our dataset. With every loop the number of clusters ",(0,i.kt)("em",{parentName:"p"},"i")," is increased by 1 until we reach 15."),(0,i.kt)("p",null,"We can then convert the array to a Numpy array and plot the data. We can see a sharp decrease in rate around 4-6, telling us that ",(0,i.kt)("em",{parentName:"p"},"k=5")," would be a good place to initialize our analysis with."),(0,i.kt)("p",null,"We can now initialize our analysis by setting the number of centroids to 5, fit the dataset with k-means and predict what cluster a data point belongs to. To visualize the result we want to set the colour for each data point inside a cluster to ",(0,i.kt)("inlineCode",{parentName:"p"},"c=pred_kmeans"),"."),(0,i.kt)("p",null,"The results below show the plot for a number of 5, 4 and 6 clusters inside the dataset:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(72538).Z,width:"800",height:"1096"})),(0,i.kt)("h2",{id:"mean-shift-algorithm"},"Mean-Shift Algorithm"),(0,i.kt)("p",null,"While the ",(0,i.kt)("strong",{parentName:"p"},"k-mean")," algorithm assigns a data point to a cluster as a function of the distance to a centroid, the ",(0,i.kt)("strong",{parentName:"p"},"mean-shift")," algorithm evaluates the density of data points in the data space to define clusters."),(0,i.kt)("p",null,"The mean-shift algorithm represents the data points as a ",(0,i.kt)("strong",{parentName:"p"},"density distribution")," (KDE - ",(0,i.kt)("em",{parentName:"p"},"Kernel Density Estimation"),")."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(68836).Z,width:"801",height:"1149"})),(0,i.kt)("p",null,"As we can see we are getting the same result here as with the k-means algorithm - we just did not have to define the number of centroids."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Note")," that you can influence the amount of clusters found by assigning different ",(0,i.kt)("strong",{parentName:"p"},"bandwidths")," - the example above varies the bandwidth between 0.4-0.6 as the data set was normalized between 0-1 (when you are working with data with values between 0-2000, you should adjust the bandwidth to ~ 100-500 to get sensible results). The bandwidth represents the size of a window that is drawn around each data point. With every iteration the mean-shift algorithm calculates the mean of each window, based on the data points it contains, and shifts each window towards the mean. This process is repeated until every point is shifted to the nearest peak in the density distribution. The number of shifts a data point has to undergo depends on the distance to the peak and the window size (bandwidth) that is used. Every data point in a peak of the distribution belongs to that cluster."),(0,i.kt)("h2",{id:"dbscan-algorithm"},"DBSCAN Algorithm"),(0,i.kt)("p",null,"The ",(0,i.kt)("em",{parentName:"p"},"density-based spatial clustering of applications with noise")," (",(0,i.kt)("strong",{parentName:"p"},"DBSCAN"),") algorithm groups points that are close to each other and marks points that have no close neighbors as ",(0,i.kt)("strong",{parentName:"p"},"outliers"),"."),(0,i.kt)("p",null,"The algorithm requires two main parameters:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Epsilon")," (",(0,i.kt)("em",{parentName:"li"},"eps"),"): as the maximum distances within which the algorithm searches for neighbors. Epsilon, just like the bandwidth in case of the mean-shift algorithm, has to be adapted to the value of your data point."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Minimum Number of Observations")," (",(0,i.kt)("em",{parentName:"li"},"min","_","sample"),"): as the number of data points required to form a high density area. Note that SciKit learn this value is set to 5 by default and it is optional for you to change this value if necessary.")),(0,i.kt)("p",null,"Given these requirements each data point can be classified as:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Core Point"),": when it has at least the minimum number of data points within it's eps radius."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Border Point"),": when it is within the eps radius of a core point but does not have the required minimum number of data points within it's own radius."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Noise Point"),": when none of the above is true.")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(6053).Z,width:"800",height:"1093"})),(0,i.kt)("h2",{id:"evaluating-performance"},"Evaluating Performance"),(0,i.kt)("p",null,"Once we applied an algorithm to form clusters inside our data set we now have to have a way to evaluate the performance of those clusters - was the algorithm and parameters we choose the best option for the given task?"),(0,i.kt)("p",null,"In case of a ",(0,i.kt)("strong",{parentName:"p"},"Supervised Algorithm")," this can be done by comparing the predictions we get with the true value we know. In case of an ",(0,i.kt)("strong",{parentName:"p"},"Unsupervised Model")," this is not possible. For clustering algorithm we have the option to measure the similarity of data points within a cluster to estimate the performance of the chosen algorithm."),(0,i.kt)("p",null,"SciKit Learn offers two method to evaluate the performance of unsupervised clustering algorithms by measuring how well-defined the clusters edges are (instead of measuring the dispersion within the cluster). We have to keep in mind that those methods don't take the size of each cluster into account."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Silhouette Coefficient Score"),": Calculates the mean distance between each point inside their cluster (a) and the mean distance to it's nearest other clusters (b). The coefficient is calculated by ",(0,i.kt)("inlineCode",{parentName:"li"},"s = (b-a)/max(a,b)")," and results in a score between -1 and 1 - the lower the value, the worse the performance of the cluster. A special case is a value of 0 where clusters start to overlap. This scoring system ",(0,i.kt)("strong",{parentName:"li"},"does not work")," with density based algorithms like DBSCAN."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Calinski-Harabasz Index"),": Calculates the variance of each cluster by the mean square error of each point to the centroid of that cluster. This is then compared to the overall inter-cluster variance. A higher value describes a better definition/separation of each cluster. This scoring system ",(0,i.kt)("strong",{parentName:"li"},"does not work")," with density based algorithms like DBSCAN.")),(0,i.kt)("p",null,"SciKit Learn does not offer a scoring system that works reliable for density-based algorithms."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(77562).Z,width:"800",height:"385"})),(0,i.kt)("p",null,"As we see above, we are getting comparable scores for the k-mean and mean-shift algorithm with the ",(0,i.kt)("strong",{parentName:"p"},"silhouette","_","score")," - k-mean (",(0,i.kt)("em",{parentName:"p"},"0.360"),") is works slightly better than mean-shift (",(0,i.kt)("em",{parentName:"p"},"0.344"),"). The DBSCAN algorithm performs poorly in comparison (",(0,i.kt)("em",{parentName:"p"},"0.089"),"). But the scoring system might fail us here."),(0,i.kt)("p",null,"The sores we get from the ",(0,i.kt)("strong",{parentName:"p"},"calinski","_","harabaz","_","score")," are in line with this observation (k-means_score=1377.88, meanshift=1304.07, dbscan_sore=0.16)."),(0,i.kt)("h1",{id:"supervised-learning-introduction"},"Supervised Learning Introduction"),(0,i.kt)("p",null,"A supervised model explores the relation between a set of features and a target value (label / class). E.g. a person's demographic and their ability to re-pay loans."),(0,i.kt)("h2",{id:"classification-tasks"},"Classification Tasks"),(0,i.kt)("p",null,"Classifications are used to build models with discrete categories as labels. Such tasks output a prediction as a probability of an instance belonging to a label. Common Classification Algorithms are:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Decision Trees"),": A tree-like structure that simulates the decision process based on previous decisions."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Naive Bayes Classifier"),": Relies on probabilistic equations which assume independence among features with the ability to consider several attributes."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Artificial Neutral Networks"),": Replicate the structure of a biological neural network to perform pattern recognition tasks.")),(0,i.kt)("h2",{id:"regression-tasks"},"Regression Tasks"),(0,i.kt)("p",null,"Used for data with continuous quantities as labels, where the value is represented by a quantity and not a set of possible outcomes - e.g. a linear regression."),(0,i.kt)("h1",{id:"supervised-learning-irl"},"Supervised Learning IRL"),(0,i.kt)("p",null,"Finding an algorithm for a task is usually a process of trial & error using testing to validate the resulting model and comparing the result with results from other algorithms."),(0,i.kt)("h2",{id:"data-split"},"Data Split"),(0,i.kt)("p",null,"To avoid introducing bias into a supervised model, the data set is partitioned into 3 sets:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Training Set"),": This set is used to train the models with different algorithms. It consists of input data paired with an outcome / label. It is ",(0,i.kt)("em",{parentName:"li"},"not used")," for performance evaluation of each model later on."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Validation Set"),": This set is used to perform unbiased evaluations of each model and fine-tune parameters to achieve the best performance. The validation set therefore influences the training indirectly. In the end the model that performs the best is chosen to be used on the next set of data."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Testing Set"),": This set consists of data that had no influence over the trained model and is used for a final, unbiased performance evaluation for future predictions by the model.")),(0,i.kt)("p",null,"The ",(0,i.kt)("strong",{parentName:"p"},"Split Ratio")," for those three sets depends on the size of our data set. For sets that contain ",(0,i.kt)("em",{parentName:"p"},"100 - 100,000 instances")," a split ration for training, validating, testing of ",(0,i.kt)("em",{parentName:"p"},"60/20/20%")," is used. For large data sets with ",(0,i.kt)("em",{parentName:"p"},"more than a million instances")," a split ration of ",(0,i.kt)("em",{parentName:"p"},"98/1/1%")," can be sufficient to determining the performance of a model."),(0,i.kt)("p",null,"The ",(0,i.kt)("strong",{parentName:"p"},"Algorithm")," also has an effect on the split ratio you have to use. E.g. if you are working with a model that has a lot of parameters, you might want to work with a larger validation set, while the testing set can remain small."),(0,i.kt)("p",null,"In the following we are using the Iris dataset from sklearn and load it into a Panda data frame. Printing the shape of the data frame, we can see that it consists of 150 rows and 4 columns of data and 1 column for the target value with 150 rows."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(99622).Z,width:"1182",height:"862"})),(0,i.kt)("p",null,"To split up our data in a training (",(0,i.kt)("em",{parentName:"p"},"_","train"),"), validating (",(0,i.kt)("em",{parentName:"p"},"_","dev"),") testing (",(0,i.kt)("em",{parentName:"p"},"_","test"),") set, we can use the ",(0,i.kt)("strong",{parentName:"p"},"train","_","test","_","split")," method from sklearn. We are going to use a ",(0,i.kt)("inlineCode",{parentName:"p"},"test_size")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"0.2")," - meaning that we are training our model with 80% of the data - which corresponds to 120 rows, while 30 rows are allocated for testing our model later on."),(0,i.kt)("p",null,"In a second step we need to subtract another portion of the data to from our validation group - which removes another 30 rows from our training data."),(0,i.kt)("h2",{id:"cross-validation"},"Cross Validation"),(0,i.kt)("p",null,"To further reduce bias in our models we can further partition our data into ",(0,i.kt)("strong",{parentName:"p"},"K")," number of groups and re-sample data from those groups as we train and validate our model - this is called ",(0,i.kt)("strong",{parentName:"p"},"K-fold Cross Validation"),", where K is usually replaced by the number of samples used in the training process."),(0,i.kt)("p",null,"This process can replace the testing step discussed above or can be done in addition:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"three-split approach"),": A testing set is subtracted from the data. The rest of the data is used in the cross-validation process."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"two-split approach"),": The complete data set is used for cross-validation.")),(0,i.kt)("p",null,"In practice this process contains 4 steps that are repeated ",(0,i.kt)("strong",{parentName:"p"},"K-times"),":"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"First the data is shuffled."),(0,i.kt)("li",{parentName:"ol"},"The data is split according to the process described above."),(0,i.kt)("li",{parentName:"ol"},"The model is trained and the selected validation group is used to fine-tune parameters."),(0,i.kt)("li",{parentName:"ol"},"The results from the training are stored and the process begins again with step 1 and is repeated K times.")),(0,i.kt)("p",null,"You end up with K data sets and a model that is trained K-times - the results refined with every iteration."),(0,i.kt)("p",null,"In the following we are going to use the same data set as before and use the ",(0,i.kt)("strong",{parentName:"p"},"three-split-approach")," by first removing 20% of our data to form a testing set. We then import the ",(0,i.kt)("strong",{parentName:"p"},"KFold Method")," from Sklearn to split our data in 10 subgroups:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(97771).Z,width:"1181",height:"600"})),(0,i.kt)("p",null,"We now have a data set with 10 subgroups that can be referenced by an index number. We can use a ",(0,i.kt)("em",{parentName:"p"},"for-loop")," to iterate over the data."),(0,i.kt)("h2",{id:"metric-evaluation"},"Metric Evaluation"),(0,i.kt)("p",null,"The accuracy of a model can be calculated as a percentage by comparing it's predicted values with real (unseen as not used in the training of the model) measurements. "),(0,i.kt)("p",null,"This can visualized in the so called ",(0,i.kt)("strong",{parentName:"p"},"Confusion Matrix")," - a 2-dimensional matrix that contains the predictions as columns and the occurrence or non-occurrence of events as rows."),(0,i.kt)("p",null,"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa)."),(0,i.kt)("p",null,"If a classification system has been trained to distinguish between apples, oranges and pineapples, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 27 animals \u2014 8 apples, 6 oranges, and 13 pineapples, the resulting confusion matrix could look like the table below:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Prediction / Actual"),(0,i.kt)("th",{parentName:"tr",align:"center"},"Apple (A)"),(0,i.kt)("th",{parentName:"tr",align:"center"},"Orange (A)"),(0,i.kt)("th",{parentName:"tr",align:"center"},"Pineapple (A)"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Apple (P)"),(0,i.kt)("td",{parentName:"tr",align:"center"},(0,i.kt)("strong",{parentName:"td"},"5")),(0,i.kt)("td",{parentName:"tr",align:"center"},"2"),(0,i.kt)("td",{parentName:"tr",align:"center"},"0")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Orange (P)"),(0,i.kt)("td",{parentName:"tr",align:"center"},"3"),(0,i.kt)("td",{parentName:"tr",align:"center"},(0,i.kt)("strong",{parentName:"td"},"3")),(0,i.kt)("td",{parentName:"tr",align:"center"},"2")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Pineapple (P)"),(0,i.kt)("td",{parentName:"tr",align:"center"},"0"),(0,i.kt)("td",{parentName:"tr",align:"center"},"1"),(0,i.kt)("td",{parentName:"tr",align:"center"},(0,i.kt)("strong",{parentName:"td"},"11"))))),(0,i.kt)("p",null,"In this confusion matrix, of the 8 actual apples, the system predicted that three were oranges, and of the six oranges, it predicted that one was a pineapple and two were apples. We can see from the matrix that the system in question has trouble distinguishing between apples and oranges, but can make the distinction between pineapples and other types of animals pretty well. All correct predictions are loaded in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal."),(0,i.kt)("h2",{id:"table-of-confusion"},"Table of Confusion"),(0,i.kt)("p",null,"In predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 apples and only 5 oranges in the data, a particular classifier might classify all the observations as apples. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate (sensitivity) for the apple class but a 0% recognition rate for the orange class."),(0,i.kt)("p",null,"Assuming the confusion matrix above, its corresponding table of confusion, for the apple class, would be:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Prediction / Actual"),(0,i.kt)("th",{parentName:"tr",align:null},"Apple (A)"),(0,i.kt)("th",{parentName:"tr",align:null},"Not-Apple (A)"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Apple (P)"),(0,i.kt)("td",{parentName:"tr",align:null},"5 True Positives"),(0,i.kt)("td",{parentName:"tr",align:null},"2 False Positives")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Not-Apple (P)"),(0,i.kt)("td",{parentName:"tr",align:null},"3 False Negatives"),(0,i.kt)("td",{parentName:"tr",align:null},"17 True Negatives")))),(0,i.kt)("h3",{id:"evaluation-of-tasks-with-binary-output-labels"},"Evaluation of Tasks with binary output labels"),(0,i.kt)("p",null,"The performance of the model can be calculated based on the number of predictions that turned out to be true. The performance is given per feature (e.g. our model has a high accuracy to predict pineapples, but is a bit confused when it comes to distinguishing between apples and oranges). The performance table for a pineapple gives us the accuracy for the models prediction capabilities to recognize pineapples as follows:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Actual / Prediction"),(0,i.kt)("th",{parentName:"tr",align:null},"Pineapple (P)"),(0,i.kt)("th",{parentName:"tr",align:null},"Any other fruit (P)"),(0,i.kt)("th",{parentName:"tr",align:null},"Sum"),(0,i.kt)("th",{parentName:"tr",align:null},"Accuracy"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Pineapple (A)"),(0,i.kt)("td",{parentName:"tr",align:null},"976"),(0,i.kt)("td",{parentName:"tr",align:null},"34"),(0,i.kt)("td",{parentName:"tr",align:null},"1000"),(0,i.kt)("td",{parentName:"tr",align:null},"97.6 %")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Any other fruit (A)"),(0,i.kt)("td",{parentName:"tr",align:null},"157"),(0,i.kt)("td",{parentName:"tr",align:null},"843"),(0,i.kt)("td",{parentName:"tr",align:null},"1000"),(0,i.kt)("td",{parentName:"tr",align:null},"84.3 %")))),(0,i.kt)("p",null,"When the model saw a pineapple, out of 1000 instances it correctly predicted the type of fruit 976 times - giving us an accuracy of 97.6 %. But when seeing the image of an orange or apple it still concluded that it was a pineapple 157 times out of 1000 - resulting in an accuracy of 84.3 %."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Accuracy Metric")," : To calculate the Accuracy of the model over all instances the sum of all ",(0,i.kt)("strong",{parentName:"li"},"True Positives")," and ",(0,i.kt)("strong",{parentName:"li"},"True Negatives")," is divided by the total number of instances: ",(0,i.kt)("em",{parentName:"li"},"Accuracy = (TP + TN)/m"))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Precision Metric")," : To calculate the precision of model over all instances to classify positive binary labels we need to calculate: ",(0,i.kt)("em",{parentName:"li"},"Precision = TP / (TP + FP)")," e.g. (976) / (976 + 157) = 86.1%")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Recall Metric")," : The recall metric is the number of correctly predicted positive labels against all correctly predicted labels, positive and negative: ",(0,i.kt)("em",{parentName:"li"},"Recall = TP / (TP + FN)")," e.g. (976) / (976 + 34) = 97.6%")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(22710).Z,width:"1139",height:"1085"})),(0,i.kt)("h3",{id:"evaluation-of-regression-tasks"},"Evaluation of Regression Tasks"),(0,i.kt)("p",null,"Regression tasks have continuous outputs without a fixed number of output labels - here we cannot use any of the metrics above to evaluate the accuracy or precision of the predictions of our model."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Mean Absolute Error")," : The MAE is the average distance between the predicted result and the actual value, without taking into account the direction of the error: ",(0,i.kt)("em",{parentName:"li"},"MAE = 1/m * \u2211 (over all m instances) (y(actual) - y(predicted)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Root Mean Square Error")," : RMSE = \u221a (1/m * \u2211 (over all m instances) (y(actual) - y(predicted))\xb2")),(0,i.kt)("p",null,"In both cases the ideal model (prediction = actual value) would result in an error of ",(0,i.kt)("inlineCode",{parentName:"p"},"0"),". In real applications you would try different models on your data and compare their prediction error - the one with the lowest error value wins. "),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Python SciKit-Learn",src:a(49248).Z,width:"1149",height:"1013"})))}h.isMDXComponent=!0},84990:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-33796028333_a7fa30ab08_o-c59d7b60abca3582796fbd0c7132d682.jpg"},46780:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_01-b32c58e4ab112551197dafed7766a2bb.png"},76934:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_02-4a8ed7c5a0b12a21cb92dafe5082075c.png"},80906:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_03-55716561bd24d727414489bec67ab005.png"},49148:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_04-0b0de1549b0c18119e77a656287529e2.png"},72538:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_05-1047b7d4f8989a44599e6864f3cd010d.png"},68836:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_06-dd7e0d9df21912cb0167df840e984d14.png"},6053:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_07-a3836daef336dd3df85bb237a44461e3.png"},77562:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_08-32fbcc8977c6082b1597ce7362a56fc5.png"},99622:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_09-4ef7bb5476df94ab25d36d23051a8413.png"},97771:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_10-e761a252eb62c598eb0f72999b1ba369.png"},22710:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_11-47cbc3354547787ab12b49d157aeab59.png"},49248:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/python-ml_12-744f7e670042cd19a1f016b7bc796fd2.png"}}]);