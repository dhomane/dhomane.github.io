"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[89015],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),h=o,m=u["".concat(s,".").concat(h)]||u[h]||d[h]||i;return n?a.createElement(m,r(r({ref:t},c),{},{components:n})):a.createElement(m,r({ref:t},c))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},82382:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const i={sidebar_position:4830,slug:"2023-01-05",title:"YOLOv7 Introduction",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO"],description:"Getting started with object detection in YOLOv7"},r=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index",id:"IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index",title:"YOLOv7 Introduction",description:"Getting started with object detection in YOLOv7",source:"@site/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2023-01-05-yolov7",slug:"/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"YOLO",permalink:"/docs/tags/yolo"}],version:"current",sidebarPosition:4830,frontMatter:{sidebar_position:4830,slug:"2023-01-05",title:"YOLOv7 Introduction",authors:"mpolinowski",tags:["Python","Machine Learning","YOLO"],description:"Getting started with object detection in YOLOv7"},sidebar:"tutorialSidebar",previous:{title:"MiDaS Depth Vision",permalink:"/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08"},next:{title:"Recurrent Neural Networks",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-31-tf-rnn-text-generation/2022-12-31"}},s={},p=[{value:"Run YOLOv7 on Arch LINUX",id:"run-yolov7-on-arch-linux",level:2},{value:"Clone the Repository",id:"clone-the-repository",level:3},{value:"Install all Dependencies",id:"install-all-dependencies",level:3},{value:"Download pre-trained Weights",id:"download-pre-trained-weights",level:3},{value:"Test the Model",id:"test-the-model",level:2},{value:"Image Files",id:"image-files",level:3},{value:"Video Files",id:"video-files",level:3},{value:"Videostreaming",id:"videostreaming",level:3}],c={toc:p};function d(e){let{components:t,...i}=e;return(0,o.kt)("wrapper",(0,a.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Guangzhou, China",src:n(64898).Z,width:"1500",height:"652"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#run-yolov7-on-arch-linux"},"Run YOLOv7 on Arch LINUX"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#clone-the-repository"},"Clone the Repository")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#install-all-dependencies"},"Install all Dependencies")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#download-pre-trained-weights"},"Download pre-trained Weights")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#test-the-model"},"Test the Model"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#image-files"},"Image Files")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#video-files"},"Video Files")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#videostreaming"},"Videostreaming"))))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("strong",{parentName:"p"},"Citation"),": ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2207.02696"},"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors")," ",(0,o.kt)("em",{parentName:"p"},"Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao"),": YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/WongKinYiu/yolov7"},"Github"))),(0,o.kt)("h2",{id:"run-yolov7-on-arch-linux"},"Run YOLOv7 on Arch LINUX"),(0,o.kt)("h3",{id:"clone-the-repository"},"Clone the Repository"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/mpolinowski/yolov7.git\ncd yolov7\n")),(0,o.kt)("h3",{id:"install-all-dependencies"},"Install all Dependencies"),(0,o.kt)("p",null,"The repository comes with an ",(0,o.kt)("inlineCode",{parentName:"p"},"Requirements.txt")," that you can install globally or inside a virtEnvironment ",(0,o.kt)("inlineCode",{parentName:"p"},"pip3 install -r requirements.txt"),". But I am going to use ",(0,o.kt)("a",{parentName:"p",href:"/docs/Development/Python/2022-12-11-pipenv/2022-12-11"},"PipEnv")," instead:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pipenv --version\npipenv, version 2022.12.19\n\npipenv install -r requirements.txt\nCreating a Pipfile for this project...\nRequirements file provided! Importing into Pipfile...\nPipfile.lock not found, creating...\nInstalling dependencies from Pipfile.lock (9ec603)...\nTo activate this project's virtualenv, run pipenv shell.\nAlternatively, run a command inside the virtualenv with pipenv run.\n")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"pipfile")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'[[source]]\nurl = "https://pypi.org/simple"\nverify_ssl = true\nname = "pypi"\n\n[packages]\nmatplotlib = ">=3.2.2"\nnumpy = "<1.24.0,>=1.18.5"\nopencv-python = ">=4.1.1"\npillow = ">=7.1.2"\npyyaml = ">=5.3.1"\nrequests = ">=2.23.0"\nscipy = ">=1.4.1"\ntorch = ">=1.7.0,!=1.12.0"\ntorchvision = ">=0.8.1,!=0.13.0"\ntqdm = ">=4.41.0"\nprotobuf = "<4.21.3"\ntensorboard = ">=2.4.1"\npandas = ">=1.1.4"\nseaborn = ">=0.11.0"\nipython = "*"\npsutil = "*"\nthop = "*"\n\n[dev-packages]\n\n[requires]\npython_version = "3.10"\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"UPDATE"),": I ran into some issues changing the torch and torchvision version here to their CUDA counterparts. Since I had all the dependencies globally installed anyway, I dropped the virtual environment and made sure that ",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/"},"PyTorch was installed with GPU support"),":"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Introduction",src:n(85145).Z,width:"812",height:"307"})),(0,o.kt)("p",null,"You can empty the virtual environment with ",(0,o.kt)("inlineCode",{parentName:"p"},"pipenv uninstall --all")," and run all scripts without the ",(0,o.kt)("inlineCode",{parentName:"p"},"pipenv run")," prefix if you encounter the same issue:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source hongkong.jpg\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Introduction",src:n(75179).Z,width:"2385",height:"1591"})),(0,o.kt)("h3",{id:"download-pre-trained-weights"},"Download pre-trained Weights"),(0,o.kt)("p",null,"The weights we can use are ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/yolov7#testing"},"linked in the repo README")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n")),(0,o.kt)("h2",{id:"test-the-model"},"Test the Model"),(0,o.kt)("p",null,"The ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/yolov7#inference"},"README")," also holds the two commands we can use to try out the pre-trained model - one for video and one for still images:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source yourvideo.mp4\npython detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg\n")),(0,o.kt)("h3",{id:"image-files"},"Image Files"),(0,o.kt)("p",null,"Since I used ",(0,o.kt)("inlineCode",{parentName:"p"},"pipenv")," I have to prepend the ",(0,o.kt)("inlineCode",{parentName:"p"},"python")," in this command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source harbin.jpeg\n\n21 persons, 7 cars, 1 bus, 1 truck, 4 traffic lights, 1 umbrella, 3 handbags, Done. (49.1ms) Inference, (11.6ms) NMS\n The image with the result is saved in: runs/detect/exp/harbin.jpeg\nDone. (0.786s)\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Introduction",src:n(42918).Z,width:"5362",height:"3029"})),(0,o.kt)("h3",{id:"video-files"},"Video Files"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source video.mp4\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Introduction",src:n(76757).Z,width:"485",height:"271"})),(0,o.kt)("h3",{id:"videostreaming"},"Videostreaming"),(0,o.kt)("p",null,"According to the documentation you can select a connected webcam from your system as a video source by selected the video source, e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"--source 0"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"--source 1")," etc.:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --view-img --nosave --source 0\n")),(0,o.kt)("p",null,"But looking at the ",(0,o.kt)("inlineCode",{parentName:"p"},"detect.py")," file shows me that it also accepts RTSP streams from IP cameras as source:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"}," webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n")),(0,o.kt)("p",null,"So let't try this with an INSTAR WQHD IP camera:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pipenv run python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --view-img --nosave --source rtsp://admin:instar@192.168.2.120:554/livestream/13\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"YOLOv7 Introduction",src:n(39430).Z,width:"1007",height:"572"})))}d.isMDXComponent=!0},75179:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/YOLOv7_Introduction_00-384c3de34bcedbf8432d4aad47f5ecc2.jpg"},85145:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/YOLOv7_Introduction_00-0746a1b6adab6f65d30ca7fee9d1b169.png"},42918:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/YOLOv7_Introduction_01-77eeea431addd10589622e75691f74e6.jpg"},76757:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/YOLOv7_Introduction_02-8f61d47cbb03764072abb1c2af14954b.gif"},39430:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/YOLOv7_Introduction_03-46f40120c7d47fbdfc92dac93c97b574.png"},64898:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ba3b23aa3d5392c02b451d1b2b911721.jpg"}}]);