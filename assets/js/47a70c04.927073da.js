"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[87656],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=d(a),m=r,h=u["".concat(s,".").concat(m)]||u[m]||c[m]||o;return a?n.createElement(h,l(l({ref:t},p),{},{components:a})):n.createElement(h,l({ref:t},p))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=u;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var d=2;d<o;d++)l[d]=a[d];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},3743:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var n=a(87462),r=(a(67294),a(3905));const o={sidebar_position:4900,slug:"2022-12-19",title:"Tensorflow Hub",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"TensorFlow Hub is a repository of trained machine learning models."},l=void 0,i={unversionedId:"IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/index",id:"IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/index",title:"Tensorflow Hub",description:"TensorFlow Hub is a repository of trained machine learning models.",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-19-tf-hub",slug:"/IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/2022-12-19",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/2022-12-19",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4900,frontMatter:{sidebar_position:4900,slug:"2022-12-19",title:"Tensorflow Hub",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"TensorFlow Hub is a repository of trained machine learning models."},sidebar:"tutorialSidebar",previous:{title:"Tensorflow Representation Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-20-tf-representation/2022-12-19"},next:{title:"Tensorflow Transfer Learning",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-18-tf-transfer-learning/2022-12-18"}},s={},d=[{value:"The MobileNet v2 Model",id:"the-mobilenet-v2-model",level:2},{value:"Test the Pre-trained Model on an random Image",id:"test-the-pre-trained-model-on-an-random-image",level:3},{value:"Test the Model on the Flower Dataset",id:"test-the-model-on-the-flower-dataset",level:3},{value:"Model Building and Training",id:"model-building-and-training",level:2},{value:"Evaluation",id:"evaluation",level:3}],p={toc:d};function c(e){let{components:t,...o}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Guangzhou, China",src:a(3151).Z,width:"1500",height:"383"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#the-mobilenet-v2-model"},"The MobileNet v2 Model"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#test-the-pre-trained-model-on-an-random-image"},"Test the Pre-trained Model on an random Image")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#test-the-model-on-the-flower-dataset"},"Test the Model on the Flower Dataset")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#model-building-and-training"},"Model Building and Training"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#evaluation"},"Evaluation"))))),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-flowers"},"Github"))),(0,r.kt)("p",null,"In the previous tutorial I used the ",(0,r.kt)("a",{parentName:"p",href:"https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2022-12-18-tf-transfer-learning/2022-12-18"},"ResNet50")," pre-trained model to help me with a binary image classification task - dogs or cats."),(0,r.kt)("p",null,"Tensorflow offers a large variety of pre-trained models that we can use on it's hub. ",(0,r.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/hub"},"TensorFlow Hub")," is a repository of trained machine learning models ready for fine-tuning and deployable anywhere."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow Transfer Learning",src:a(29962).Z,width:"1288",height:"536"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"pip install --upgrade tensorflow_hub\n")),(0,r.kt)("h2",{id:"the-mobilenet-v2-model"},"The MobileNet v2 Model"),(0,r.kt)("p",null,"From the ",(0,r.kt)("a",{parentName:"p",href:"https://tfhub.dev/s?tf-version=tf2"},"available collection of models")," we can choose ",(0,r.kt)("a",{parentName:"p",href:"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"},"MobileNet v2"),". MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks, originally published by"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Mark Sandler"),", ",(0,r.kt)("strong",{parentName:"p"},"Andrew Howard"),", ",(0,r.kt)("strong",{parentName:"p"},"Menglong Zhu"),", ",(0,r.kt)("strong",{parentName:"p"},"Andrey Zhmoginov"),", ",(0,r.kt)("strong",{parentName:"p"},"Liang-Chieh Chen"),": ",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1801.04381"},"Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation"),", 2018.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'# import mobile net with trained weights\nTrained_MobileNet_url ="https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4"\n\nTrained_MobileNet = tf.keras.Sequential([\n    hub.KerasLayer(Trained_MobileNet_url, input_shape=(224,224,3))])\n')),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Side note"),": ",(0,r.kt)("inlineCode",{parentName:"p"},"OSError: https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4 does not appear to be a valid module.")," means you do not have a connection to the server. I was able to open this page and download the model manually but the Hub download here refused to do that. China network issue... switching to another VPN server solved it.")),(0,r.kt)("h3",{id:"test-the-pre-trained-model-on-an-random-image"},"Test the Pre-trained Model on an random Image"),(0,r.kt)("p",null,"Just like with the ",(0,r.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-18-tf-transfer-learning/2022-12-18"},"ResNet50 model")," we can use the MobileNet model out-of-the-box to classify images it has already been trained on."),(0,r.kt)("p",null,"Before feeding it a test image we first have to preprocess the image - identical to the ResNet50 example before:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# test the raw mobilenet model\nsample_image = tf.keras.preprocessing.image.load_img(r'./test_images/watch.png', target_size = (224, 224))\n\nsample_image = np.array(sample_image)/255.0\npredicted_class = Trained_MobileNet.predict(np.expand_dims(sample_image, axis = 0))\npredicted_class = np.argmax(predicted_class)\n")),(0,r.kt)("p",null,"The MobileNet classes are identical to the 1000 ImageNet labels it has been trained against and have to be downloaded manually this time:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\n")),(0,r.kt)("p",null,"Now we can run our prediction on our random image - the model will return a number label that we can assign an ImageNet label to using the downloaded file above:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'# show image with predicted class\nplt.imshow(sample_image)\npredicted_class_name = imagenet_labels[predicted_class]\nplt.title("Prediction: " + predicted_class_name.title())\nplt.show()\n')),(0,r.kt)("p",null,"And we get a magnetic compass... oh well..."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow Transfer Learning",src:a(12257).Z,width:"980",height:"517"})),(0,r.kt)("p",null,"But as they say, when things start to fail, go with cats - excellent \ud83d\udc4d"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow Transfer Learning",src:a(93221).Z,width:"1010",height:"492"})),(0,r.kt)("h3",{id:"test-the-model-on-the-flower-dataset"},"Test the Model on the Flower Dataset"),(0,r.kt)("p",null,"We can use the flower dataset provided by Tensorflow to train our model:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# download and process training dataset\n# specify path of the flowers dataset\nflowers_data_url = tf.keras.utils.get_file(\n  'flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n   untar=True)\n# preprocessing\nimage_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\nflowers_data = image_generator.flow_from_directory(str(flowers_data_url), target_size=(224,224), batch_size = 64, shuffle = True)\n")),(0,r.kt)("p",null,"The dataset consists of 3670 pictures in 5 classes:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'for flowers_data_input_batch, flowers_data_label_batch in flowers_data:\n  print("Image batch shape: ", flowers_data_input_batch.shape)\n  print("Label batch shape: ", flowers_data_label_batch.shape)\n  break\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"Found 3670 images belonging to 5 classes.\nImage batch shape:  (64, 224, 224, 3)\nLabel batch shape:  (64, 5)\n")),(0,r.kt)("p",null,"We can now run a prediction on the first batch (64 images) of the flower dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"predictions_batch = Trained_MobileNet.predict(flowers_data_input_batch)\n# We have a batch size of 64 with 1000 classes\n# print(predictions_batch.shape)\n# get classnames for imagenet classes\npredicted_class_names = imagenet_labels[np.argmax(predictions_batch, axis=-1)]\n# print(predicted_class_names)\n\n# print all 64 images\n# and add predicted class\nplt.figure(figsize=(15,15))\n\nfor n in range(64):\n  plt.subplot(8,8,n+1)\n  plt.tight_layout()\n  plt.imshow(flowers_data_input_batch[n])\n  plt.title(predicted_class_names[n])\n  plt.axis('off')\n\nplt.show()\n")),(0,r.kt)("p",null,"We can see that the model performed poorly without prior training with our dataset:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow Transfer Learning",src:a(5918).Z,width:"1447",height:"971"})),(0,r.kt)("h2",{id:"model-building-and-training"},"Model Building and Training"),(0,r.kt)("p",null,"To improve the performance of our model we can download the feature extraction layer from MobileNet only and freeze it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'# download the MobileNet without the classification head\nMobileNet_feature_extractor_url = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2"\nMobileNet_feature_extractor_layer = hub.KerasLayer(MobileNet_feature_extractor_url, input_shape=(224, 224, 3))\n# freeze the feature extraction layer from mobilenet\nMobileNet_feature_extractor_layer.trainable = False\n')),(0,r.kt)("p",null,"Again, we will need to download our dataset and preprocess it for consumption by our neural network:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# download and process training dataset\n# specify path of the flowers dataset\nflowers_data_url = tf.keras.utils.get_file(\n  'flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n   untar=True)\n# pre-processing data\nimage_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\nflowers_data = image_generator.flow_from_directory(str(flowers_data_url), target_size=(224,224), batch_size = 64, shuffle = True)\n\n# create input batches from flower data\nfor flowers_data_input_batch, flowers_data_label_batch in flowers_data:\n  break\n")),(0,r.kt)("p",null,"But instead of feeding the data directly into MobileNet we now need to build our own model - based on the locked feature detection layer from MobileNet and an additional dense that we can train with our dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# build new model out of feature extraction layer\n# and a fresh dense layer for us to train\nmodel = tf.keras.Sequential([\n  MobileNet_feature_extractor_layer,\n  tf.keras.layers.Dense(flowers_data.num_classes, activation='softmax')\n])\n")),(0,r.kt)("p",null,"We can now build the model and train it's dense layer using the flower dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# build the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n# and train it with flower dataset\nhistory = model.fit_generator(flowers_data, epochs=50)\n")),(0,r.kt)("p",null,"And again, after a very short training we are already at an accuracy closing in to 100%:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"Epoch 50/50\n58/58 [==============================] - 6s 104ms/step - loss: 0.0212 - accuracy: 0.9984\n")),(0,r.kt)("h3",{id:"evaluation"},"Evaluation"),(0,r.kt)("p",null,"To test our model we can first extract the classification labels from our dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# evaluate the model\n## get classification labels from dataset\nclass_names = sorted(flowers_data.class_indices.items(), key=lambda pair:pair[1])\nclass_names = np.array([key.title() for key, value in class_names])\nprint(class_names)\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"The dataset uses the following labels and image that go along with them ",(0,r.kt)("inlineCode",{parentName:"p"},"['Daisy' 'Dandelion' 'Roses' 'Sunflowers' 'Tulips']"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'# now we can feed a fresh batch of images to the trained model\npredicted_batch = model.predict(flowers_data_input_batch)\n# predict labels for each image inside the batch\npredicted_id = np.argmax(predicted_batch, axis=-1)\npredicted_label_batch = class_names[predicted_id]\n\nlabel_id = np.argmax(flowers_data_input_batch, axis=-1)\n\n# show images with labels\nplt.figure(figsize=(10,9))\nplt.subplots_adjust(hspace=0.5)\n\nfor n in range(64):\n  plt.subplot(8,8,n+1)\n  plt.tight_layout()\n  plt.imshow(flowers_data_input_batch[n])\n  # color = "green" if predicted_id[n] == label_id[n] else "red"\n  color = "black"\n  plt.title(predicted_label_batch[n].title(), color=color)\n  plt.axis(\'off\')\n\nplt.show()\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tensorflow Transfer Learning",src:a(58750).Z,width:"995",height:"895"})))}c.isMDXComponent=!0},29962:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Tensorflow_Transfer_Learning_01-da7b26e0a598dda265be40c96433386c.png"},12257:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Tensorflow_Transfer_Learning_02-89f2c48f2b6bab91a0b5de23b0527204.png"},93221:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Tensorflow_Transfer_Learning_03-06f2c531495e29eed45b8f30c9ebdafb.png"},5918:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Tensorflow_Transfer_Learning_04-bd3e77079d561c75026689e3e1489c07.png"},58750:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Tensorflow_Transfer_Learning_05-a678dd04ed9b3570dc23afd1009f8b5d.png"},3151:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);