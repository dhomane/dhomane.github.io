"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[41035],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),h=p(n),g=r,m=h["".concat(l,".").concat(g)]||h[g]||d[g]||o;return n?a.createElement(m,i(i({ref:t},c),{},{components:n})):a.createElement(m,i({ref:t},c))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},56478:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const o={sidebar_position:5090,slug:"2022-02-18",title:"Yolo App - Tesseract Optical Character Recognition",authors:"mpolinowski",tags:["Tensorflow","Machine Learning","Python","YOLO"]},i=void 0,s={unversionedId:"IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/index",id:"IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/index",title:"Yolo App - Tesseract Optical Character Recognition",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr",slug:"/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/2022-02-18",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/2022-02-18",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/index.md",tags:[{label:"Tensorflow",permalink:"/docs/tags/tensorflow"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"YOLO",permalink:"/docs/tags/yolo"}],version:"current",sidebarPosition:5090,frontMatter:{sidebar_position:5090,slug:"2022-02-18",title:"Yolo App - Tesseract Optical Character Recognition",authors:"mpolinowski",tags:["Tensorflow","Machine Learning","Python","YOLO"]},sidebar:"tutorialSidebar",previous:{title:"Yolo App - Flask Web Application",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-19--yolo-app-flask/2022-02-19"},next:{title:"Yolo App - Pipeline Predictions",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-02-17--yolo-app-prediction-pipeline/2022-02-17"}},l={},p=[{value:"Load an Image",id:"load-an-image",level:2},{value:"Extract Number Plate",id:"extract-number-plate",level:2},{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"Use Tesseract",id:"use-tesseract",level:2},{value:"Skew Detection and Correction",id:"skew-detection-and-correction",level:2}],c={toc:p};function d(e){let{components:t,...o}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Shenzhen, China",src:n(22752).Z,width:"1500",height:"688"})),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15"},"Prepare your Images and get Data")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-16--yolo-app-tensorflow-model/2022-02-16"},"Train your Tensorflow Model")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-17--yolo-app-prediction-pipeline/2022-02-17"},"Use your Model to do Predictions")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/2022-02-18"},"Use Tesseract to Read Number Plates")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-19--yolo-app-flask/2022-02-19"},"Flask Web Application")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-02-20--yolo-app-yolov5-data-prep/2022-02-20"},"Yolo v5 - Data Prep"))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#load-an-image"},"Load an Image")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#extract-number-plate"},"Extract Number Plate")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#data-preprocessing"},"Data Preprocessing")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#use-tesseract"},"Use Tesseract")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#skew-detection-and-correction"},"Skew Detection and Correction"))),(0,r.kt)("p",null,"Now that I have the Tensorflow model that finds number plates inside images I can use OpenCV to cut them out and hand them over to Tesseract - see ",(0,r.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2021-10-31--tesseract_ocr_arch_linux/2021-10-31"},"Install Tesseract on Arch LINUX")," - to apply some Optical Character Recognition (",(0,r.kt)("strong",{parentName:"p"},"OCR"),")."),(0,r.kt)("h2",{id:"load-an-image"},"Load an Image"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"import pytesseract as pt\n\npath = './test_images/index10.jpg'\nimage, cods = object_detection(path)\n\nplt.figure(figsize=(10,8))\nplt.imshow(image)\nplt.show()\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tesseract OCR",src:n(34402).Z,width:"1081",height:"601"})),(0,r.kt)("h2",{id:"extract-number-plate"},"Extract Number Plate"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"img = np.array(load_img(path))\nxmin ,xmax,ymin,ymax = cods[0]\nroi = img[ymin:ymax,xmin:xmax]\nplt.imshow(roi)\nprint('Original')\nplt.show()\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tesseract OCR",src:n(63255).Z,width:"1078",height:"295"})),(0,r.kt)("h2",{id:"data-preprocessing"},"Data Preprocessing"),(0,r.kt)("p",null,"Depending on the image source we might have to add ",(0,r.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/2021-12-03#image-thresholding"},"some processing")," to the extracted region of interest to make it more readable:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# Turn grayscale\ngray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\ngray_roi = cv2.bitwise_not(gray_roi)\n\n# threshold the image, setting all foreground pixels to\n# 255 and all background pixels to 0 (invert)\nthresh_roi = cv2.threshold(gray_roi, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\nplt.imshow(thresh_roi)\nprint('Threshold')\nplt.show()\n\ncanny_roi = cv2.Canny(roi, 85, 255)\nplt.imshow(canny_roi)\nprint('Canny')\nplt.show()\n")),(0,r.kt)("p",null,"Tweak the thresholds until you get the best OCR results for your use case:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tesseract OCR",src:n(6030).Z,width:"1075",height:"630"})),(0,r.kt)("h2",{id:"use-tesseract"},"Use Tesseract"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# OCR the ROI using Tesseract\ntext_roi = pt.image_to_string(roi)\nprint('Original:',text_roi)\ntext_thresh = pt.image_to_string(thresh_roi)\nprint('Threshold:',text_thresh)\ntext_canny = pt.image_to_string(canny_roi)\nprint('Canny:',text_canny)\n")),(0,r.kt)("p",null,"Threshold gives me the best results for the given image, while canny fails completely. Your results will differ depending on your image source:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"Original: i ~HR26I]KOB30|\n\nThreshold: i\\ HR26DK0830\n\nCanny:  \n")),(0,r.kt)("h2",{id:"skew-detection-and-correction"},"Skew Detection and Correction"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"THIS DOES NOT WORK"),": I will have to look into this more - the method by ",(0,r.kt)("a",{parentName:"p",href:"https://pyimagesearch.com/2017/02/20/text-skew-correction-opencv-python/"},"pyimagesearch")," seems to be distracted when the text is not only rotated but there is also some perspective shift on it."),(0,r.kt)("p",null,"Tesseract, unfortunately, is very sensitive against skew angles. As soon as the text is not perfectly horizontal you are not going to get any results:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Tesseract OCR",src:n(6514).Z,width:"1066",height:"639"})),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"http://felix.abecassis.me/2011/10/opencv-rotation-deskewing/"},"Here is an implementation to de-skew your text")," using OpenCV. I can use it to compute the minimum rotated bounding box that contains the text regions:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# grab the (x, y) coordinates of all pixel values that\n# are greater than zero, then use these coordinates to\n# compute a rotated bounding box that contains all\n# coordinates\npix_coords = np.column_stack(np.where(thresh_roi_skew > 0))\nangle = cv2.minAreaRect(pix_coords)[-1]\n# the `cv2.minAreaRect` function returns values in the\n# range [-90, 0); as the rectangle rotates clockwise the\n# returned angle trends to 0 -- in this special case we\n# need to add 90 degrees to the angle\nif angle < -45:\n    angle = -(90 + angle)\n# otherwise, just take the inverse of the angle to make\n# it positive\nelse:\n    angle = -angle\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# rotate the image to deskew it\n(h, w) = thresh_roi_skew.shape[:2]\ncenter = (w // 2, h // 2)\nM = cv2.getRotationMatrix2D(center, angle, 1.0)\nrotated = cv2.warpAffine(thresh_roi_skew, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'# draw the correction angle on the image so we can validate it\ncv2.putText(rotated, "Angle: {:.2f} degrees".format(angle), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n# show the output image\nprint("[INFO] rotation angle: {:.3f}".format(angle))\nplt.imshow(rotated)\nplt.show()\n')))}d.isMDXComponent=!0},34402:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Yolo_App_Tesseract_OCR_01-892a9b0319da6446eee5e64d1336efc9.png"},63255:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Yolo_App_Tesseract_OCR_02-191eae389eb010cd275988de82750b49.png"},6030:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Yolo_App_Tesseract_OCR_03-c6cbb7d96027984a9ddf8ab6c5fd366c.png"},6514:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Yolo_App_Tesseract_OCR_04-c83fcf308a6a3e664a9735d3928de20f.png"},22752:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-ffe80356d19fb4b090a3bef79b45aab3.jpg"}}]);