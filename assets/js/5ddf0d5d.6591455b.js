"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[31127],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>h});var n=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function c(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),l=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=l(e.components);return n.createElement(s.Provider,{value:a},e.children)},d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=c(e,["components","mdxType","originalType","parentName"]),m=l(t),h=r,u=m["".concat(s,".").concat(h)]||m[h]||d[h]||i;return t?n.createElement(u,o(o({ref:a},p),{},{components:t})):n.createElement(u,o({ref:a},p))}));function h(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=m;var c={};for(var s in a)hasOwnProperty.call(a,s)&&(c[s]=a[s]);c.originalType=e,c.mdxType="string"==typeof e?e:r,o[1]=c;for(var l=2;l<i;l++)o[l]=t[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},6864:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>c,toc:()=>l});var n=t(87462),r=(t(67294),t(3905));const i={sidebar_position:9010,slug:"2021-11-07",title:"OpenCV Crash Course Part II",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},o=void 0,c={unversionedId:"IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/index",id:"IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/index",title:"OpenCV Crash Course Part II",description:"Shenzhen, China",source:"@site/docs/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii",slug:"/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/2021-11-07",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/2021-11-07",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/index.md",tags:[{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Python",permalink:"/docs/tags/python"},{label:"OpenCV",permalink:"/docs/tags/open-cv"}],version:"current",sidebarPosition:9010,frontMatter:{sidebar_position:9010,slug:"2021-11-07",title:"OpenCV Crash Course Part II",authors:"mpolinowski",tags:["Machine Learning","Python","OpenCV"]},sidebar:"tutorialSidebar",previous:{title:"activate_webcam",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-08--tensorflow-crash-course-part-i/activate_webcam"},next:{title:"OpenCV Crash Course Part I",permalink:"/docs/IoT-and-Machine-Learning/ML/2021-11-06--opencv-crash-course-part-i/2021-11-06"}},s={},l=[{value:"Perspective Transformation",id:"perspective-transformation",level:2},{value:"Color Detection",id:"color-detection",level:2},{value:"Contours and Shape Detection",id:"contours-and-shape-detection",level:2},{value:"Face Detection",id:"face-detection",level:2},{value:"License Plate Detection",id:"license-plate-detection",level:2}],p={toc:l};function d(e){let{components:a,...i}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,i,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Shenzhen, China",src:t(90716).Z,width:"1500",height:"667"})),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.computervision.zone"},"Murtaza's Workshop - Robotics and AI"),": We are going learn everything required to get started with OpenCV in Python. We will be using Python since it is one of the most popular programming languages. And it  has opened numerous job opportunities in various sectors. We will start from the  installation process right up to creating exciting projects such as detecting colors , shapes humans and even vehicle number plates. So If you are a beginner  don't worry this course is for you. We will skip all the boring theory stuff and focus on the practical implementation. So you can get the computer vision skill set you have always wanted in your CV. By the end of the course you will become familiar with the core principle of opencv and apply different techniques to solve real world problems using computer vision."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-06--opencv-crash-course-part-i/2021-11-06"},"OpenCV Crash Course Part I")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/2021-11-07"},"OpenCV Crash Course Part II"))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#perspective-transformation"},"Perspective Transformation")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#color-detection"},"Color Detection")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#contours-and-shape-detection"},"Contours and Shape Detection")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#face-detection"},"Face Detection")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#license-plate-detection"},"License Plate Detection"))),(0,r.kt)("h2",{id:"perspective-transformation"},"Perspective Transformation"),(0,r.kt)("p",null,"Get the corner points of the object you need to de-warp by hovering over them in MS Paint or Gimp:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(74443).Z,width:"1331",height:"829"})),(0,r.kt)("p",null,"Write down the coordinates for the upper-left, upper-right, lower-left and lower-right point in that order."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"img = cv2.imread('resources/sign.jpg')\n\nwidth, height = 250, 350\n# upper-left, upper-right, lower-left and lower-right point\npts1 = np.float32([[920, 227], [1216, 244], [873, 780], [1182, 809]])\npts2 = np.float32([[0, 0], [width, 0], [0, height], [width, height]])\n\nmatrix = cv2.getPerspectiveTransform(pts1, pts2)\n\nimgOutput = cv2.warpPerspective(img, matrix, (width,height))\n\n\ncv2.imshow('Image', imgOutput)\ncv2.waitKey(10000)\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(86715).Z,width:"369",height:"424"})),(0,r.kt)("h2",{id:"color-detection"},"Color Detection"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from time import time, sleep\npath = 'resources/songhua-river.jpg'\n# Create a hue slider that helps us\n# find the correct colour to select\ndef empty(a):\n    pass\ncv2.namedWindow('TrackBars') # Create the Window\ncv2.resizeWindow('TrackBars', 640, 240) # Give it a size\ncv2.createTrackbar('Hue Min', 'TrackBars', 0, 179, empty) # Add a slider for min Hue 0-179\ncv2.createTrackbar('Hue Max', 'TrackBars', 179, 179, empty) # Add a slider for max Hue 179\ncv2.createTrackbar('Sat Min', 'TrackBars', 0, 255, empty) # Add a slider for min Saturation 0-255\ncv2.createTrackbar('Sat Max', 'TrackBars', 255, 255, empty) # Add a slider for max Saturation 255\ncv2.createTrackbar('Val Min', 'TrackBars', 0, 255, empty) # Add a slider for min Value 0-255\ncv2.createTrackbar('Val Max', 'TrackBars', 255, 255, empty) # Add a slider for max Value 255\n\nwhile True: # Run loop to continuously update from trackbars\n    img = cv2.imread(path)\n    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h_min = cv2.getTrackbarPos('Hue Min', 'TrackBars')\n    h_max = cv2.getTrackbarPos('Hue Max', 'TrackBars')\n    s_min = cv2.getTrackbarPos('Sat Min', 'TrackBars')\n    s_max = cv2.getTrackbarPos('Sat Max', 'TrackBars')\n    v_min = cv2.getTrackbarPos('Val Min', 'TrackBars')\n    v_max = cv2.getTrackbarPos('Val Max', 'TrackBars')\n\n    print(h_min, h_max, s_min, s_max, v_min, v_max)\n\n    lower = np.array([h_min, s_min, v_min])\n    upper = np.array([h_max, s_max, v_max])\n    mask = cv2.inRange(imgHSV, lower, upper)\n\n    cv2.imshow('Original', img)\n    cv2.imshow('HSV', imgHSV)\n    cv2.imshow('Mask', mask)\n    cv2.waitKey(1000)\n")),(0,r.kt)("p",null,"Adjust the sliders until only the colour you wanted is selected (white) and the rest of the image is masked:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(45473).Z,width:"744",height:"876"})),(0,r.kt)("p",null,"Here the optimal settings are ",(0,r.kt)("inlineCode",{parentName:"p"},"0 179 107 255 180 255")," which we can use as default values for out TrackBars. Now all we have to do is to apply the layer mask to our image:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from time import time, sleep\npath = 'resources/songhua-river.jpg'\n# Create a hue slider that helps us\n# find the correct colour to select\ndef empty(a):\n    pass\ncv2.namedWindow('TrackBars') # Create the Window\ncv2.resizeWindow('TrackBars', 640, 240) # Give it a size\ncv2.createTrackbar('Hue Min', 'TrackBars', 0, 179, empty) # Add a slider for min Hue 0-179\ncv2.createTrackbar('Hue Max', 'TrackBars', 179, 179, empty) # Add a slider for max Hue 179\ncv2.createTrackbar('Sat Min', 'TrackBars', 107, 255, empty) # Add a slider for min Saturation 0-255\ncv2.createTrackbar('Sat Max', 'TrackBars', 255, 255, empty) # Add a slider for max Saturation 255\ncv2.createTrackbar('Val Min', 'TrackBars', 180, 255, empty) # Add a slider for min Value 0-255\ncv2.createTrackbar('Val Max', 'TrackBars', 255, 255, empty) # Add a slider for max Value 255\n\nwhile True: # Run loop to continuously update from trackbars\n    img = cv2.imread(path)\n    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h_min = cv2.getTrackbarPos('Hue Min', 'TrackBars')\n    h_max = cv2.getTrackbarPos('Hue Max', 'TrackBars')\n    s_min = cv2.getTrackbarPos('Sat Min', 'TrackBars')\n    s_max = cv2.getTrackbarPos('Sat Max', 'TrackBars')\n    v_min = cv2.getTrackbarPos('Val Min', 'TrackBars')\n    v_max = cv2.getTrackbarPos('Val Max', 'TrackBars')\n\n    print(h_min, h_max, s_min, s_max, v_min, v_max)\n\n    lower = np.array([h_min, s_min, v_min])\n    upper = np.array([h_max, s_max, v_max])\n    mask = cv2.inRange(imgHSV, lower, upper) # Create a selection mask based on thresholds\n    imgSelection = cv2.bitwise_and(img, img, mask=mask) # Apply layer mask to image\n\n    imgHor = np.hstack((img, imgSelection))\n\n    cv2.imshow('Original & Selection', imgHor)\n    cv2.imshow('Mask', mask)\n    cv2.waitKey(1000)\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(97641).Z,width:"1499",height:"579"})),(0,r.kt)("h2",{id:"contours-and-shape-detection"},"Contours and Shape Detection"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def getContours(img): # Retrieve contours from detected shapes\n    contours, hierachy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n    for cnt in contours:\n        area = cv2.contourArea(cnt) # Get areas for all contours\n        # print(area) # Print calculated areas\n        if area > 400: # Set threshold to exclude noise\n            cv2.drawContours(imgBlack, cnt, -1, (255, 0, 0), 1)  # Draw those areas onto the image\n            peri = cv2.arcLength(cnt, True) # Get contour perimeter\n            approx = cv2.approxPolyDP(cnt, 0.02*peri, True) # Approximate polygonal curve\n            # print(len(approx)) # Print the number corner points of each contour\n            objCorners = len(approx)\n            x, y, w, h = cv2.boundingRect(approx) # Get coordinates from curve\n\n            if objCorners == 3:\n                objectType = "Triangle" # Define object based on corner count\n\n            elif objCorners == 4:\n                aspectRatio = w/float(h) # Check if w/h=1 => square\n                if 0.95 < aspectRatio < 1.05: objectType = "Square"\n                else: objectType = "Rectangle"\n\n            elif objCorners == 5:\n                objectType = "Pentagon"\n\n            elif objCorners == 6:\n                objectType = "Hexagon"\n\n            elif objCorners == 7:\n                objectType = "Heptagon"\n\n            elif objCorners > 7:\n                objectType = "Circle?"\n\n            else: objectType = "Unknown"\n\n            cv2.rectangle(imgBlack, (x, y), (x+w, y+h), (0, 0, 255, 1)) # Print bounding box\n            cv2.putText(imgBlack, objectType,\n                        (x+(w//2)-10, y+(h//2)-10), # Put objectType in Center\n                        cv2.QT_FONT_NORMAL, 0.5, (255, 255, 0), 1)\n\n# Contours and Shape detection\npath = \'resources/objects_dark.png\'\n# path = \'resources/objects_light.png\'\nimg = cv2.imread(path)\nimgBlack = np.zeros_like(img)\n# imgWhite = np.ones_like(img)\n\nimgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimgBlur = cv2.GaussianBlur(imgGray, (7, 7), 1)\nimgEdge = cv2.Canny(imgBlur, 50, 50)\n\ngetContours(imgEdge)\n\ncv2.imshow("Shape", imgBlack)\ncv2.imshow("Original", img)\ncv2.waitKey(15000)\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(85529).Z,width:"736",height:"395"})),(0,r.kt)("h2",{id:"face-detection"},"Face Detection"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html"},"Object Detection using Haar feature-based cascade classifiers"),' is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, "Rapid Object Detection using a Boosted Cascade of Simple Features" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.'),(0,r.kt)("p",null,"Here we will work with face detection. Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier."),(0,r.kt)("p",null,"OpenCV provides a training method (see ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4/dc/d88/tutorial_traincascade.html"},"Cascade Classifier Training"),") or pretrained models, that can be read using the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html#a1a5884c8cc749422f9eb77c2471958bc"},"cv::CascadeClassifier::load")," method. The pretrained models are located in the data folder in the OpenCV installation or can be found ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/opencv/opencv/tree/3.4/data"},"here"),"."),(0,r.kt)("p",null,"The following code example will use pretrained Haar cascade models to detect faces and eyes in an image. First, a ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html"},"cv::CascadeClassifier")," is created and the necessary XML file is loaded using the cv::CascadeClassifier::load method. Afterwards, the detection is done using the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html#aaf8181cb63968136476ec4204ffca498"},"cv::CascadeClassifier::detectMultiScale")," method, which returns boundary rectangles for the detected faces or eyes."),(0,r.kt)("p",null,"Download the pre-trained you want to use from ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/opencv/opencv/tree/3.4/data"},"Github"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd resources\nwget https://raw.githubusercontent.com/opencv/opencv/3.4/data/haarcascades/haarcascade_frontalface_default.xml\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import cv2\nimport numpy as np\n\n# Face Detection\nfaceCascade = cv2.CascadeClassifier('resources/haarcascade_frontalface_default.xml')\nimagePath = 'resources/hongkong-metro.png'\nimg = cv2.imread(imagePath)\nimgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Create grayscale image\n\nfaces = faceCascade.detectMultiScale(imgGray, 1.1, 4) # Detect all faces in image\n\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n\ncv2.imshow(\"Face Detection\", img)\ncv2.waitKey(5000)\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(27066).Z,width:"991",height:"527"})),(0,r.kt)("h2",{id:"license-plate-detection"},"License Plate Detection"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd resources\nwget https://raw.githubusercontent.com/opencv/opencv/3.4/data/haarcascades/haarcascade_russian_plate_number.xml\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import cv2\n\n# License Plate Detection\nlpCascade = cv2.CascadeClassifier('resources/haarcascade_russian_plate_number.xml')\nimagePath = 'resources/HK_Taxi.jpg'\nimg = cv2.imread(imagePath)\nimgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Create grayscale image\n\nplates = lpCascade.detectMultiScale(imgGray, 1.1, 4) # Detect all faces in image\n\nfor (x, y, w, h) in plates:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n\ncv2.imshow(\"License Plate Detection\", img)\ncv2.waitKey(5000)\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"OpenCV Python",src:t(90442).Z,width:"1000",height:"666"})))}d.isMDXComponent=!0},74443:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_01-c8c5f3c629057ea5197eeef501b2040b.png"},86715:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_02-9eacde498ad8552ba90db9877979b644.png"},45473:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_03-0362fdcc25e013735c0643de7b8b268a.png"},97641:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_04-5e646de474129d8004ead40d76442ff5.png"},85529:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_05-99ca3318bc199cfe95f58e3239a6bc57.png"},27066:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_06-147cdc1af2dea431423cad24c7a8c32c.png"},90442:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/opencv-python_07-67ad2728a8c231f9fa8af559c27ccddb.png"},90716:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-cbaf0ac1e969d7e110b992b34d57b91c.jpg"}}]);