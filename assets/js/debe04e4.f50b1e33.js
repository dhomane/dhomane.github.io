"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[63813],{3905:(e,n,r)=>{r.d(n,{Zo:()=>d,kt:()=>m});var t=r(67294);function o(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function a(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function i(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?a(Object(r),!0).forEach((function(n){o(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function l(e,n){if(null==e)return{};var r,t,o=function(e,n){if(null==e)return{};var r,t,o={},a=Object.keys(e);for(t=0;t<a.length;t++)r=a[t],n.indexOf(r)>=0||(o[r]=e[r]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(t=0;t<a.length;t++)r=a[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var s=t.createContext({}),p=function(e){var n=t.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):i(i({},n),e)),r},d=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var r=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(r),m=o,g=u["".concat(s,".").concat(m)]||u[m]||c[m]||a;return r?t.createElement(g,i(i({ref:n},d),{},{components:r})):t.createElement(g,i({ref:n},d))}));function m(e,n){var r=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=r.length,i=new Array(a);i[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var p=2;p<a;p++)i[p]=r[p];return t.createElement.apply(null,i)}return t.createElement.apply(null,r)}u.displayName="MDXCreateElement"},81096:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var t=r(87462),o=(r(67294),r(3905));const a={sidebar_position:4830,slug:"2023-01-01",title:"Tensorflow Docker Model Server",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Use Tensorflow Serving to Provision your ML Model"},i=void 0,l={unversionedId:"IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server/index",id:"IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server/index",title:"Tensorflow Docker Model Server",description:"Use Tensorflow Serving to Provision your ML Model",source:"@site/docs/IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server/index.md",sourceDirName:"IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server",slug:"/IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server/2023-01-01",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server/2023-01-01",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/AIOps/2023-01-01-tf-model-server/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"}],version:"current",sidebarPosition:4830,frontMatter:{sidebar_position:4830,slug:"2023-01-01",title:"Tensorflow Docker Model Server",authors:"mpolinowski",tags:["Python","Machine Learning"],description:"Use Tensorflow Serving to Provision your ML Model"},sidebar:"tutorialSidebar",previous:{title:"Tensorflow Serving REST API",permalink:"/docs/IoT-and-Machine-Learning/AIOps/2023-01-02-tf-serve-own-models/2023-01-02"},next:{title:"Home Automation",permalink:"/docs/category/home-automation"}},s={},p=[{value:"TensorFlow Serving with Docker",id:"tensorflow-serving-with-docker",level:2},{value:"Serving with Docker using your GPU",id:"serving-with-docker-using-your-gpu",level:3}],d={toc:p};function c(e){let{components:n,...a}=e;return(0,o.kt)("wrapper",(0,t.Z)({},d,a,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Guangzhou, China",src:r(86493).Z,width:"1500",height:"662"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#tensorflow-serving-with-docker"},"TensorFlow Serving with Docker"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#serving-with-docker-using-your-gpu"},"Serving with Docker using your GPU"))))),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/tfx/guide/serving"},"TensorFlow Serving")," is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data."),(0,o.kt)("h2",{id:"tensorflow-serving-with-docker"},"TensorFlow Serving with Docker"),(0,o.kt)("p",null,"The serving server can be pulled from ",(0,o.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/tensorflow/serving/tags/"},"Docker Hub")," and is available with and without ",(0,o.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2022-11-27-containerized-deep-learning/2022-11-27"},"GPU support")," - pick the one you need:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker pull tensorflow/serving:latest-gpu\ndocker pull tensorflow/serving:latest\n")),(0,o.kt)("p",null,"The serving images (both CPU and GPU) have the following properties:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Port ",(0,o.kt)("inlineCode",{parentName:"li"},"8500")," exposed for ",(0,o.kt)("em",{parentName:"li"},"gRPC")),(0,o.kt)("li",{parentName:"ul"},"Port ",(0,o.kt)("inlineCode",{parentName:"li"},"8501")," exposed for the ",(0,o.kt)("em",{parentName:"li"},"REST API")),(0,o.kt)("li",{parentName:"ul"},"Optional environment variable ",(0,o.kt)("strong",{parentName:"li"},"MODEL_NAME")," (defaults to ",(0,o.kt)("inlineCode",{parentName:"li"},"mode"),"l)"),(0,o.kt)("li",{parentName:"ul"},"Optional environment variable ",(0,o.kt)("strong",{parentName:"li"},"MODEL_BASE_PATH")," (defaults to ",(0,o.kt)("inlineCode",{parentName:"li"},"/models"),")")),(0,o.kt)("p",null,"The ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/tensorflow/serving"},"Tensorflow Serving Repository")," already provides a ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables/tensorflow/testdata"},"few models for testing")," that we can use:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/tensorflow/serving\ncd ./serving\n")),(0,o.kt)("p",null,"Now we can run the docker container and mount one of those models:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'docker run -t --rm -p 8501:8501 \\\n    -v "$(pwd)/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu:/models/half_plus_two" \\\n    -e MODEL_NAME=half_plus_two \\\n    tensorflow/serving:latest-gpu\n')),(0,o.kt)("p",null,"This will run the docker container, launch the TensorFlow Serving Model Server, bind the REST API port 8501, and map our desired model from our host to where models are expected in the container. We also pass the name of the model as an environment variable, which will be important when we query the model."),(0,o.kt)("p",null,"Even though I already installed ",(0,o.kt)("a",{parentName:"p",href:"/docs/IoT-and-Machine-Learning/ML/2022-11-27-containerized-deep-learning/2022-11-27"},"Nvidias GPU support for Docker")," I am still getting an error message here:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("inlineCode",{parentName:"p"},"Failed to start server. Error: UNKNOWN: 1 servable(s) did not become available: {{{name: half_plus_two version: 123} due to error: INVALID_ARGUMENT: Cannot assign a device for operation a: {{node a}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device"))),(0,o.kt)("h3",{id:"serving-with-docker-using-your-gpu"},"Serving with Docker using your GPU"),(0,o.kt)("p",null,"Before serving with a GPU, in addition to installing Docker, you will need:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"NVIDIA drivers")," for your system"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"nvidia-docker"),": ",(0,o.kt)("a",{parentName:"li",href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker"},"can follow the installation instructions here"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"nvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.60.11    Driver Version: 525.60.11    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n|  0%   56C    P0    29W / 130W |    780MiB /  6144MiB |      4%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n")),(0,o.kt)("p",null,"Rerunning the GPU image - it seems all that I was missing was the ",(0,o.kt)("strong",{parentName:"p"},"GPU flag")," ",(0,o.kt)("inlineCode",{parentName:"p"},"--gpus all"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker run --gpus all -p 8501:8501 \\\n--mount type=bind,source=$(pwd)/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu,target=/models/half_plus_two \\\n-e MODEL_NAME=half_plus_two -t tensorflow/serving:latest-gpu &\n")),(0,o.kt)("p",null,"And this time we have lift-off:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\nI tensorflow_serving/model_servers/server.cc:118] Using InsecureServerCredentials\nI tensorflow_serving/model_servers/server.cc:383] Profiler service is enabled\nI tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...\nI tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...\n")),(0,o.kt)("p",null,"The Half Plus Two model generates ",(0,o.kt)("inlineCode",{parentName:"p"},"0.5 * x + 2")," for the values of x we provide for prediction. This model will now have ops bound to the GPU device, and will not run on the CPU. We can now make a prediction using the Tensorflow Serving REST API. When sending the values for ",(0,o.kt)("inlineCode",{parentName:"p"},"x")," 1,2 and 5 I expect a returned prediction of 2.5, 3.5 and 4.5:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n  -X POST http://localhost:8501/v1/models/half_plus_two:predict\n")),(0,o.kt)("p",null,"Taaadaa:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "predictions": [2.5, 3.0, 4.5]\n}\n')))}c.isMDXComponent=!0},86493:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-296769d73822f07b0ac5dc952f56bfa1.jpg"}}]);