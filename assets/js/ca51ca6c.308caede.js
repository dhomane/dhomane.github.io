"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[97819],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||o;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},56936:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const o={sidebar_position:9030,slug:"2022-10-07",title:"Python - Build an Elasticsearch Index for your Docusaurus Blog",authors:"mpolinowski",tags:["Python"],description:"I was able to bring both together by pulling text from an URL, process the content and write it into an Elasticsearch JSON Document. Now I want to automate this process a little by using a pages sitemap."},i=void 0,s={unversionedId:"Development/Python/2022-10-07-python-docusaurus-elasticsearch/index",id:"Development/Python/2022-10-07-python-docusaurus-elasticsearch/index",title:"Python - Build an Elasticsearch Index for your Docusaurus Blog",description:"I was able to bring both together by pulling text from an URL, process the content and write it into an Elasticsearch JSON Document. Now I want to automate this process a little by using a pages sitemap.",source:"@site/docs/Development/Python/2022-10-07-python-docusaurus-elasticsearch/index.md",sourceDirName:"Development/Python/2022-10-07-python-docusaurus-elasticsearch",slug:"/Development/Python/2022-10-07-python-docusaurus-elasticsearch/2022-10-07",permalink:"/docs/Development/Python/2022-10-07-python-docusaurus-elasticsearch/2022-10-07",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Development/Python/2022-10-07-python-docusaurus-elasticsearch/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"}],version:"current",sidebarPosition:9030,frontMatter:{sidebar_position:9030,slug:"2022-10-07",title:"Python - Build an Elasticsearch Index for your Docusaurus Blog",authors:"mpolinowski",tags:["Python"],description:"I was able to bring both together by pulling text from an URL, process the content and write it into an Elasticsearch JSON Document. Now I want to automate this process a little by using a pages sitemap."},sidebar:"tutorialSidebar",previous:{title:"Python - Deploying a Web App with Flask",permalink:"/docs/Development/Python/2022-10-08-python-flask-app/2022-10-08"},next:{title:"Python - Minify Text for Elasticsearch",permalink:"/docs/Development/Python/2022-10-06-python-minify-text/2022-10-06"}},l={},p=[{value:"Elasticsearch",id:"elasticsearch",level:2},{value:"Mapping",id:"mapping",level:3},{value:"Automate with Python",id:"automate-with-python",level:2},{value:"XML Sitemap Parsing",id:"xml-sitemap-parsing",level:3},{value:"Generate Elasticsearch Docs",id:"generate-elasticsearch-docs",level:3}],c={toc:p};function d(e){let{components:t,...o}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Sham Sui Po, Hong Kong",src:n(4633).Z,width:"1500",height:"548"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#elasticsearch"},"Elasticsearch"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#mapping"},"Mapping")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#automate-with-python"},"Automate with Python"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#xml-sitemap-parsing"},"XML Sitemap Parsing")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#generate-elasticsearch-docs"},"Generate Elasticsearch Docs"))))),(0,r.kt)("p",null,"I looked into retrieving text from webpages ",(0,r.kt)("a",{parentName:"p",href:"/docs/Development/Python/2022-06-27-python-web-scraping/2022-06-27"},"using Beautiful Soup"),". And continued with looking into ",(0,r.kt)("a",{parentName:"p",href:"/docs/Development/Python/2022-10-05-python-text-processing/2022-10-05"},"processing text"),". I was able to bring both together by pulling text from an URL, process the content and write it into an ",(0,r.kt)("a",{parentName:"p",href:"/docs/Development/Python/2022-10-06-python-minify-text/2022-10-06"},"Elasticsearch JSON Document"),". Now I want to automate this process a little by using a pages sitemap."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/docusaurus_elasticsearch"},"Gihub Repo")),(0,r.kt)("h2",{id:"elasticsearch"},"Elasticsearch"),(0,r.kt)("p",null,"There are many different ways to automatically generate Elasticsearch entries. But I always had the problem that the Elasticsearch entry should contain a few information that are not part of the page itself. Since I only rarely add new content I have been doing it by had so far."),(0,r.kt)("h3",{id:"mapping"},"Mapping"),(0,r.kt)("p",null,"First you create a mapping that suits your content. This is optional in NoSQL - but it allows you to define analyzer that Elasticsearch should use. Also, if you want to filter your Search results you have to set those fields to be treated as keywords and stored unprocessed. The example mapping for this blog is:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'PUT /dev_2022_10_09\n{\n  "settings": {\n    "analysis": {\n      "analyzer": {\n        "custom_analyzer": {\n          "type": "custom",\n          "char_filter": [\n            "symbol",\n            "html_strip"\n          ],\n          "tokenizer": "punctuation",\n          "filter": [\n            "lowercase",\n            "word_delimiter",\n            "english_stop",\n            "english_stemmer"\n          ]\n        }\n      },\n      "filter": {\n        "english_stop": {\n          "type": "stop",\n          "stopwords": "_english_ "\n        },\n        "english_stemmer": {\n          "type": "stemmer",\n          "language": "english"\n        }\n      },\n      "tokenizer": {\n        "punctuation": {\n          "type": "pattern",\n          "pattern": "[.,!?&=_:;\']"\n        }\n      },\n      "char_filter": {\n        "symbol": {\n          "type": "mapping",\n          "mappings": [\n              "& => and",\n              ":) => happy",\n              ":( => unhappy",\n              "+ => plus"\n            ]\n        }\n      }\n    }\n  },\n  "mappings": {\n    "properties": {\n      "title": {\n        "type": "text",\n        "analyzer": "custom_analyzer",\n        "index": "true"\n      },\n      "type": {\n        "type": "text",\n        "index": "true",\n        "fields": {\n          "raw": {\n            "type": "keyword"\n          }\n        }\n      },\n      "date": {\n        "type": "text",\n        "index": "true",\n        "fields": {\n          "raw": {\n            "type": "keyword"\n          }\n        }\n      },\n      "description": {\n        "type": "text",\n        "analyzer": "custom_analyzer",\n        "index": "true"\n      },\n      "link": {\n        "type": "text",\n        "index": "false"\n      },\n      "chapter": {\n        "type": "text",\n        "index": "true",\n        "fields": {\n          "raw": {\n            "type": "keyword"\n          }\n        }\n      },\n      "tags": {\n        "type": "text",\n        "index": "true",\n        "fields": {\n          "raw": {\n            "type": "keyword"\n          }\n        }\n      },\n      "imagesquare": {\n        "type": "text",\n        "index": "false"\n      },\n      "abstract": {\n        "type": "text",\n        "analyzer": "custom_analyzer",\n        "index": "true"\n      },\n      "short": {\n        "type": "text",\n        "analyzer": "custom_analyzer",\n        "index": "true"\n      }\n    }\n  }\n}\n')),(0,r.kt)("p",null,"Just modify it to your needs and feed it into ",(0,r.kt)("strong",{parentName:"p"},"Kibana")," to set up your Elasticsearch index:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Python - Build an Elasticsearch Index for your Docusaurus Blog",src:n(80052).Z,width:"1201",height:"427"})),(0,r.kt)("p",null,"Every article now needs to be added following the structure defined in your mapping. For example the JSON doc for the ",(0,r.kt)("a",{parentName:"p",href:"/docs/Development/Go/2022-10-05-go-reducing-binary-size/2022-10-05"},"Go build!")," looks like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'PUT /dev_2022_10_09/_doc/go-reducing-binary-size\n{\n    "title": "Go Build!",\n    "type": "Note",\n    "descripton": "Go Build! Go Build! Cross-compiling with Go Golang on a Diet UPX In the previous post I build an NTS Client from source. I now need to compile it for an ARM system and reduce the file size of the generated binary.Cross-compiling with GoThe original binary was compiled on a x86-64 Linux system and with default settings resulted in the following file:file ntsclientntsclient: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, Go BuildID=jk1ySsUE9xCbQQElzPBj/mKJ-lNqe4rCLOALwJ0Uh/lg1ziBDAYw1FdjS_SnD_/6JHvJ15niw3mnL010nFR, with debug_info, not strippedTo use the binary on an ARM system we need to use the amazing cross-compiling capabilities of Go. E.g. to create the arm64 version of the file I can run:env GOOS=linux GOARCH=arm64 go build -o ntsclient_arm64The resulting binary can be used on an 64bit ARM system:file ntsclient_arm64ntsclient_arm64: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, Go BuildID=MR5GvnV5S4XWxDENNLTE/ZZ2k_Dx5K6CdKRTzWlK5/Zbl1ahI9CUA-f28opmhH/0Zh0CFmxrhuWRZ-kOiMt, with debug_info, not strippedYou can check the supported combinations of GOOS and GOARCH with:go tool dist listaix/ppc64 | android/386 | android/amd64 | android/arm | android/arm64 | darwin/amd64 | darwin/arm64 | dragonfly/amd64 | freebsd/386 | freebsd/amd64 | freebsd/arm | freebsd/arm64 | illumos/amd64 | ios/amd64 | ios/arm64 | js/wasm | linux/386 | linux/amd64 | linux/arm | linux/arm64 | linux/loong64 | linux/mips | linux/mips64 | linux/mips64le | linux/mipsle | linux/ppc64 | linux/ppc64le | linux/riscv64 | linux/s390x | netbsd/386 | netbsd/amd64 | netbsd/arm | netbsd/arm64 | openbsd/386 | openbsd/amd64 | openbsd/arm | openbsd/arm64 | openbsd/mips64 | plan9/386 | plan9/amd64 | plan9/arm | solaris/amd64 | windows/386 | windows/amd64 | windows/arm | windows/arm64We can automate a multi-architecture build with a script build.sh: usr/bin/basharchs=(amd64 arm arm64)for arch in ${archs[@]}do env GOOS=linux GOARCH=${arch} go build -o prepnode_${arch}doneGolang on a DietThe resulting files - compared to a similar C program - are generally huge. In the NTS client example I end up with 7.4 - 7.7 MB files:7965759 Oct 5 15:37 ntsclient_amd647665133 Oct 5 15:37 ntsclient_arm7635040 Oct 5 15:24 ntsclient_arm64The following build flags can help us reducing the binary size:ldflags-s omits the symbol table and debug information-w omits DWARF debugging information.So lets update the build script accordingly: usr/bin/basharchs=(amd64 arm arm64)for arch in ${archs[@]}do env GOOS=linux GOARCH=${arch} go build -ldflags -s -w -o prepnode_${arch}doneNow we are down to 5.0 - 5.2 MB:5459968 Oct 5 16:55 ntsclient_amd645242880 Oct 5 16:55 ntsclient_arm5242880 Oct 5 16:55 ntsclient_arm64UPXUPX is a free, secure, portable, extendable, high-performance executable packer for several executable formats. You can install the latest version from Github or use your package manager:sudo pacman -S utxupx --help Ultimate Packer for eXecutables Copyright (C) 1996 - 2020UPX git-d7ba31+ Markus Oberhumer, Laszlo Molnar & John Reiser Jan 23rd 2020Usage: upx [-123456789dlthVL] [-qvfk] [-o file] file..Commands: -1 compress faster -9 compress better --best compress best (can be slow for big files) -d decompress -l list compressed file -t test compressed file -V display version number -h give this help -L display software licenseOptions: -q be quiet -v be verbose -oFILE write output to FILE -f force compression of suspicious files --no-color, --mono, --color, --no-progress change lookCompression tuning options: --brute try all available compression methods & filters [slow] --ultra-brute try even more compression variants [very slow]So to get the maximum amount of compress let s try Ultra Brute:upx --ultra-brute -ontsclient_upx_arm ntsclient_arm File size Ratio Format Name-------------------- ------ ----------- -----------5242880 -> 1421184 27.11% linux/arm ntsclient_upx_arm So we went from around 7 MB down to 1.4 MB. Tags: Go",\n    "link": "/docs/Development/Go/2022-10-05-go-reducing-binary-size/2022-10-05",\n    "chapter": "Dev Notes",\n    "date": "2022-10-05",\n    "tags": [\n        "Go",\n        "LINUX"\n    ],\n    "imagesquare": "/img/search/go.png",\n    "abstract": "Cross-compile in Go and reduce the binary size of your Go program."\n}\n')),(0,r.kt)("p",null,"And can be added to your index using Kibana and the be searched in your ",(0,r.kt)("a",{parentName:"p",href:"/Search"},"React Frontend"),":"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Python - Build an Elasticsearch Index for your Docusaurus Blog",src:n(69445).Z,width:"1202",height:"450"})),(0,r.kt)("h2",{id:"automate-with-python"},"Automate with Python"),(0,r.kt)("p",null,"In the previous step I used ",(0,r.kt)("a",{parentName:"p",href:"/docs/Development/Python/2022-10-06-python-minify-text/2022-10-06"},"Python to create the Elasticsearch Document for a single page")," - ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/python-text-processing/blob/main/jsonfyOnlineText.py"},"Github"),"."),(0,r.kt)("p",null,"Now I want to use this approach to pull the Blog's ",(0,r.kt)("strong",{parentName:"p"},"XML Sitemap"),", loop over all URLs and create a mostly filled template for each page. There are still a few fields that I cannot fill based on the pages content. But I will just add some placeholder text and copy&paste the rest in by hand (In the future I will see to it that all of those information will be embedded in all articles ~ but this is already a big improvement over doing all of it by hand)."),(0,r.kt)("p",null,"Ok, I will break this project up into 2 steps. First, I will pull the sitemap and write those URLs into a text file. The second part will then be to loop over this file and generate the JSON objects. The advantage of breaking this up into two scripts is that I will be able to feed manually created text files to the second script and append new articles later on."),(0,r.kt)("h3",{id:"xml-sitemap-parsing"},"XML Sitemap Parsing"),(0,r.kt)("p",null,"I can use ",(0,r.kt)("inlineCode",{parentName:"p"},"lxml")," to parse the XML Sitemap in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/docusaurus_elasticsearch/blob/master/parse_sitemap.py"},"parse_sitemap.py"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from lxml import etree\nimport requests\n\nsideLinks = {}\n\nr = requests.get(\"https://mpolinowski.github.io/sitemap.xml\")\nroot = etree.fromstring(r.content)\n\nprint(\"INFO :: {0} pages imported from sitemap.\".format(len(root)))\n\nfor sitemap in root:\n    children = sitemap.getchildren()\n    sideLinks[children[0].text] = children[1].text\n\n# write to file\n\nwith open('pages/sideLinks.txt', 'w') as file:\n    file.writelines('\\n'.join(sideLinks))\n")),(0,r.kt)("p",null,"This will generate a file ",(0,r.kt)("inlineCode",{parentName:"p"},"sideLinks.txt")," with the extracted URLs. ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/docusaurus_elasticsearch/blob/master/pages/sideLinks.txt"},"One URL per line"),"."),(0,r.kt)("h3",{id:"generate-elasticsearch-docs"},"Generate Elasticsearch Docs"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/docusaurus_elasticsearch/blob/master/write_object.py"},"write_object.py")," now loops over every line of the generated text file and runs ",(0,r.kt)("a",{parentName:"p",href:"/docs/Development/Python/2022-10-06-python-minify-text/2022-10-06"},"the code to generate the JSON object"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'import requests\nimport re\nfrom bs4 import BeautifulSoup\n\n# add some infos that are not on the page\n\ncamera_series = \'["PLACEHOLDER"]\'\narticle_type = \'PLACEHOLDER\'\nchapter = \'PLACEHOLDER\'\ntags = \'["PLACEHOLDER"]\'\nimage = \'PLACEHOLDER\'\nimagesquare = \'PLACEHOLDER\'\n\n\n# read in list of page urls\n\npages = open(\'pages/sideLinkscopy.txt\', \'r\')\n\n# loop over urls and request content\n\nfor line in pages:\n    page = line.split()[0]\n    # print(page[0])\n    # exit()\n    # use page url to fetch/parse content\n    response = requests.get(page)\n    html = response.text\n    soup = BeautifulSoup(html, \'html.parser\')\n\n    # find in content\n\n    ## get article title from meta tag\n    article_title = (soup.find("meta", property="og:title"))["content"]\n    ## get article description from meta tag\n    article_description = (soup.find("meta", property="og:description"))["content"]\n    ## get article description from meta tag\n    article_content = soup.find(\'main\', attrs={\'class\': \'docMainContainer_gTbr\'}).text\n    ## replace quotation marks\n    jsonfied_content = article_content.replace(\'"\', \' \')\n    ## strip multiple-space character\n    single_space = re.sub(\'\\s+\',\' \',jsonfied_content)\n\n    # create json object from results\n\n    json_template = """{\n        "title": "ARTICLE_TITLE",\n        "series": ARTICLE_SERIES,\n        "type": "ARTICLE_TYPE",\n        "description": "ARTICLE_BODY",\n        "sublink1": "ARTICLE_URL",\n        "chapter": "ARTICLE_CHAPTER",\n        "tags": ARTICLE_TAGS,\n        "image": "ARTICLE_THUMB",\n        "imagesquare": "ARTICLE_SQUAREIMAGE",\n        "short": "ARTICLE_SHORT",\n        "abstract": "ARTICLE_ABSTRACT"\n    }"""\n\n\n    add_body = json_template.replace(\'ARTICLE_BODY\', single_space)\n    add_title = add_body.replace(\'ARTICLE_TITLE\', article_title)\n    add_series = add_title.replace(\'ARTICLE_SERIES\', camera_series)\n    add_type = add_series.replace(\'ARTICLE_TYPE\', article_type)\n    add_url = add_type.replace(\'ARTICLE_URL\', page[29:])\n    add_chapter = add_url.replace(\'ARTICLE_CHAPTER\', chapter)\n    add_tags = add_chapter.replace(\'ARTICLE_TAGS\', tags)\n    add_image = add_tags.replace(\'ARTICLE_THUMB\', image)\n    add_imagesquare = add_image.replace(\'ARTICLE_SQUAREIMAGE\', imagesquare)\n    add_short = add_imagesquare.replace(\'ARTICLE_SHORT\', article_description)\n    add_abstract = add_short.replace(\'ARTICLE_ABSTRACT\', article_description)\n\n    with open(\'pages/articles.json\', \'a\') as file:\n        file.write(add_abstract)\n\npages.close()\n')),(0,r.kt)("p",null,"This now generates the entries I need to feed into Kibana - with a few ",(0,r.kt)("inlineCode",{parentName:"p"},"PLACEHOLDER"),"'s added where I still need to add informations by hand - ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/docusaurus_elasticsearch/blob/master/pages/articles.json"},"articles.json"),"."),(0,r.kt)("p",null,"Good enough for now ~ to be improved later :)"))}d.isMDXComponent=!0},80052:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Python-Docusaurus-Elasticsearch_01-46fe29ad6b62694d2f18798005fb9108.png"},69445:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Python-Docusaurus-Elasticsearch_02-23965a3a7ad24b5b721ca2bcfa3b7a93.png"},4633:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-5f44d483789c3ce79f05418f930f5cd2.jpg"}}]);