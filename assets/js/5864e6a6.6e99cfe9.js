"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[91937],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=o,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||r;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},19117:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const r={sidebar_position:9080,slug:"2019-01-17",title:"Kubernetes and Microservices",authors:"mpolinowski",tags:["LINUX","Docker","Kubernetes"]},i=void 0,s={unversionedId:"DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices/index",id:"DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices/index",title:"Kubernetes and Microservices",description:"Sydney, Australia",source:"@site/docs/DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices/index.mdx",sourceDirName:"DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices",slug:"/DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices/2019-01-17",permalink:"/docs/DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices/2019-01-17",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/DevOps/Kubernetes/2019-01-17--kubernetes-and-microservices/index.mdx",tags:[{label:"LINUX",permalink:"/docs/tags/linux"},{label:"Docker",permalink:"/docs/tags/docker"},{label:"Kubernetes",permalink:"/docs/tags/kubernetes"}],version:"current",sidebarPosition:9080,frontMatter:{sidebar_position:9080,slug:"2019-01-17",title:"Kubernetes and Microservices",authors:"mpolinowski",tags:["LINUX","Docker","Kubernetes"]},sidebar:"tutorialSidebar",previous:{title:"Kubernetes Cluster Monitoring & Logging",permalink:"/docs/DevOps/Kubernetes/2019-01-19--kubernetes-cluster-logging/2019-01-19"},next:{title:"Creating a Kubernetes Cluster",permalink:"/docs/DevOps/Kubernetes/2019-01-15--creating-a-kubernetes-cluster/2019-01-15"}},l={},p=[{value:"Domain Driven Design and boundedContext",id:"domain-driven-design-and-boundedcontext",level:2},{value:"Deploying the Queue",id:"deploying-the-queue",level:3},{value:"Deploying the Position Simulator",id:"deploying-the-position-simulator",level:3},{value:"How to Debug a failed Deployment",id:"how-to-debug-a-failed-deployment",level:3},{value:"Deploying the Position Tracker",id:"deploying-the-position-tracker",level:3},{value:"Deploying the API Gateway",id:"deploying-the-api-gateway",level:3},{value:"Deploying the Angular Frontend",id:"deploying-the-angular-frontend",level:3},{value:"Persistence in a Kubernetes Cluster",id:"persistence-in-a-kubernetes-cluster",level:2},{value:"MongoDB Pod",id:"mongodb-pod",level:3},{value:"MongoDB Service",id:"mongodb-service",level:3},{value:"Volume Mounts",id:"volume-mounts",level:3},{value:"Using PersistentVolumeClaims",id:"using-persistentvolumeclaims",level:3},{value:"Cloud Deployment",id:"cloud-deployment",level:2}],c={toc:p};function d(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Sydney, Australia",src:n(79037).Z,width:"1933",height:"899"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#domain-driven-design-and-boundedcontext"},"Domain Driven Design and boundedContext"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#deploying-the-queue"},"Deploying the Queue")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#deploying-the-position-simulator"},"Deploying the Position Simulator")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#how-to-debug-a-failed-deployment"},"How to Debug a failed Deployment")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#deploying-the-position-tracker"},"Deploying the Position Tracker")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#deploying-the-api-gateway"},"Deploying the API Gateway")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#deploying-the-angular-frontend"},"Deploying the Angular Frontend")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#persistence-in-a-kubernetes-cluster"},"Persistence in a Kubernetes Cluster"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#mongodb-pod"},"MongoDB Pod")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#mongodb-service"},"MongoDB Service")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#volume-mounts"},"Volume Mounts")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#using-persistentvolumeclaims"},"Using PersistentVolumeClaims")))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#cloud-deployment"},"Cloud Deployment"))),(0,o.kt)("h2",{id:"domain-driven-design-and-boundedcontext"},"Domain Driven Design and boundedContext"),(0,o.kt)("p",null,"A Microservice architecture is an implementation of the ",(0,o.kt)("a",{parentName:"p",href:"https://martinfowler.com/tags/domain%20driven%20design.html"},"Domain Drive Design")," principle and ",(0,o.kt)("a",{parentName:"p",href:"https://martinfowler.com/bliki/BoundedContext.html"},"bounded Context")," is a central pattern in Domain-Driven Design. It is the focus of DDD's strategic design section which is all about dealing with large models and teams. DDD deals with large models by dividing them into different Bounded Contexts and being explicit about their interrelationships."),(0,o.kt)("p",null,"The idea behind the principle is to break architectures into small, ",(0,o.kt)("strong",{parentName:"p"},"cohesive components")," that only fullfil one job (",(0,o.kt)("strong",{parentName:"p"},"single responsibility principle"),") for one other component (",(0,o.kt)("strong",{parentName:"p"},"loosely coupled"),"). E.g. instead of having one big Integration Database that is build with a single schema and serves all parts of your monolithic application we will add a small, exclusive database to every component that needs data storage."),(0,o.kt)("p",null,"We are going to continue to build the web application we ",(0,o.kt)("a",{parentName:"p",href:"https://mpolinowski.github.io/docs/DevOps/Kubernetes/2019-01-15--creating-a-kubernetes-cluster/2019-01-15"},"started to build earlier")," - with an ActiveMQ Message broker in the backend and an Angular web frontend that will show us the position of our car fleet on a map. The architecture will consist of 5 pods:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Position Simulator -> ActiveMQ -> Position Tracker <- API Gateway <- NGINX Reverse Proxy <-> Web Browser\n")),(0,o.kt)("p",null,"The necessary Docker images can be found on ",(0,o.kt)("a",{parentName:"p",href:"https://hub.docker.com/u/richardchesterwood"},"DockerHUB")," - their corresponding source code can be found on ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/DickChesterwood/k8s-fleetman"},"Github"),"."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The ",(0,o.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/richardchesterwood/k8s-fleetman-position-simulator"},"Position Simulator")," will simulate the GPS signal from our cars, reporting in their position every 10s."),(0,o.kt)("li",{parentName:"ul"},"The Position Simulator is then send to our ",(0,o.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/richardchesterwood/k8s-fleetman-queue"},"ActiveMQ")," service we already deployed earlier."),(0,o.kt)("li",{parentName:"ul"},"This Queue server / Message Broker then forwards the information it received to the ",(0,o.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/richardchesterwood/k8s-fleetman-position-tracker"},"Position Tracker")," that is storing the information as well as doing some basic calculations on it, like estimating the average speed the car is traveling with."),(0,o.kt)("li",{parentName:"ul"},"To prevent the frontend code to directly contact this backend part of our application, we will add an ",(0,o.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/richardchesterwood/k8s-fleetman-api-gateway"},"API Gateway")," that will serve as an interface between front- and backend. This way changes in the backend will never directly affect the frontend, or vice versa - see ",(0,o.kt)("a",{parentName:"li",href:"https://microservices.io/patterns/apigateway.html"},"API Gateway Pattern"),"."),(0,o.kt)("li",{parentName:"ul"},"The final container will run our ",(0,o.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/richardchesterwood/k8s-fleetman-webapp-angular"},"Web Application Frontend")," with the help of an NGINX reverse proxy.")),(0,o.kt)("h3",{id:"deploying-the-queue"},"Deploying the Queue"),(0,o.kt)("p",null,"We want to start with a fresh cluster - if you already ",(0,o.kt)("a",{parentName:"p",href:"https://mpolinowski.github.io/docs/DevOps/Kubernetes/2019-01-15--creating-a-kubernetes-cluster/2019-01-15"},"followed the earlier steps"),", just enter the directory that contains all your configuration files (services.yaml, pods.yaml, networking-tests.yaml) and force delete everything that was build from them on your cluster:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl delete -f .\nrm networking-tests.yaml\nmv pods.yaml workloads.yaml\nnano workloads.yaml\n")),(0,o.kt)("p",null,"We are also deleting an unnecessary file from the previous step and renaming another - just if you are following along. We can now add the queue server to the ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml")," file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  # Unique key of the ReplicaSet instance\n  name: queue\nspec:\n  selector:\n    matchLabels:\n      # the ReplicaSet manages all Pods\n      # where the label = app: queue\n      app: queue\n  # only 1 Pod should exist atm - if it\n  # crashes, a new pod will be spawned.\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: queue\n    spec:\n      containers:\n      - name: queue\n        image: richardchesterwood/k8s-fleetman-queue:release1\n")),(0,o.kt)("p",null,"Secondly, we need to apply a service that exposes our Queue container, which is done in the ",(0,o.kt)("inlineCode",{parentName:"p"},"services.yaml")," file (if you still have the webapp service from the previous step - just leave it in for now):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Service\nmetadata:\n  # Unique key of the Service instance\n  name: fleetman-queue\nspec:\n  ports:\n    - name: admin-console\n      port: 8161\n      targetPort: 8161\n      # The nodePort is available from outside of the\n      # cluster when is set to NodePort. It's value has\n      # to be > 30000\n      nodePort: 30010\n    - name: endpoint\n      port: 61616\n      targetPort: 61616\n  selector:\n    # Define which pods are going to\n    # be represented by this service\n    # The service makes an network\n    # endpoint for our app\n    app: queue\n  # Setting the Service type to ClusterIP makes the\n  # service only available from inside the cluster\n  # To expose a port use NodePort instead\n  type: NodePort\n")),(0,o.kt)("p",null,"The Queue service is going to expose the port ",(0,o.kt)("strong",{parentName:"p"},"8161")," for the administration console (this should be removed once the app goes into production!) and makes it accessible over the port ",(0,o.kt)("strong",{parentName:"p"},"30010")," from outside of the Kubernetes cluster. Additionally we need to expose the port ",(0,o.kt)("strong",{parentName:"p"},"61616")," that ActiveMQ is using to broker messages."),(0,o.kt)("p",null,"You can now start both the pod as well as the service with:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f workloads.yaml\nkubectl apply -f services.yaml\nkubectl get all\n")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(7235).Z,width:"812",height:"217"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"The Admin panel should now be accessible over the IP address of your Kubernetes master server with the port ",(0,o.kt)("strong",{parentName:"p"},"30010"),":"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(95422).Z,width:"812",height:"349"})),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"deploying-the-position-simulator"},"Deploying the Position Simulator"),(0,o.kt)("p",null,"We can now add the Position Simulator to the ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml")," file - directly under the configuration of our queue server, divided by ",(0,o.kt)("inlineCode",{parentName:"p"},"---"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  # Unique key of the ReplicaSet instance\n  name: position-simulator\nspec:\n  selector:\n    matchLabels:\n      app: position-simulator\n  # only 1 Pod should exist atm - if it\n  # crashes, a new pod will be spawned.\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: position-simulator\n    spec:\n      containers:\n      - name: position-simulator\n        image: richardchesterwood/k8s-fleetman-position-simulator:release1\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n")),(0,o.kt)("p",null,"The configuration for deployment is similar to the ActiveMQ config. We only need to add a ",(0,o.kt)("strong",{parentName:"p"},"Environment Variable")," that sets the service to ",(0,o.kt)("em",{parentName:"p"},"Production Settings")," - ",(0,o.kt)("inlineCode",{parentName:"p"},"SPRING_PROFILES_ACTIVE: production-microservice")," (the service can be started with different profiles, depending if you are in a development or production environment)."),(0,o.kt)("p",null,"We can apply the new workloads configuration, check for the name of the new container (it will be called ",(0,o.kt)("inlineCode",{parentName:"p"},"position-simulator")," + an deployment ID + an replicationSet ID) and check if it is build correctly:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f workloads.yaml\nkubectl get all\nkubectl describe pod position-simulator-68bfc8d6fb-8vxkt\n")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(84540).Z,width:"815",height:"159"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"This service does not need to be accessed from outside of the Kubernetes cluster - so ",(0,o.kt)("strong",{parentName:"p"},"we do not need to create a service")," for it."),(0,o.kt)("p",null,"We can test the deployment by accessing the IP Address followed by the port ",(0,o.kt)("strong",{parentName:"p"},"30010"),", click on ",(0,o.kt)("em",{parentName:"p"},"Managing ActiveMQ Broker"),", sign in with ",(0,o.kt)("strong",{parentName:"p"},"admin, admin")," and click on ",(0,o.kt)("strong",{parentName:"p"},"Queues"),":"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(88858).Z,width:"804",height:"356"})),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(92527).Z,width:"812",height:"337"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"You should see a rising number of pending messages, telling you that our ",(0,o.kt)("strong",{parentName:"p"},"Position Simulator")," is successfully sending GPS coordinates to the message broker."),(0,o.kt)("h3",{id:"how-to-debug-a-failed-deployment"},"How to Debug a failed Deployment"),(0,o.kt)("p",null,"As we have seen above the deployment work and the Position Simulator is up and running - but how would we debug a container that cannot be started for some reason? We can create this circumstance by adding a typo in the environment variable inside the config file we created above, e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"SPRING_PROFILES_ACTIVE: production-microservice-typo"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f workloads.yaml\nkubectl get all\nkubectl describe pod position-simulator-77dcb74d75-dt27n\n")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(54268).Z,width:"816",height:"146"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"We can now see that the deployment is failing and Kubernetes is trying to restart the container in a loop (",(0,o.kt)("strong",{parentName:"p"},"CrashLoopBackOff"),"). To check what is going wrong, we can call the Kubernetes logs for the failing container:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl logs position-simulator-77dcb74d75-dt27n\n")),(0,o.kt)("p",null,"And the developer of the application should be able to spot the typo inside the selected profile for you:"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(84714).Z,width:"815",height:"388"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"Fixing the ",(0,o.kt)("em",{parentName:"p"},"typo"),' and re-applying the configuration should show you that the "loop-crashing" container is now being discarded and replaced by a working version:'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f workloads.yaml\nkubectl get all\n")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(1351).Z,width:"804",height:"77"})),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"deploying-the-position-tracker"},"Deploying the Position Tracker"),(0,o.kt)("p",null,"Now we need to start up the position tracker who's job it is take out the message that are send by our position simulator and are currently piling up in our ActiveMQ Server. The position tracked does some calculation on those messages and exposes his results through an REST interface to the API gateway."),(0,o.kt)("p",null,"We can now add the ",(0,o.kt)("strong",{parentName:"p"},"Position Tracker")," to the ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml")," file - directly under the configuration of our position simulator, divided by ",(0,o.kt)("inlineCode",{parentName:"p"},"---"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  # Unique key of the ReplicaSet instance\n  name: position-tracker\nspec:\n  selector:\n    matchLabels:\n      app: position-tracker\n  # only 1 Pod should exist atm - if it\n  # crashes, a new pod will be spawned.\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: position-tracker\n    spec:\n      containers:\n      - name: position-tracker\n        image: richardchesterwood/k8s-fleetman-position-tracker:release1\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n")),(0,o.kt)("p",null,"We can now deploy the position tracker and then take a look at our message queue. We should be able to see that the tracker is working and messages are getting de-queued:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f workloads.yaml\n")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(8111).Z,width:"812",height:"342"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"We can now add a service for the position tracker to expose the REST interface directly on port ",(0,o.kt)("strong",{parentName:"p"},"8080")," (",(0,o.kt)("em",{parentName:"p"},"optional - only for testing"),"). For this we need to add the following lines to our ",(0,o.kt)("inlineCode",{parentName:"p"},"services.yaml"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: v1\nkind: Service\nmetadata:\n  # Unique key of the Service instance\n  name: fleetman-position-tracker\nspec:\n  ports:\n    - name: rest-interface\n      port: 8080\n      targetPort: 8080\n      # The nodePort is available from outside of the\n      # cluster when is set to NodePort. It's value has\n      # to be > 30000\n      nodePort: 30020\n  selector:\n    app: position-tracker\n  # Setting the Service type to ClusterIP makes the\n  # service only available from inside the cluster\n  # To expose a port use NodePort instead\n  type: NodePort\n")),(0,o.kt)("p",null,"Now apply the changes ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl apply -f services.yaml")," and open the REST interface on your Master server IP address with the port ",(0,o.kt)("strong",{parentName:"p"},"30020"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"http://195.201.148.210:30020/vehicles/City%20Truck\n")),(0,o.kt)("p",null,"You should be able to see the current location and speed of the vehicle with the designation ",(0,o.kt)("strong",{parentName:"p"},"City Truck"),":"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(20223).Z,width:"812",height:"173"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"Since it would be dangerous to expose the REST API directly to the internet, we will remove the NodePort and have the API be available only from inside our cluster on port 8080:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: v1\nkind: Service\nmetadata:\n  # Unique key of the Service instance\n  name: fleetman-position-tracker\nspec:\n  ports:\n    - name: rest-interface\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: position-tracker\n  # Setting the Service type to ClusterIP makes the\n  # service only available from inside the cluster\n  # To expose a port use NodePort instead\n  type: ClusterIP\n")),(0,o.kt)("h3",{id:"deploying-the-api-gateway"},"Deploying the API Gateway"),(0,o.kt)("p",null,"We can now add the ",(0,o.kt)("strong",{parentName:"p"},"API Gateway")," to the ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml")," file - directly under the configuration of our position tracker, divided by ",(0,o.kt)("inlineCode",{parentName:"p"},"---"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  # Unique key of the ReplicaSet instance\n  name: api-gateway\nspec:\n  selector:\n    matchLabels:\n      app: api-gateway\n  # only 1 Pod should exist atm - if it\n  # crashes, a new pod will be spawned.\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n    spec:\n      containers:\n      - name: api-gateway\n        image: richardchesterwood/k8s-fleetman-api-gateway:release1\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n")),(0,o.kt)("p",null,"We can now deploy the gateway with ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl apply -f workloads.yaml"),". And then expose the port ",(0,o.kt)("strong",{parentName:"p"},"8080")," to the Kubernetes cluster in ",(0,o.kt)("inlineCode",{parentName:"p"},"services.yaml"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: v1\nkind: Service\nmetadata:\n  # Unique key of the Service instance\n  name: fleetman-api-gateway\nspec:\n  ports:\n    - name: rest-interface\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: api-gateway\n  # Setting the Service type to ClusterIP makes the\n  # service only available from inside the cluster\n  # To expose a port use NodePort instead\n  type: ClusterIP\n")),(0,o.kt)("h3",{id:"deploying-the-angular-frontend"},"Deploying the Angular Frontend"),(0,o.kt)("p",null,"We can now add the ",(0,o.kt)("strong",{parentName:"p"},"Web APP")," to the ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml")," file - directly under the configuration of our position tracker, divided by ",(0,o.kt)("inlineCode",{parentName:"p"},"---"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  # Unique key of the ReplicaSet instance\n  name: webapp\nspec:\n  selector:\n    matchLabels:\n      app: webapp\n  # only 1 Pod should exist atm - if it\n  # crashes, a new pod will be spawned.\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: richardchesterwood/k8s-fleetman-webapp-angular:release1\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n")),(0,o.kt)("p",null,"We can now deploy the frontend with ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl apply -f workloads.yaml"),". And then expose the port ",(0,o.kt)("strong",{parentName:"p"},"80")," to the Kubernetes cluster in ",(0,o.kt)("inlineCode",{parentName:"p"},"services.yaml"),", as well as adding the public port (NodePort) ",(0,o.kt)("strong",{parentName:"p"},"30080"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Service\nmetadata:\n  # Unique key of the Service instance\n  name: fleetman-webapp\nspec:\n  ports:\n    # Accept traffic sent to port 80\n    - name: http\n      port: 80\n      targetPort: 80\n      # The nodePort is available from outside of the\n      # cluster when is set to NodePort. It's value has\n      # to be > 30000\n      nodePort: 30080\n  selector:\n    # Define which pods are going to\n    # be represented by this service\n    # The service makes an network\n    # endpoint for our app\n    app: webapp\n  # Setting the Service type to ClusterIP makes the\n  # service only available from inside the cluster\n  # To expose a port use NodePort instead\n  type: NodePort\n")),(0,o.kt)("p",null,"Your complete Microservice deployment should now look something like this:"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(88266).Z,width:"816",height:"497"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"And you should be able to access the web interface on port ",(0,o.kt)("strong",{parentName:"p"},"30080")," on your master server IP address:"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(24052).Z,width:"812",height:"428"})),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"persistence-in-a-kubernetes-cluster"},"Persistence in a Kubernetes Cluster"),(0,o.kt)("p",null,"In Docker all data that is generated inside a container is lost when you restart it. So if we, for example, want to store the geo location of our car fleet and calculate a travel path from it, all of that is gone, when the container restarts. Docker offers persistent in external volumes to prevent this from happening."),(0,o.kt)("p",null,"To add ",(0,o.kt)("strong",{parentName:"p"},"vehicle tracking")," to our app, we need to update all of our images to the ",(0,o.kt)("inlineCode",{parentName:"p"},":release2")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue\nspec:\n  selector:\n    matchLabels:\n      app: queue\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: queue\n    spec:\n      containers:\n      - name: queue\n        image: richardchesterwood/k8s-fleetman-queue:release2\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: position-simulator\nspec:\n  selector:\n    matchLabels:\n      app: position-simulator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: position-simulator\n    spec:\n      containers:\n      - name: position-simulator\n        image: richardchesterwood/k8s-fleetman-position-simulator:release2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: position-tracker\nspec:\n  selector:\n    matchLabels:\n      app: position-tracker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: position-tracker\n    spec:\n      containers:\n      - name: position-tracker\n        image: richardchesterwood/k8s-fleetman-position-tracker:release2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway\nspec:\n  selector:\n    matchLabels:\n      app: api-gateway\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n    spec:\n      containers:\n      - name: api-gateway\n        image: richardchesterwood/k8s-fleetman-api-gateway:release2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n          \n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\nspec:\n  selector:\n    matchLabels:\n      app: webapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: richardchesterwood/k8s-fleetman-webapp-angular:release2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: production-microservice\n")),(0,o.kt)("p",null,"Restarting everything with ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl apply -f workloads.yaml")," now shows us the v2 interface on port ",(0,o.kt)("strong",{parentName:"p"},"30080"),". Clicking on a vehicle name in the list on the left will jump you to the selected truck and highlight the path that vehicle has taken:"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(86142).Z,width:"812",height:"541"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"This data is stored in an internal data structure inside the ",(0,o.kt)("strong",{parentName:"p"},"Position Tracker")," container and will be lost if you reload that container. You can get the webapp pod name and delete it:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"kubectl get all\nkubectl delete pod -f pod/position-tracker-684d9d84cb-st8pc\n")),(0,o.kt)("p",null,"Your deployment will take care of restarting the webapp for you - if you reload the web interface you will see that the tracker data has been lost. To prevent this from happening, we now want to add a ",(0,o.kt)("strong",{parentName:"p"},"MongoDB")," database to our cluster that stores the data tracker produces in a persistent way."),(0,o.kt)("h3",{id:"mongodb-pod"},"MongoDB Pod"),(0,o.kt)("p",null,"We have a new release of the app ",(0,o.kt)("inlineCode",{parentName:"p"},":release3")," that is build, expecting there to be a ",(0,o.kt)("a",{parentName:"p",href:"https://hub.docker.com/_/mongo"},"MongoDB Database")," (",(0,o.kt)("em",{parentName:"p"},"3.6.10-stretch"),") on our cluster to store the tracking data in. We can create a new Kubernetes deployment for this Docker images and we are going to configure it in a file called ",(0,o.kt)("inlineCode",{parentName:"p"},"mongo-stack.yaml"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  selector:\n    matchLabels:\n      app: mongodb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.10-stretch\n")),(0,o.kt)("p",null,"Now update only the ",(0,o.kt)("strong",{parentName:"p"},"Position Tracker")," image in ",(0,o.kt)("inlineCode",{parentName:"p"},"workloads.yaml")," to v3 (same as before, see above) and apply all changes to the cluster:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"kubectl apply -f mongo-stack.yaml\nkubectl apply -f workloads.yaml\nkubectl get all\n")),(0,o.kt)("h3",{id:"mongodb-service"},"MongoDB Service"),(0,o.kt)("p",null,"To enable our tracker to use our new database we need to add a Kubernetes service and expose the MongoDB port to the Cluster. And we can define this services inside the ",(0,o.kt)("inlineCode",{parentName:"p"},"mongo-stack.yaml")," file, right under the pod config:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  selector:\n    matchLabels:\n      app: mongodb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.10-stretch\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: fleetman-mongodb\nspec:\n  ports:\n    - name: mongo-default\n      port: 27017\n      targetPort: 27017\n  selector:\n    app: mongodb\n  type: ClusterIP\n")),(0,o.kt)("p",null,"It is critical to add the metadata name ",(0,o.kt)("inlineCode",{parentName:"p"},"fleetman-mongodb")," as this is going to be the domain name that CoreDNS inside our cluster is using to resolve the pod IP address and our v3 ",(0,o.kt)("strong",{parentName:"p"},"Production Tracker")," is ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/DickChesterwood/k8s-fleetman/blob/release3/k8s-fleetman-position-tracker/src/main/resources/application-production-microservice.properties"},"configured to search")," for the MongoDB database on ",(0,o.kt)("inlineCode",{parentName:"p"},"mongodb.host=fleetman-mongodb.default.svc.cluster.local"),"!"),(0,o.kt)("p",null,"Also make sure that the service ",(0,o.kt)("strong",{parentName:"p"},"Selector")," is set to match the ",(0,o.kt)("strong",{parentName:"p"},"matchLabels")," in the pod config above, as this is used to connect the service. Now re-apply the Mongo stack configuration to start up the service. If you repeat the experiment from earlier and delete the ",(0,o.kt)("strong",{parentName:"p"},"Position Tracker")," the collected data will persist. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"kubectl apply -f mongo-stack.yaml\nkubectl get all\nkubectl delete pod -f pod/position-tracker-684d9d84cb-st8pc\n")),(0,o.kt)("h3",{id:"volume-mounts"},"Volume Mounts"),(0,o.kt)("p",null,"Right now the data is still stored on the filesystem of the MongoDB container. We need to configure the MongoDB container to persist the data outside of the container itself, on our Kubernetes node filesystem in a ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/persistent-volumes/"},"Persistent Volume")," to make it survive a complete reload of our cluster."),(0,o.kt)("p",null,"To mount an external volume into the MongoDB container, we need to add a few lines to the pod configuration file. If you scroll to the bottom of the  DockerHub page (",(0,o.kt)("a",{parentName:"p",href:"https://hub.docker.com/_/mongo"},"Where to Store Data?"),") you can see that the default data storage path inside the MongoDB container is ",(0,o.kt)("inlineCode",{parentName:"p"},"/data/db"),". We now have to link this path inside the container (",(0,o.kt)("inlineCode",{parentName:"p"},"mountPath"),") to a volume on our host server (or an EBS volume on AWS). You can find all the options for the receiving volume in the ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#volume-v1-core"},"Kubernetes documentation"),". We are going to use a ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/volumes/#hostpath"},"hostPath")," with the type ",(0,o.kt)("strong",{parentName:"p"},"DirectoryOrCreate")," - meaning that we don't have to go into the host and create the directory first (it will be created if not existing):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  selector:\n    matchLabels:\n      app: mongodb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.10-stretch\n        volumeMounts:\n          - name: mongo-persistent-storage\n            mountPath: /data/db\n      volumes:\n        - name: mongo-persistent-storage      \n          hostPath:\n            # directory location on host\n            path: /mnt/kubernetes/mongodb\n            # DirectoryOrCreate, Directory, FileOrCreate, File, etc.\n            type: DirectoryOrCreate\n")),(0,o.kt)("p",null,"Apply those changes to your cluster and use the ",(0,o.kt)("inlineCode",{parentName:"p"},"describe")," command to check if the mount was successful:"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(68050).Z,width:"815",height:"921"})),(0,o.kt)("hr",null),(0,o.kt)("p",null,"From the event log at the bottom we can see that the container was deployed to ",(0,o.kt)("inlineCode",{parentName:"p"},"in-centos-minion2")," (alternatively use ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl get pods -o wide")," to list the nodes your pods are hosted on) - a quick check confirms that the volume ",(0,o.kt)("inlineCode",{parentName:"p"},"/mnt/kubernetes/mongodb")," was created and MongoDB started to use it:"),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(26797).Z,width:"806",height:"354"})),(0,o.kt)("hr",null),(0,o.kt)("h3",{id:"using-persistentvolumeclaims"},"Using PersistentVolumeClaims"),(0,o.kt)("p",null,"A ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/volumes/#persistentvolumeclaim"},"persistentVolumeClaim")," volume is used to mount a PersistentVolume into a Pod. PersistentVolumes are a way for users to \u201cclaim\u201d durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment. This is just a minor change to our ",(0,o.kt)("inlineCode",{parentName:"p"},"mongo-stack.yaml")," file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb\nspec:\n  selector:\n    matchLabels:\n      app: mongodb\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:3.6.10-stretch\n        volumeMounts:\n          - name: mongo-persistent-storage\n            mountPath: /data/db\n      volumes:\n        - name: mongo-persistent-storage\n          persistentVolumeClaim:\n            claimName: mongo-pvc\n")),(0,o.kt)("p",null,"The persistent volume is then configured in a separate configuration file we will call ",(0,o.kt)("inlineCode",{parentName:"p"},"storage.yaml")," - this way, if we have to move our cluster to a new cloud provider we do not have to make any changes to the workload or service file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'# What do want?\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  # has to match the name you used as claimName!\n  name: mongo-pvc\nspec:\n  # linking the claim with the implementation below\n  storageClassName: mylocalstorage\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      # Let Kubernetes find a node that offers at least the amount of storage\n      storage: 1Gi\n\n---\n# How do we want it implemented\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: local-storage\nspec:\n  storageClassName: mylocalstorage\n  capacity:\n    # create a storage claim for this amount - e.g. create a EBS volume on AWS\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    # directory location on host\n    path: "/mnt/kubernetes/mongodb-tracking-data"\n    # DirectoryOrCreate, Directory, FileOrCreate, File, etc.\n    type: DirectoryOrCreate\n')),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"PersistentVolumeClaim")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"PersistentVolume")," are matched up by the ",(0,o.kt)("inlineCode",{parentName:"p"},"storageClassName"),". The cloud administrator has to create persistent storages based on available hardware (or cloud storage partitions). The web developer then creates a claim for storage with a certain capacity - so Kubernetes can search for a fitting volume among the available."),(0,o.kt)("p",null,"We choose the ",(0,o.kt)("inlineCode",{parentName:"p"},"storageClassName: mylocalstorage")," - in production this would be something more useful. E.g. your pods really need very fast storage - so you can set a claim for a ",(0,o.kt)("strong",{parentName:"p"},"storageClassName")," that refers to high performance ",(0,o.kt)("em",{parentName:"p"},"SSD")," storage."),(0,o.kt)("p",null,"Noteworthy also is the ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes"},"Access Mode")," that can be:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"ReadWriteOnce \u2013 the volume can be mounted as read-write by a single node"),(0,o.kt)("li",{parentName:"ul"},"ReadOnlyMany \u2013 the volume can be mounted read-only by many nodes"),(0,o.kt)("li",{parentName:"ul"},"ReadWriteMany \u2013 the volume can be mounted as read-write by many nodes")),(0,o.kt)("p",null,"In the CLI, the access modes are abbreviated to:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"RWO - ReadWriteOnce"),(0,o.kt)("li",{parentName:"ul"},"ROX - ReadOnlyMany"),(0,o.kt)("li",{parentName:"ul"},"RWX - ReadWriteMany")),(0,o.kt)("p",null,"Once you done with the configuration, apply it and check if the pv (persistent volume) was created and bound to the MongoDB pod:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f storage.yaml\nkubectl apply -f mongo-stack.yaml\nkubectl get pv\nkubectl get pvc\n")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Kubernetes Cluster &amp; Microservices",src:n(26797).Z,width:"806",height:"354"})),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"cloud-deployment"},"Cloud Deployment"),(0,o.kt)("p",null,"kubectl expose deployment webapp --type=LoadBalancer --name=exposed-webapp"),(0,o.kt)("p",null,"Examples:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000.\nkubectl expose rc nginx --port=80 --target-port=8000"),(0,o.kt)("li",{parentName:"ul"},'Create a service for a replication controller identified by type and name specified in "nginx-controller.yaml",\nwhich serves on port 80 and connects to the containers on port 8000.\nkubectl expose -f nginx-controller.yaml --port=80 --target-port=8000\n'),(0,o.kt)("li",{parentName:"ul"},'Create a service for a pod valid-pod, which serves on port 444 with the name "frontend"\nkubectl expose pod valid-pod --port=444 --name=frontend'),(0,o.kt)("li",{parentName:"ul"},'Create a second service based on the above service, exposing the container port 8443 as port 443 with the name\n"nginx-https"\nkubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https\n'),(0,o.kt)("li",{parentName:"ul"},"Create a service for a replicated streaming application on port 4100 balancing UDP traffic and named 'video-stream'.\nkubectl expose rc streamer --port=4100 --protocol=udp --name=video-stream"),(0,o.kt)("li",{parentName:"ul"},"Create a service for a replicated nginx using replica set, which serves on port 80 and connects to the containers on\nport 8000.\nkubectl expose rs nginx --port=80 --target-port=8000\n"),(0,o.kt)("li",{parentName:"ul"},"Create a service for an nginx deployment, which serves on port 80 and connects to the containers on port 8000.\nkubectl expose deployment nginx --port=80 --target-port=8000")))}d.isMDXComponent=!0},7235:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_01-5d7fdb5a975efb00f383d2b838d77b9d.png"},95422:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_02-c69da73b0d9502439143a33749e18a1b.png"},84540:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_03-e99f3290bd2f2c84228945ce8195da79.png"},54268:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_04-7a6fd5964d0dc6d46e46b6086233eee8.png"},84714:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_05-f76bb30fb466add35e52b4cf38c9606d.png"},1351:(e,t,n)=>{n.d(t,{Z:()=>a});const a="data:image/png;base64,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"},88858:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_07-61349108d51d3eae71ac0d6bb74b30d4.png"},92527:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_08-94fe9e169958662555d4a589e03ddd6e.png"},8111:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_09-0a4a51867cdef86daf0a3848cb2fb691.png"},20223:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_10-a3384faf98b9218811f2aa64604f7e68.png"},88266:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_11-504e4b18ecf5be7efac5cff8a4b29ccd.png"},24052:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_12-0b3ea67289dae4b50a496caae45377db.gif"},86142:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_13-ec19a98bf72fda2746c4f91c03c53932.png"},68050:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_14-fdf86d0995e9324db6b8ac586f9ac2c5.png"},26797:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_microservices_15-3cafdbbacc607aa0fb627dbf04c0eb62.png"},79037:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/photo-34364880182_fe2d33582b_o-8d19bb0c73954ba4d77b58ef1262f6a9.jpg"}}]);