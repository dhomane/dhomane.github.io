"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[3462],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>h});var n=t(67294);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,o=function(e,a){if(null==e)return{};var t,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var d=n.createContext({}),s=function(e){var a=n.useContext(d),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},c=function(e){var a=s(e.components);return n.createElement(d.Provider,{value:a},e.children)},m={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},p=n.forwardRef((function(e,a){var t=e.components,o=e.mdxType,i=e.originalType,d=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=s(t),h=o,u=p["".concat(d,".").concat(h)]||p[h]||m[h]||i;return t?n.createElement(u,r(r({ref:a},c),{},{components:t})):n.createElement(u,r({ref:a},c))}));function h(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var i=t.length,r=new Array(i);r[0]=p;var l={};for(var d in a)hasOwnProperty.call(a,d)&&(l[d]=a[d]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var s=2;s<i;s++)r[s]=t[s];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}p.displayName="MDXCreateElement"},98796:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var n=t(87462),o=(t(67294),t(3905));const i={sidebar_position:4960,slug:"2022-12-11",title:"Breast Histopathology Image Segmentation Part 3",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Model creation based on a pre-trained and a custom model"},r=void 0,l={unversionedId:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/index",id:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/index",title:"Breast Histopathology Image Segmentation Part 3",description:"Model creation based on a pre-trained and a custom model",source:"@site/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/index.md",sourceDirName:"IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3",slug:"/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/index.md",tags:[{label:"Python",permalink:"/docs/tags/python"},{label:"Machine Learning",permalink:"/docs/tags/machine-learning"},{label:"Tensorflow",permalink:"/docs/tags/tensorflow"}],version:"current",sidebarPosition:4960,frontMatter:{sidebar_position:4960,slug:"2022-12-11",title:"Breast Histopathology Image Segmentation Part 3",authors:"mpolinowski",tags:["Python","Machine Learning","Tensorflow"],description:"Model creation based on a pre-trained and a custom model"},sidebar:"tutorialSidebar",previous:{title:"Breast Histopathology Image Segmentation Part 4",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11"},next:{title:"Breast Histopathology Image Segmentation Part 2",permalink:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11"}},d={},s=[{value:"Model Creation - ResNet50",id:"model-creation---resnet50",level:2},{value:"Model Creation - Custom CNN Model",id:"model-creation---custom-cnn-model",level:2},{value:"Construction",id:"construction",level:3}],c={toc:s};function m(e){let{components:a,...i}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,i,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Guangzhou, China",src:t(12101).Z,width:"1500",height:"383"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10"},"Part 1: Data Inspection and Pre-processing")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11"},"Part 2: Weights, Data Augmentations and Generators")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11"},"Part 3: Model creation based on a pre-trained and a custom model")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11"},"Part 4: Train our model to fit the dataset")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12"},"Part 5: Evaluate the performance of your trained model")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12"},"Part 6: Running Predictions"))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("a",{parentName:"p",href:"https://github.com/mpolinowski/tf-bc-classification"},"Github"))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#model-creation---resnet50"},"Model Creation - ResNet50")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#model-creation---custom-cnn-model"},"Model Creation - Custom CNN Model"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#construction"},"Construction"))))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Based on ",(0,o.kt)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images"},"Breast Histopathology Images")," by ",(0,o.kt)("a",{parentName:"p",href:"https://www.kaggle.com/paultimothymooney"},"Paul Mooney"),".\n",(0,o.kt)("inlineCode",{parentName:"p"},"Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide."),"\n",(0,o.kt)("a",{parentName:"p",href:"https://youtu.be/8XsiMQQ-4mM"},"Can recurring breast cancer be spotted with AI tech? - BBC News"))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Citation: ",(0,o.kt)("a",{parentName:"li",href:"https://pubmed.ncbi.nlm.nih.gov/27563488/"},"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases")),(0,o.kt)("li",{parentName:"ul"},"Dataset: 198,738 IDC(negative) image patches; 78,786 IDC(positive) image patches")),(0,o.kt)("h2",{id:"model-creation---resnet50"},"Model Creation - ResNet50"),(0,o.kt)("p",null,"Start by loading the pre-trained model:"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"./train","_","ResNet50","_","32","_","20k.py")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'# Loading the ResNet50, ensuring the head Full Connected layers are left off / removed\nbaseModel = ResNet50(weights="imagenet", include_top=False, input_tensor=Input(shape=(48, 48, 3)))\n')),(0,o.kt)("p",null,"Construct the head of the model that will be placed on top of the the base model:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'## Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. \n## We flatten the output of the convolutional layers to create a single long feature vector. \n### Average pooling computes the average of the elements present in the region of feature map covered by the filter.\n#### ReLU stands for Rectified Linear Unit. \n#### The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n## Dropout is a technique where randomly selected neurons are ignored during training.\n\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7, 7), padding="same")(headModel)\nheadModel = Flatten(name="flatten")(headModel)\nheadModel = Dense(256, activation="relu")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(len(config.CLASSES), activation="softmax")(headModel)\n\n# Placing the head model on top of the base model\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n# Loop over all the layers of the base model and freeze them so that they are \n# not updated during the training process\nfor layer in baseModel.layers:\n    layer.trainable = False\n')),(0,o.kt)("h2",{id:"model-creation---custom-cnn-model"},"Model Creation - Custom CNN Model"),(0,o.kt)("h3",{id:"construction"},"Construction"),(0,o.kt)("p",null,"Instead of downloading a pre-trained model we can build our own. The layers of the ",(0,o.kt)("strong",{parentName:"p"},"Custom Model")," are constructed according to:"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"utils/conv","_","bc","_","model.py")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'class BC_Model:\n    @staticmethod\n    def build(width, height, depth, classes):\n    \n        # Lets first initialize the model with input shape to be "channels last" and channel\'s dimension\n        model = Sequential()\n        inputShape = (height, width, depth)\n        chanDim = -1\n        \n        # If we are using "channels first", then let\'s update the input shape and channel\'s dimension\n        if K.image_data_format() == "channels_first":\n            inputShape = (depth, height, width)\n            chanDim = 1\n            \n        # (CONV2D => RELU => BatchNormalization ) * 1 => POOL => DROPOUT\n        model.add(Conv2D(32, (3, 3), padding="same", input_shape=inputShape))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # (CONV2D => RELU => BatchNormalization ) * 2 => POOL => DROPOUT\n        model.add(Conv2D(64, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(64, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # (CONV2D => RELU => BatchNormalization ) * 3 => POOL => DROPOUT\n        model.add(Conv2D(128, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(128, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(128, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # (CONV2D => RELU => BatchNormalization ) * 4 => POOL => DROPOUT\n        model.add(Conv2D(256, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(256, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(256, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(256, (3, 3), padding="same"))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # FC => RELU layers => BatchNormalization => DROPOUT\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation("relu"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        \n        # Dense layer and softmax classifier\n        model.add(Dense(classes))\n        model.add(Activation("softmax"))\n        \n        # Returning the created network architecture\n        return model\n')),(0,o.kt)("p",null,"We can use this function to build the model:"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"./train","_","CustomModel","_","32","_","conv","_","20k.py")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'# Building the model\nprint("Building the model")\nmodel = BC_Model.build(width=48, height=48, depth=3, classes=2)\n')))}m.isMDXComponent=!0},12101:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-918471126c0472aad97358a725e1a399.jpg"}}]);