"use strict";(self.webpackChunkmikes_dev_notebook=self.webpackChunkmikes_dev_notebook||[]).push([[34595],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>d});var n=a(67294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),h=c(a),d=s,g=h["".concat(l,".").concat(d)]||h[d]||u[d]||o;return a?n.createElement(g,r(r({ref:t},p),{},{components:a})):n.createElement(g,r({ref:t},p))}));function d(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var o=a.length,r=new Array(o);r[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:s,r[1]=i;for(var c=2;c<o;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},51609:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var n=a(87462),s=(a(67294),a(3905));const o={sidebar_position:9010,slug:"2021-03-21",title:"Logstash 7 Digesting Webserver Logs",authors:"mpolinowski",tags:["LINUX","Elasticsearch"]},r=void 0,i={unversionedId:"DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/index",id:"DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/index",title:"Logstash 7 Digesting Webserver Logs",description:"Guangzhou, China",source:"@site/docs/DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/index.md",sourceDirName:"DevOps/Elasticsearch/2021-03-21-logstash-digesting-data",slug:"/DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/2021-03-21",permalink:"/docs/DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/2021-03-21",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/DevOps/Elasticsearch/2021-03-21-logstash-digesting-data/index.md",tags:[{label:"LINUX",permalink:"/docs/tags/linux"},{label:"Elasticsearch",permalink:"/docs/tags/elasticsearch"}],version:"current",sidebarPosition:9010,frontMatter:{sidebar_position:9010,slug:"2021-03-21",title:"Logstash 7 Digesting Webserver Logs",authors:"mpolinowski",tags:["LINUX","Elasticsearch"]},sidebar:"tutorialSidebar",previous:{title:"Logstash 7 Working with Structured Data",permalink:"/docs/DevOps/Elasticsearch/2021-03-22-logstash-working-with-structured-data/2021-03-22"},next:{title:"Elasticsearch 7 Refresher",permalink:"/docs/DevOps/Elasticsearch/2021-03-20-elasticsearch-7-refresher/2021-03-20"}},l={},c=[{value:"Setup Elasticsearch",id:"setup-elasticsearch",level:2},{value:"Use CURL to Upload and Query Data",id:"use-curl-to-upload-and-query-data",level:3},{value:"Setup Logstash",id:"setup-logstash",level:2},{value:"Preparing the Configuration Files",id:"preparing-the-configuration-files",level:3},{value:"Setup Kibana",id:"setup-kibana",level:2}],p={toc:c};function u(e){let{components:t,...o}=e;return(0,s.kt)("wrapper",(0,n.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Guangzhou, China",src:a(72051).Z,width:"1500",height:"616"})),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#setup-elasticsearch"},"Setup Elasticsearch"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#use-curl-to-upload-and-query-data"},"Use CURL to Upload and Query Data")))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#setup-logstash"},"Setup Logstash"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#preparing-the-configuration-files"},"Preparing the Configuration Files")))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"#setup-kibana"},"Setup Kibana"))),(0,s.kt)("h2",{id:"setup-elasticsearch"},"Setup Elasticsearch"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"/docs/DevOps/Elasticsearch/2021-03-20-elasticsearch-7-refresher/2021-03-20"},"Before I was using an existing ELK Cluster")," and it is working well in production. But I have difficulties connecting to it through CURL (I am using Kibana to manage it so I never noticed):"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"curl localhost:9200\n\ncurl: (7) Failed to connect to localhost port 9200: Connection refused\n")),(0,s.kt)("p",null,"So I now set up a fresh version of the Elasticsearch Docker Container :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"docker pull elasticsearch:7.13.4\ndocker pull logstash:7.13.4\ndocker pull kibana:7.13.4\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'docker run -d \\\n       --name elasticsearch \\\n       --privileged \\\n       --net=host\\\n       -e "discovery.type=single-node" \\\n       -e "XPACK_SECURITY_ENABLED=false" \\\n       -e "XPACK_REPORTING_ENABLED=false" \\\n       -e "XPACK_MONITORING_ENABLED=false" \\\n       elasticsearch:7.13.4\n')),(0,s.kt)("p",null,"And tada! Now - without all the virtual network trickery - it works:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'curl localhost:9200\n\n{\n  "name" : "Debian11",\n  "cluster_name" : "docker-cluster",\n  "cluster_uuid" : "3Gi_d-bqT06e3qJwNai6hw",\n  "version" : {\n    "number" : "7.13.4",\n    "build_flavor" : "default",\n    "build_type" : "docker",\n    "build_hash" : "c5f60e894ca0c61cdbae4f5a686d9f08bcefc942",\n    "build_date" : "2021-07-14T18:33:36.673943207Z",\n    "build_snapshot" : false,\n    "lucene_version" : "8.8.2",\n    "minimum_wire_compatibility_version" : "6.8.0",\n    "minimum_index_compatibility_version" : "6.0.0-beta1"\n  },\n  "tagline" : "You Know, for Search"\n}\n')),(0,s.kt)("p",null,"To make sure my installation is save (without any security settings) I will close the door to the outside world:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"ufw deny 9200/tcp\n\nRule updated\nRule updated (v6)\n\nufw deny 9300/tcp\n\nRule updated\nRule updated (v6)\n")),(0,s.kt)("h3",{id:"use-curl-to-upload-and-query-data"},"Use CURL to Upload and Query Data"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"/docs/DevOps/Elasticsearch/2021-03-20-elasticsearch-7-refresher/2021-03-20"},"Before I was using a dataset")," that was too big to be used in Kibana and had to be broken up. The Data that is ",(0,s.kt)("a",{parentName:"p",href:"http://media.sundog-soft.com/es7/shakespeare_7.0.json"},"available for download here"),". Just download the mapping and data to your server:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"curl http://media.sundog-soft.com/es7/shakes-mapping.json >> shakespeare_mapping.json\ncurl http://media.sundog-soft.com/es7/shakespeare_7.0.json >> shakespeare.json\n")),(0,s.kt)("p",null,"We can now use CURL against the Elasticsearch REST API to create our Shakespeare index:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -H "Content-Type: application/json" -XPUT 127.0.0.1:9200/shakespeare --data-binary @shakespeare_mapping.json\n\n{"acknowledged":true,"shards_acknowledged":true,"index":"shakespeare"}\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -H "Content-Type: application/json" -XPOST 127.0.0.1:9200/shakespeare/_bulk --data-binary @shakespeare.json\n\n{"acknowledged":true,"shards_acknowledged":true,"index":"shakespeare"}\n')),(0,s.kt)("p",null,"We can now query the index:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -H "Content-Type: application/json" -XGET \'127.0.0.1:9200/shakespeare/_search?pretty\' -d \'{"query": {"match_phrase": {"text_entry": "Juliet is the sun."}}}\'\n')),(0,s.kt)("p",null,"And get the response:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "took" : 35,\n  "timed_out" : false,\n  "_shards" : {\n    "total" : 1,\n    "successful" : 1,\n    "skipped" : 0,\n    "failed" : 0\n  },\n  "hits" : {\n    "total" : {\n      "value" : 1,\n      "relation" : "eq"\n    },\n    "max_score" : 16.28321,\n    "hits" : [\n      {\n        "_index" : "shakespeare",\n        "_type" : "_doc",\n        "_id" : "86136",\n        "_score" : 16.28321,\n        "_source" : {\n          "type" : "line",\n          "line_id" : 86137,\n          "play_name" : "Romeo and Juliet",\n          "speech_number" : 1,\n          "line_number" : "2.2.3",\n          "speaker" : "ROMEO",\n          "text_entry" : "It is the east, and Juliet is the sun."\n        }\n      }\n    ]\n  }\n}\n')),(0,s.kt)("h2",{id:"setup-logstash"},"Setup Logstash"),(0,s.kt)("h3",{id:"preparing-the-configuration-files"},"Preparing the Configuration Files"),(0,s.kt)("p",null,"Before we start the Logstash container we first have to add two configuration files. Start by creating the following directories:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"mkdir -p /opt/logstash/pipeline\n")),(0,s.kt)("p",null,"We can download some Apache log sample data from ",(0,s.kt)("a",{parentName:"p",href:"http://media.sundog-soft.com/es/access_log"},"here"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"curl http://media.sundog-soft.com/es/access_log >> /opt/logstash/access_log\n")),(0,s.kt)("p",null,"It is essential to place your pipeline configuration where it can be found by Logstash. By default, the container will look in ",(0,s.kt)("inlineCode",{parentName:"p"},"/usr/share/logstash/pipeline/")," for pipeline configuration files. If you don\u2019t provide configuration to Logstash, it will run with a minimal config that listens for messages from the Beats input plugin and echoes any that are received to ",(0,s.kt)("inlineCode",{parentName:"p"},"stdout"),"."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"nano /opt/logstash/pipeline/logstash.conf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'input {\n  file {\n    path => "/usr/share/logstash/access_log"\n    start_position => "beginning"\n  }\n}\n\nfilter {\n  grok {\n    match => {"message" => "%{COMBINEDAPACHELOG}"}\n  }\n  date {\n    match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => ["http://localhost:9200"]\n    }\n    stdout {\n      codec => rubydebug\n    }\n}\n')),(0,s.kt)("p",null,"Now we can run the countainer while mounting our configuration file + sample log data:"),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"Bind-mounted configuration files will retain the same permissions and ownership within the container that they have on the host system. Be sure to set permissions such that the files will be readable and, ideally, not writeable by the container\u2019s logstash user (UID 1000).")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"chown -R 1000:1000 /opt/logstash\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'docker run \\\n   --name logstash \\\n   --net=host \\\n   --rm -it \\\n   -v /opt/logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\\n   -v /opt/logstash/access_log:/usr/share/logstash/access_log \\\n   -e "ELASTIC_HOST=localhost:9200" \\\n   -e "XPACK_SECURITY_ENABLED=false" \\\n   -e "XPACK_REPORTING_ENABLED=false" \\\n   -e "XPACK_MONITORING_ENABLED=false" \\\n   -e "XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic" \\\n   -e "XPACK_MONITORING_ELASTICSEARCH_PASSWORD=changeme" \\\n   logstash:7.13.4\n')),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"This is the point where ",(0,s.kt)("strong",{parentName:"p"},"I abandoned my meek virtual online server"),". Every time I tried starting Logstash, Elasticsearch ran out of memory and crashed. So I continue on my local Intel NUC (equivalent) that brings the power to handle this setup.")),(0,s.kt)("p",null,"Once I rebuild my setup here I was able to start Logstash and see that it started working on the Apache Log:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'2021/07/28 11:53:58 Setting \'xpack.monitoring.enabled\' from environment.\n2021/07/28 11:53:58 Setting \'xpack.monitoring.elasticsearch.username\' from environment.\n2021/07/28 11:53:58 Setting \'xpack.monitoring.elasticsearch.password\' from environment.\nUsing bundled JDK: /usr/share/logstash/jdk\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\nSending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties\n[2021-07-28T11:54:14,361][INFO ][logstash.runner          ] Log4j configuration path used is: /usr/share/logstash/config/log4j2.properties\n[2021-07-28T11:54:14,370][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.13.4", "jruby.version"=>"jruby 9.2.16.0 (2.5.7) 2021-03-03 f82228dc32 OpenJDK 64-Bit Server VM 11.0.11+9 on 11.0.11+9 +indy +jit [linux-x86_64]"}\n[2021-07-28T11:54:14,391][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.queue", :path=>"/usr/share/logstash/data/queue"}\n[2021-07-28T11:54:14,403][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.dead_letter_queue", :path=>"/usr/share/logstash/data/dead_letter_queue"}\n[2021-07-28T11:54:14,722][INFO ][logstash.agent           ] No persistent UUID file found. Generating new UUID {:uuid=>"b0829cb5-dc99-4b99-a2f2-8b5d0f6acfcb", :path=>"/usr/share/logstash/data/uuid"}\n[2021-07-28T11:54:15,418][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}\n[2021-07-28T11:54:16,074][INFO ][org.reflections.Reflections] Reflections took 39 ms to scan 1 urls, producing 24 keys and 48 values \n[2021-07-28T11:54:17,018][WARN ][deprecation.logstash.outputs.elasticsearch] Relying on default value of `pipeline.ecs_compatibility`, which may change in a future major release of Logstash. To avoid unexpected changes when upgrading Logstash, please explicitly declare your desired ECS Compatibility mode.\n[2021-07-28T11:54:17,223][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["http://localhost:9200"]}\n[2021-07-28T11:54:17,573][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}\n[2021-07-28T11:54:17,729][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}\n[2021-07-28T11:54:17,795][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.13.4) {:es_version=>7}\n[2021-07-28T11:54:17,797][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won\'t be used to determine the document _type {:es_version=>7}\n[2021-07-28T11:54:17,898][WARN ][logstash.outputs.elasticsearch][main] Configuration is data stream compliant but due backwards compatibility Logstash 7.x will not assume writing to a data-stream, default behavior will change on Logstash 8.0 (set `data_stream => true/false` to disable this warning)\n[2021-07-28T11:54:17,899][WARN ][logstash.outputs.elasticsearch][main] Configuration is data stream compliant but due backwards compatibility Logstash 7.x will not assume writing to a data-stream, default behavior will change on Logstash 8.0 (set `data_stream => true/false` to disable this warning)\n[2021-07-28T11:54:17,917][WARN ][deprecation.logstash.filters.grok][main] Relying on default value of `pipeline.ecs_compatibility`, which may change in a future major release of Logstash. To avoid unexpected changes when upgrading Logstash, please explicitly declare your desired ECS Compatibility mode.\n[2021-07-28T11:54:17,966][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}\n[2021-07-28T11:54:18,082][INFO ][logstash.outputs.elasticsearch][main] Installing Elasticsearch template {:name=>"logstash"}\n[2021-07-28T11:54:18,179][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>500, "pipeline.sources"=>["/usr/share/logstash/pipeline/logstash.conf"], :thread=>"#<Thread:0x5dc1d77c run>"}[2021-07-28T11:54:18,431][INFO ][logstash.outputs.elasticsearch][main] Created rollover alias {:name=>"<logstash-{now/d}-000001>"}\n[2021-07-28T11:54:18,467][INFO ][logstash.outputs.elasticsearch][main] Installing ILM policy {"policy"=>{"phases"=>{"hot"=>{"actions"=>{"rollover"=>{"max_size"=>"50gb", "max_age"=>"30d"}}}}}} {:name=>"logstash-policy"}\n[2021-07-28T11:54:19,417][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.23}\n[2021-07-28T11:54:19,619][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_3c3582877a316ab29d93318b4e2f134b", :path=>["/usr/share/logstash/access_log"]}\n[2021-07-28T11:54:19,640][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}\n[2021-07-28T11:54:19,699][INFO ][filewatch.observingtail  ][main][d3d6ab5c59cc3016c39f3dda745d68e612eae8fca705ae266565c45069b70e27] START, creating Discoverer, Watch with file and sincedb collections\n[2021-07-28T11:54:19,706][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}\n{\n       "response" => "200",\n           "host" => "debian11",\n          "ident" => "-",\n           "auth" => "-",\n    "httpversion" => "1.1",\n      "timestamp" => "30/Apr/2017:04:28:11 +0000",\n       "referrer" => "\\"-\\"",\n       "@version" => "1",\n     "@timestamp" => 2017-04-30T04:28:11.000Z,\n          "agent" => "\\"Mozilla/5.0 (compatible; DotBot/1.1; http://www.opensiteexplorer.org/dotbot, help@moz.com)\\"",\n       "clientip" => "216.244.66.246",\n           "path" => "/usr/share/logstash/access_log",\n           "verb" => "GET",\n        "request" => "/docs/triton/pages.html",\n        "message" => "216.244.66.246 - - [30/Apr/2017:04:28:11 +0000] \\"GET /docs/triton/pages.html HTTP/1.1\\" 200 5639 \\"-\\" \\"Mozilla/5.0 (compatible; DotBot/1.1; http://www.opensiteexplorer.org/dotbot, help@moz.com)\\"",\n          "bytes" => "5639"\n}\n{\n       "response" => "200",\n           "host" => "debian11",\n          "ident" => "-",\n           "auth" => "-",\n    "httpversion" => "1.1",\n      "timestamp" => "30/Apr/2017:04:30:31 +0000",\n       "referrer" => "\\"-\\"",\n       "@version" => "1",\n     "@timestamp" => 2017-04-30T04:30:31.000Z,\n          "agent" => "\\"Mozilla/5.0 (compatible; AhrefsBot/5.2; +http://ahrefs.com/robot/)\\"",\n       "clientip" => "217.182.132.36",\n           "path" => "/usr/share/logstash/access_log",\n           "verb" => "GET",\n        "request" => "/2012/08/sundog-software-featured-in-august-2012-issue-of-develop/",\n        "message" => "217.182.132.36 - - [30/Apr/2017:04:30:31 +0000] \\"GET /2012/08/sundog-software-featured-in-august-2012-issue-of-develop/ HTTP/1.1\\" 200 14326 \\"-\\" \\"Mozilla/5.0 (compatible; AhrefsBot/5.2; +http://ahrefs.com/robot/)\\"",\n          "bytes" => "14326"\n}\n\n\n...\n\n')),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Fantastic!")," And checking Elasticsearch I am able to see that the Logstash index was successfully generated:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"curl 'localhost:9200/_cat/indices?v'\n\nhealth status index                      uuid                   pri rep docs.count docs.deleted store.size pri.store.size\nyellow open   logstash-2021.07.28-000001 7jZ-amZEQZCKqNxk8wFPGA   1   1     102972            0     28.1mb         28.1mb\nyellow open   shakespeare                xkrzXAdeSWKmYiEUUA4N7g   1   1     111396            0     18.9mb         18.9mb\n")),(0,s.kt)("p",null,"The index was generated and we are able to search it:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -XGET \'localhost:9200/logstash-2021.07.28-000001/_search?pretty\'                                                                                      \n{\n  "took" : 22,\n  "timed_out" : false,\n  "_shards" : {\n    "total" : 1,\n    "successful" : 1,\n    "skipped" : 0,\n    "failed" : 0\n  },\n  "hits" : {\n    "total" : {\n      "value" : 10000,\n      "relation" : "gte"\n    },\n    "max_score" : 1.0,\n    "hits" : [\n      {\n        "_index" : "logstash-2021.07.28-000001",\n        "_type" : "_doc",\n        "_id" : "q_b37HoBWtrhasx0jqIE",\n        "_score" : 1.0,\n        "_source" : {\n          "response" : "200",\n          "host" : "debian11",\n          "ident" : "-",\n          "auth" : "-",\n          "httpversion" : "1.1",\n          "timestamp" : "02/May/2017:03:41:24 +0000",\n          "referrer" : "\\"http://sundog-soft.com/wp-cron.php?doing_wp_cron=1493696484.8063950538635253906250\\"",\n          "@version" : "1",\n          "@timestamp" : "2017-05-02T03:41:24.000Z",\n          "agent" : "\\"WordPress/4.7.4; http://sundog-soft.com\\"",\n          "clientip" : "54.210.20.202",\n          "path" : "/usr/share/logstash/access_log",\n          "verb" : "POST",\n          "request" : "/wp-cron.php?doing_wp_cron=1493696484.8063950538635253906250",\n          "message" : "54.210.20.202 - - [02/May/2017:03:41:24 +0000] \\"POST /wp-cron.php?doing_wp_cron=1493696484.8063950538635253906250 HTTP/1.1\\" 200 - \\"http://sundog-soft.com/wp-cron.php?doing_wp_cron=1493696484.8063950538635253906250\\" \\"WordPress/4.7.4; http://sundog-soft.com\\""\n        }\n      },\n      {\n        "_index" : "logstash-2021.07.28-000001",\n        "_type" : "_doc",\n        "_id" : "CPb37HoBWtrhasx0jqOZ",\n        "_score" : 1.0,\n        "_source" : {\n          "response" : "301",\n          "host" : "debian11",\n          "ident" : "-",\n          "auth" : "-",\n          "httpversion" : "1.1",\n          "timestamp" : "02/May/2017:03:47:28 +0000",\n          "referrer" : "\\"-\\"",\n          "@version" : "1",\n          "@timestamp" : "2017-05-02T03:47:28.000Z",\n          "agent" : "\\"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\\"",\n          "clientip" : "180.76.15.32",\n          "path" : "/usr/share/logstash/access_log",\n          "verb" : "GET",\n          "request" : "/?utm_source=newsletter&utm_medium=email&utm_campaign=triton-151",\n          "message" : "180.76.15.32 - - [02/May/2017:03:47:28 +0000] \\"GET /?utm_source=newsletter&utm_medium=email&utm_campaign=triton-151 HTTP/1.1\\" 301 - \\"-\\" \\"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\\""\n        }\n      },\n\n\n      ...\n\n')),(0,s.kt)("p",null,"To search for e.g. the responses you got for a certain campaign, run the following POST search query:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -u elastic:changeme http://localhost:9200/logstash-2021.07.28-000001/_doc/_search?pretty=true -H \'Content-Type: application/json\' -d \'{\n    "query": {\n        "query_string": {"query": "triton-151"}\n    }\n}\'\n')),(0,s.kt)("h2",{id:"setup-kibana"},"Setup Kibana"),(0,s.kt)("p",null,"We can now start the Kibana container:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'docker run -d \\\n  --name kibana \\\n  --net=host \\\n  -e "ELASTICSEARCH_HOSTS=http://localhost:9200" \\\n  -e "XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic" \\\n  -e "XPACK_MONITORING_ELASTICSEARCH_PASSWORD=changeme" \\\n  kibana:7.13.4\n')),(0,s.kt)("p",null,"And create the ",(0,s.kt)("strong",{parentName:"p"},"Index Pattern")," with ",(0,s.kt)("strong",{parentName:"p"},"timestamp")," as our primary field:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Elastic Logstash",src:a(35898).Z,width:"1016",height:"684"})),(0,s.kt)("p",null,"We can now re-run our campaign search query from before and will get nice graphic showing us exactly when those campaign URL's were hit:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Elastic Logstash",src:a(18964).Z,width:"1359",height:"835"})))}u.isMDXComponent=!0},35898:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Elastic_Logstash_01-a9ec36842c035e05cbdb8f9adaf54593.png"},18964:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Elastic_Logstash_02-ed74de28e440d6929089a34383939443.png"},72051:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/photo-456tdsfggd_67gfh6dgdf4_d-173870f4fc20686d0a5e5aa08fcf7552.jpg"}}]);